{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOKIO Dashboard Prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams.update({'font.size': 18})\n",
    "import matplotlib.gridspec\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats as stats\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "import textwrap\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and refine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Relative path to the repository's root directory\n",
    "_REPO_BASE_DIR = os.path.join('..', '..')\n",
    "\n",
    "### Translates cryptic counter names into something suitable for labeling plots\n",
    "counter_labels = json.load(open(os.path.join(_REPO_BASE_DIR, 'scripts', 'counter_labels.json'), 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### black magic necessary for processing Mira log files :(\n",
    "try:\n",
    "    import pytz\n",
    "    _USE_TZ = True\n",
    "except ImportError:\n",
    "    _USE_TZ = False\n",
    "\n",
    "def utc_timestamp_to_YYYYMMDD( timestamp ):\n",
    "    \"\"\"\n",
    "    This is a batty function that allows us to compare the UTC-based\n",
    "    timestamps from Darshan logs (start_time and end_time) to the\n",
    "    Chicago-based YYYY-MM-DD dates used to index the mmdf data.\n",
    "    \"\"\"\n",
    "    if _USE_TZ:\n",
    "        ### we know that these logs are from Chicago\n",
    "        tz = pytz.timezone(\"America/Chicago\")\n",
    "        \n",
    "        ### Darshan log's start time in UTC, so turn it into a datetime with UTC on it\n",
    "        darshan_time = pytz.utc.localize(datetime.datetime.utcfromtimestamp(timestamp))\n",
    "        \n",
    "        ### Then convert this UTC start time into a local start time so\n",
    "        ### we can compare it to the local mmdf timestamp\n",
    "        darshan_time_at_argonne = darshan_time.astimezone(tz)\n",
    "        return darshan_time_at_argonne\n",
    "    else:\n",
    "        ### we assume that this script is running on Argonne time; it's the best we can do\n",
    "        warnings.warn(\"pytz is not available so mmdf data might be misaligned by a day!\")\n",
    "        return datetime.datetime.fromtimestamp(timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Edison\n",
    "df_edison = pandas.DataFrame.from_csv(os.path.join(_REPO_BASE_DIR,\n",
    "                                                   'data',\n",
    "                                                   'dat',\n",
    "                                                   'tokio-lustre',\n",
    "                                                   'edison-abc-stats_2-14_3-23.csv')).dropna()\n",
    "df_edison['darshan_rw'] = [ 'write' if x == 1 else 'read' for x in df_edison['darshan_write_mode?'] ]\n",
    "df_edison['darshan_file_mode'] = [ 'shared' if x in ['H5Part','MPIIO'] else 'fpp' for x in df_edison['darshan_api'] ]\n",
    "df_edison.rename(columns={'lmt_bytes_covered': 'coverage_factor'}, inplace=True)\n",
    "df_edison['system'] = \"edison\"\n",
    "df_edison['iops_coverage_factor'] = -1.0\n",
    "\n",
    "### Mira\n",
    "df_mira = pandas.DataFrame.from_csv(os.path.join(_REPO_BASE_DIR,\n",
    "                                                'data',\n",
    "                                                'dat',\n",
    "                                                'tokio-gpfs',\n",
    "                                                'alcf-abc-stats_2-25_3-19.dat')).dropna()\n",
    "rename_dict = { '# platform': \"system\" }\n",
    "for key in df_mira.keys():\n",
    "    if key == 'file_sys':\n",
    "        rename_dict[key] = 'darshan_file_system'\n",
    "    elif key not in rename_dict and not key.startswith('ggio_'):\n",
    "        rename_dict[key] = 'darshan_' + key\n",
    "df_mira.rename(columns=rename_dict, inplace=True)\n",
    "df_mira['darshan_file_mode'] = [ 'shared' if x in ['H5Part','MPIIO'] else 'fpp' for x in df_mira['darshan_api'] ]\n",
    "df_mira['coverage_factor'] = df_mira['darshan_total_bytes'] / (df_mira['ggio_bytes_read'] + df_mira['ggio_bytes_written'])\n",
    "df_mira['iops_coverage_factor'] = (df_mira['darshan_total_rws'] / (df_mira['ggio_read_reqs'] + df_mira['ggio_write_reqs']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_mmdf = pandas.DataFrame.from_csv(os.path.join(_REPO_BASE_DIR,\n",
    "                                                'data',\n",
    "                                                'dat',\n",
    "                                                'tokio-gpfs',\n",
    "                                                'mira_mmdf_1-25_3-23.csv'),\n",
    "                                        index_col=['file_system', 'date'])\n",
    "df_mmdf['free_kib'] = df_mmdf['free_kib_blocks'] + df_mmdf['free_kib_frags']\n",
    "df_mmdf['free_pct'] = df_mmdf['free_kib'] / df_mmdf['disk_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### I really hope iterrows behaves deterministically and preserves order...\n",
    "new_data = {\n",
    "    'mmdf_avg_fullness_pct': [],\n",
    "    'mmdf_max_fullness_pct': [],\n",
    "}\n",
    "\n",
    "### iterate over each row of the master Mira dataframe\n",
    "for row in df_mira.itertuples():\n",
    "    fs_key = row.darshan_file_system\n",
    "    mmdf_key = utc_timestamp_to_YYYYMMDD( row.darshan_start_time ).strftime(\"%Y-%m-%d\")\n",
    "    if mmdf_key in df_mmdf.loc[fs_key].index:\n",
    "        ### only look at today's data\n",
    "        df = df_mmdf.loc[fs_key].loc[mmdf_key]\n",
    "        \n",
    "        data_cols = [ True if x else False for x in df['data?'] ]\n",
    "\n",
    "        ### calculate a percent fullness - don't bother saving the id of this fullest server though\n",
    "        new_data['mmdf_max_fullness_pct'].append( 1.0 - df[ data_cols ]['free_pct'].min() )\n",
    "        new_data['mmdf_avg_fullness_pct'].append( 1.0 - df[ data_cols ]['free_pct'].mean() )\n",
    "    else:\n",
    "        new_data['mmdf_max_fullness_pct'].append( np.nan )\n",
    "        new_data['mmdf_avg_fullness_pct'].append( np.nan )\n",
    "\n",
    "for new_col_name, new_col_data in new_data.iteritems():\n",
    "    df_mira[new_col_name] = new_col_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pandas.concat( (df_mira, df_edison) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.drop(df.index[df['coverage_factor'] > 1.2], inplace=True)\n",
    "# df.drop(df.index[df['iops_coverage_factor'] > 1.2], inplace=True)\n",
    "df.drop(df.index[(df['system'] == 'mira') & (df['darshan_jobid'] == 1039807)], inplace=True)\n",
    "df.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the dashboard view\n",
    "\n",
    "Decide what to display on the dashboard by subselecting a specific view from the loaded data via `df_plot` and then populating the `row_plots` list with only those variables we want to display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Some rendering parameters for the dashboard itself\n",
    "_DASHBOARD_FONT_SIZE = 12\n",
    "_DASHBOARD_LINE_WIDTH = 1\n",
    "_DASHBOARD_HIGHLIGHT_COLORS = [ '#DA0017', '#FD6A07', '#40A43A', '#2C69A9' ]\n",
    "_DASHBOARD_LINE_COLOR = '#853692'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Variables which we wish to display on the UMAMI\n",
    "### dashboard.  Keyed by variables to display, and the values\n",
    "### indicate if a high value is good\n",
    "row_plots = None\n",
    "row_plots_master = { \n",
    "    'edison': [\n",
    "        ('darshan_agg_perf_by_slowest_gibs', True),\n",
    "        ('coverage_factor',                  True),\n",
    "        ('lmt_mds_ave',                      False),\n",
    "        ('lmt_ops_opencloses',               False),\n",
    "        ('lmt_oss_max',                      False),\n",
    "        ('ost_avg_pct',                      False),\n",
    "#       ('ost_bad_pct',                      False),\n",
    "        ('job_max_radius',                   False),\n",
    "        ('job_concurrent_jobs',              False),\n",
    "    ],\n",
    "    'mira': [\n",
    "        ('darshan_agg_perf_by_slowest_gibs', True),\n",
    "        ('coverage_factor',                  True),\n",
    "        ('iops_coverage_factor',             True),\n",
    "        ('ggio_ops_opencloses',              False),\n",
    "        ('ggio_ops_rw',                      False),\n",
    "#       ('ggio_read_dirs',                   False),\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def umami_filter(df, file_system, app, rw, other_filters=None):\n",
    "    \"\"\"\n",
    "    Translates a few basic logical input parameters into a filtered dataframe\n",
    "    \"\"\"\n",
    "    filter_list = []\n",
    "    if file_system is not None:\n",
    "        filter_list.append((df['darshan_file_system'] == file_system))\n",
    "    if app is not None:\n",
    "        filter_list.append((df['darshan_app'] == app))\n",
    "    if rw is not None:\n",
    "        filter_list.append((df[\"darshan_rw\"] == rw))\n",
    "\n",
    "    if other_filters is not None:\n",
    "        filter_list = filter_list + other_filters\n",
    "    ### Apply filters to cut down on the data we're going to present\n",
    "    num_rows = len(df)\n",
    "    print \"Start with %d rows before filtering\" % num_rows\n",
    "    net_filter = [ True for i in range(len(df.index))]\n",
    "    for idx, condition in enumerate(filter_list):\n",
    "        ct = len( [ x for x in net_filter if x ] )\n",
    "        net_filter &= condition\n",
    "        num_drops = (ct - len( [ x for x in net_filter if x ] ))\n",
    "        print \"Dropped %d rows after filter #%d (%d left)\" % (num_drops, idx, ct-num_drops)\n",
    "\n",
    "    print \"%d rows will be included in UMAMI\" % len(df[net_filter].index)\n",
    "    assert len(df[net_filter].index) > 0\n",
    "    \n",
    "    return df[net_filter].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define what we want UMAMI to show us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### For generating the Edison UMAMI figure in the paper\n",
    "TARGET_FILE_SYSTEM = 'scratch2'\n",
    "TARGET_APP = 'HACC-IO'\n",
    "TARGET_RW = 'write'\n",
    "OUTPUT_SUFFIX = None\n",
    "other_filters = [\n",
    "     (df['darshan_end_time'] >= time.mktime(datetime.datetime(2017,  2, 25,  0,  0,  0).timetuple())),\n",
    "     (df['darshan_end_time'] <= time.mktime(datetime.datetime(2017,  3,  3, 12,  0,  0).timetuple())),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### For generating the Mira UMAMI figure in the paper (v1)\n",
    "TARGET_FILE_SYSTEM = 'mira-fs1'\n",
    "TARGET_APP = 'VPIC-IO'\n",
    "TARGET_RW = 'write'\n",
    "OUTPUT_SUFFIX = None\n",
    "other_filters = [\n",
    "     (df['darshan_end_time'] >= time.mktime(datetime.datetime(2017,  3,  1,  0,  0,  0).timetuple())),\n",
    "     (df['darshan_end_time'] <= time.mktime(datetime.datetime(2017,  3, 12,  0,  0,  0).timetuple())),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### For generating the Mira UMAMI figure in the paper (v2)\n",
    "TARGET_FILE_SYSTEM = 'mira-fs1'\n",
    "TARGET_APP = 'IOR'\n",
    "TARGET_RW = 'read'\n",
    "OUTPUT_SUFFIX = '-shared-all'\n",
    "other_filters = [\n",
    "    (df['darshan_file_mode'] == \"shared\"),\n",
    "#    (df['darshan_end_time'] >= time.mktime(datetime.datetime(2016,  2, 27, 12,  0,  0).timetuple())),\n",
    "#    (df['darshan_end_time'] <= time.mktime(datetime.datetime(2017,  3,  9,  0,  0,  0).timetuple())),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### An interesting long-term behavior where an OSS's CPU was maxing out\n",
    "TARGET_FILE_SYSTEM = 'scratch3'\n",
    "TARGET_APP = 'HACC-IO'\n",
    "TARGET_RW = 'write'\n",
    "OUTPUT_SUFFIX = '-long-term'\n",
    "other_filters = [\n",
    "#   (df['darshan_file_mode'] == \"fpp\"),\n",
    "    (df['darshan_end_time'] >= time.mktime(datetime.datetime(2017,  2, 21,  0,  0,  0).timetuple())),\n",
    "    (df['darshan_end_time'] <= time.mktime(datetime.datetime(2017,  3, 15,  0,  0,  0).timetuple())),\n",
    "]\n",
    "row_plots = [\n",
    "        ('darshan_agg_perf_by_slowest_gibs', True),\n",
    "        ('lmt_oss_max',                      False),\n",
    "        ('ost_max_pct',                      False),\n",
    "        ('coverage_factor',                  True),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TARGET_FILE_SYSTEM = 'mira-fs1'\n",
    "TARGET_APP = 'HACC-IO'\n",
    "TARGET_RW = 'write'\n",
    "OUTPUT_SUFFIX = '-test'\n",
    "other_filters = [\n",
    "    (df['darshan_end_time'] >= time.mktime(datetime.datetime(2017,  3,  1,  0,  0,  0).timetuple())),\n",
    "    (df['darshan_end_time'] <= time.mktime(datetime.datetime(2017,  3, 12,  0,  0,  0).timetuple())),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a DataFrame based on UMAMI inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Apply the filters we defined above to get a dataframe suitable for UMAMI\n",
    "df_plot = umami_filter(df, TARGET_FILE_SYSTEM, TARGET_APP, TARGET_RW, other_filters)\n",
    "if row_plots is None:\n",
    "    row_plots = row_plots_master['mira' if 'mira' in TARGET_FILE_SYSTEM else 'edison']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the correct input view and outputs are defined, make a copy of the data that we manipulate to get the data into a plottable form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_plot['darshan_agg_perf_by_slowest_gibs'] = df_plot['darshan_agg_perf_by_slowest'] / 1024.0\n",
    "df_plot['lmt_ops_opencloses'] = df_plot['lmt_ops_opens'] + df_plot['lmt_ops_closes']\n",
    "df_plot['ggio_ops_opencloses'] = df_plot['ggio_opens'] + df_plot['ggio_closes']\n",
    "df_plot['ggio_ops_rw'] = df_plot['ggio_read_reqs'] + df_plot['ggio_write_reqs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Can also just add up all the MDS ops (of interest).\n",
    "### Even though they are not weighted evenly in terms\n",
    "### of cost on MDS, this is a rough approximation of\n",
    "### activity since metadata rates are also bursty and\n",
    "### probably don't overlap often.\n",
    "for i in df_plot.keys():\n",
    "    if \"_ops_\" in i:\n",
    "        if \"lmt_ops_total\" in df_plot:\n",
    "            df_plot['lmt_ops_total'] += df_plot[i]\n",
    "        else:\n",
    "            df_plot['lmt_ops_total'] = df_plot[i]\n",
    "counter_labels['lmt_ops_total'] = \"Server Metadata Ops\"\n",
    "counter_labels['lmt_ops_opencloses'] = \"Server Open/Close Ops\"\n",
    "counter_labels['ggio_ops_opencloses'] = \"Server Open/Close/Creat Ops\"\n",
    "counter_labels['ggio_ops_rw'] = \"Server Read/Write Ops\"\n",
    "\n",
    "### Scale op counts to make them plottable:\n",
    "for i in df_plot.keys():\n",
    "    if \"_ops_\" in i or i == \"ggio_read_dirs\":\n",
    "        max_val = df_plot[i].max()\n",
    "        if max_val > 2e9:\n",
    "            df_plot[i] = df_plot[i] / 1e9\n",
    "            counter_labels[i] += \" (GOps)\"\n",
    "        elif max_val > 2e6:\n",
    "            df_plot[i] = df_plot[i] / 1e6\n",
    "            counter_labels[i] += \" (MOps)\"\n",
    "        elif max_val > 2e3:\n",
    "            df_plot[i] = df_plot[i] / 1e3\n",
    "            counter_labels[i] += \" (KOps)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now actually generate the dashboard diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_size_inches(6, len(row_plots) * 12 / 9)\n",
    "\n",
    "### Required to adjust the column widths of our figure (width_ratios)\n",
    "gridspec = matplotlib.gridspec.GridSpec(len(row_plots), 2, width_ratios=[4,1])\n",
    "\n",
    "last_ax_ts = None\n",
    "for idx, (plot_variable, big_is_good) in enumerate(row_plots):\n",
    "    def dt64todatetime(dt64):\n",
    "        \"\"\"\n",
    "        the dataframe stores datetimes as np.datetime64,\n",
    "        which is expressed in nanoseconds (1e9 seconds).  To\n",
    "        convert this to a datetime.datetime object which we\n",
    "        can transform in matplotlib, some amount of\n",
    "        gymnastics is required.\n",
    "        \"\"\"\n",
    "        return datetime.datetime.fromtimestamp(dt64.astype(int) * 1e-9)\n",
    "    \n",
    "    ### Cast all pandas times (numpy.datetime64) into Python datetimes\n",
    "    x = [ datetime.datetime.fromtimestamp(x) for x in df_plot['darshan_end_time'].values ]\n",
    "    y = df_plot[plot_variable].values\n",
    "\n",
    "    ### first plot the timeseries of the given variable\n",
    "    ax_ts = fig.add_subplot(gridspec[2*idx])\n",
    "    ax_ts.plot(x, y,\n",
    "               linestyle='-',\n",
    "               marker='x',\n",
    "               linewidth=_DASHBOARD_LINE_WIDTH * 1.0,\n",
    "               color=_DASHBOARD_LINE_COLOR)\n",
    "\n",
    "    # textwrap.wrap inserts line breaks into each label\n",
    "    ax_ts.set_ylabel('\\n'.join(textwrap.wrap(\n",
    "                        text=counter_labels.get(plot_variable, plot_variable),\n",
    "                        width=15)),\n",
    "                        fontsize=_DASHBOARD_FONT_SIZE,\n",
    "                        rotation=0,\n",
    "                        horizontalalignment='right',\n",
    "                        verticalalignment='center'\n",
    "                    )\n",
    "    ax_ts.grid()\n",
    "\n",
    "    # blank out the labels for all subplots except the bottom-most one\n",
    "    if idx != len(row_plots) - 1:\n",
    "        ax_ts.set_xticklabels([])\n",
    "    else:\n",
    "        last_ax_ts = ax_ts\n",
    "        # resize and rotate the labels for the timeseries plot\n",
    "        for tick in ax_ts.xaxis.get_major_ticks():\n",
    "            tick.label.set_fontsize(_DASHBOARD_FONT_SIZE) \n",
    "            tick.label.set_rotation(45)\n",
    "\n",
    "    # also adjust the font size for the y labels\n",
    "    for tick in ax_ts.yaxis.get_major_ticks():\n",
    "        tick.label.set_fontsize(_DASHBOARD_FONT_SIZE)\n",
    "\n",
    "    ### then plot the boxplot summary of the given variable\n",
    "    ax_box = fig.add_subplot(gridspec[2*idx+1])\n",
    "    boxp = ax_box.boxplot(y[0:-1], ### note: do not include last measurement in boxplot\n",
    "                   widths=0.70,\n",
    "                   boxprops={'linewidth':_DASHBOARD_LINE_WIDTH},\n",
    "                   medianprops={'linewidth':_DASHBOARD_LINE_WIDTH},\n",
    "                   whiskerprops={'linewidth':_DASHBOARD_LINE_WIDTH},\n",
    "                   capprops={'linewidth':_DASHBOARD_LINE_WIDTH},\n",
    "                   flierprops={'linewidth':_DASHBOARD_LINE_WIDTH},\n",
    "                   whis=[5,95])\n",
    "    \n",
    "    # scale the extents of the y ranges a little for clarity\n",
    "    ax_ts.set_ylim(map(lambda a, b: a*(1 + b), ax_ts.get_ylim(), (-0.1, 0.1)))\n",
    "    \n",
    "    # lock in the y range to match the timeseries plot, just in case\n",
    "    ax_box.set_ylim(ax_ts.get_ylim())\n",
    "\n",
    "    # determine the color of our highlights based on quartile\n",
    "    percentiles = [ np.percentile(y[0:-1], percentile) for percentile in 25, 50, 75, 100 ]\n",
    "    for color_index, percentile in enumerate(percentiles):\n",
    "        if y[-1] <= percentile:\n",
    "            break\n",
    "    if big_is_good:\n",
    "        highlight_color = _DASHBOARD_HIGHLIGHT_COLORS[color_index]\n",
    "    else:\n",
    "        highlight_color = _DASHBOARD_HIGHLIGHT_COLORS[(1+color_index)*-1]\n",
    "                                        \n",
    "    # highlight the latest measurement on the timeseries plot\n",
    "    x_last = matplotlib.dates.date2num(x[-1])\n",
    "    x_2nd_last = matplotlib.dates.date2num(x[-2])\n",
    "    ax_ts.plot([x_2nd_last, x_last],\n",
    "               [y[-2], y[-1]],\n",
    "               linestyle='-',\n",
    "               color=highlight_color,\n",
    "               linewidth=_DASHBOARD_LINE_WIDTH * 2.0)\n",
    "    ax_ts.plot([x_last], [y[-1]],\n",
    "               marker='*',\n",
    "               color=highlight_color,\n",
    "               markersize=15)\n",
    "\n",
    "    # where does this last data point lie on the distribution?\n",
    "    ax_box.plot([0,2], [y[-1],y[-1]], linestyle='--', color=highlight_color, linewidth=2.0, zorder=10)\n",
    "\n",
    "    # blank out all labels\n",
    "    ax_box.set_yticklabels([\"\"])\n",
    "    ax_box.set_xticklabels([\"\"])\n",
    "    ax_box.yaxis.grid()\n",
    "\n",
    "fig.subplots_adjust(hspace=0.0, wspace=0.0)\n",
    "fig.autofmt_xdate()\n",
    "last_ax_ts.xaxis.set_major_formatter(matplotlib.dates.DateFormatter('%b %d'))\n",
    "\n",
    "output_file = \"umami-%s-%s-%s%s.pdf\" % ( TARGET_FILE_SYSTEM, TARGET_APP.replace('-IO', '').lower(), TARGET_RW, OUTPUT_SUFFIX if OUTPUT_SUFFIX is not None else \"\" )\n",
    "fig.savefig(output_file, bbox_inches=\"tight\")\n",
    "print \"Saved %s\" % output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
