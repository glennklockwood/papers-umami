{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOKIO Dashboard Prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams.update({'font.size': 18})\n",
    "import matplotlib.gridspec\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats as stats\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "import textwrap\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Relative path to the repository's root directory\n",
    "_REPO_BASE_DIR = os.path.join('..', '..')\n",
    "\n",
    "### Translates cryptic counter names into something suitable for labeling plots\n",
    "_COUNTER_LABELS = json.load(open(os.path.join(_REPO_BASE_DIR, 'scripts', 'counter_labels.json'), 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Variables which we wish to display on the UMAMI\n",
    "### dashboard.  Keyed by variables to display, and the values\n",
    "### indicate if a high value is good\n",
    "_DEFAULT_ROW_PLOTS = { \n",
    "    'edison': [\n",
    "        ('darshan_agg_perf_by_slowest_gibs', True),\n",
    "        ('coverage_factor',                  True),\n",
    "        ('nodehr_coverage_factor',           True),\n",
    "        ('lmt_mds_ave',                      False),\n",
    "        ('lmt_ops_opencloses',               False),\n",
    "        ('lmt_oss_max',                      False),\n",
    "        ('ost_avg_pct',                      False),\n",
    "#       ('ost_bad_pct',                      False),\n",
    "        ('job_max_radius',                   False),\n",
    "        ('job_concurrent_jobs',              False),\n",
    "    ],\n",
    "    'mira': [\n",
    "        ('darshan_agg_perf_by_slowest_gibs', True),\n",
    "        ('coverage_factor',                  True),\n",
    "        ('iops_coverage_factor',             True),\n",
    "        ('ggio_ops_opencloses',              False),\n",
    "        ('ggio_ops_rw',                      False),\n",
    "#       ('ggio_read_dirs',                   False),\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Some rendering parameters for the dashboard itself\n",
    "_DASHBOARD_FONT_SIZE = 12\n",
    "_DASHBOARD_LINE_WIDTH = 1\n",
    "_DASHBOARD_HIGHLIGHT_COLORS = [ '#DA0017', '#FD6A07', '#40A43A', '#2C69A9' ]\n",
    "_DASHBOARD_LINE_COLOR = '#853692'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and refine data\n",
    "\n",
    "Load both Edison and Mira data from CSVs and attach the MMDF data to Mira"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### black magic necessary for processing Mira log files :(\n",
    "try:\n",
    "    import pytz\n",
    "    _USE_TZ = True\n",
    "except ImportError:\n",
    "    _USE_TZ = False\n",
    "\n",
    "def utc_timestamp_to_YYYYMMDD( timestamp ):\n",
    "    \"\"\"\n",
    "    This is a batty function that allows us to compare the UTC-based\n",
    "    timestamps from Darshan logs (start_time and end_time) to the\n",
    "    Chicago-based YYYY-MM-DD dates used to index the mmdf data.\n",
    "    \"\"\"\n",
    "    if _USE_TZ:\n",
    "        ### we know that these logs are from Chicago\n",
    "        tz = pytz.timezone(\"America/Chicago\")\n",
    "        \n",
    "        ### Darshan log's start time in UTC, so turn it into a datetime with UTC on it\n",
    "        darshan_time = pytz.utc.localize(datetime.datetime.utcfromtimestamp(timestamp))\n",
    "        \n",
    "        ### Then convert this UTC start time into a local start time so\n",
    "        ### we can compare it to the local mmdf timestamp\n",
    "        darshan_time_at_argonne = darshan_time.astimezone(tz)\n",
    "        return darshan_time_at_argonne\n",
    "    else:\n",
    "        ### we assume that this script is running on Argonne time; it's the best we can do\n",
    "        warnings.warn(\"pytz is not available so mmdf data might be misaligned by a day!\")\n",
    "        return datetime.datetime.fromtimestamp(timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Edison\n",
    "df_edison = pandas.DataFrame.from_csv(os.path.join(_REPO_BASE_DIR,\n",
    "                                                   'data',\n",
    "                                                   'dat',\n",
    "                                                   'tokio-lustre',\n",
    "                                                   'edison-abc-stats_2-14_3-28_v2.csv')).dropna()\n",
    "df_edison['darshan_rw'] = [ 'write' if x == 1 else 'read' for x in df_edison['darshan_write_mode?'] ]\n",
    "df_edison['darshan_file_mode'] = [ 'shared' if x in ['H5Part','MPIIO'] else 'fpp' for x in df_edison['darshan_api'] ]\n",
    "df_edison.rename(columns={'lmt_bytes_covered': 'coverage_factor'}, inplace=True)\n",
    "df_edison['system'] = \"edison\"\n",
    "df_edison['iops_coverage_factor'] = -1.0\n",
    "df_edison['nodehr_coverage_factor'] = df_edison['job_num_nodes'] * \\\n",
    "                                      (df_edison['darshan_end_time'] - df_edison['darshan_start_time']) / 3600.0 / \\\n",
    "                                      (df_edison['job_concurrent_nodehrs'])\n",
    "\n",
    "### Mira\n",
    "df_mira = pandas.DataFrame.from_csv(os.path.join(_REPO_BASE_DIR,\n",
    "                                                'data',\n",
    "                                                'dat',\n",
    "                                                'tokio-gpfs',\n",
    "                                                'alcf-abc-stats_2-25_3-19.dat')).dropna()\n",
    "rename_dict = { '# platform': \"system\" }\n",
    "for key in df_mira.keys():\n",
    "    if key == 'file_sys':\n",
    "        rename_dict[key] = 'darshan_file_system'\n",
    "    elif key not in rename_dict and not key.startswith('ggio_'):\n",
    "        rename_dict[key] = 'darshan_' + key\n",
    "df_mira.rename(columns=rename_dict, inplace=True)\n",
    "df_mira['darshan_file_mode'] = [ 'shared' if x in ['H5Part','MPIIO'] else 'fpp' for x in df_mira['darshan_api'] ]\n",
    "df_mira['coverage_factor'] = df_mira['darshan_total_bytes'] / (df_mira['ggio_bytes_read'] + df_mira['ggio_bytes_written'])\n",
    "df_mira['iops_coverage_factor'] = (df_mira['darshan_total_rws'] / (df_mira['ggio_read_reqs'] + df_mira['ggio_write_reqs']))\n",
    "df_mira['nodehr_coverage_factor'] = -1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_mmdf = pandas.DataFrame.from_csv(os.path.join(_REPO_BASE_DIR,\n",
    "                                                'data',\n",
    "                                                'dat',\n",
    "                                                'tokio-gpfs',\n",
    "                                                'mira_mmdf_1-25_3-23.csv'),\n",
    "                                        index_col=['file_system', 'date'])\n",
    "df_mmdf['free_kib'] = df_mmdf['free_kib_blocks'] + df_mmdf['free_kib_frags']\n",
    "df_mmdf['free_pct'] = df_mmdf['free_kib'] / df_mmdf['disk_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### I really hope iterrows behaves deterministically and preserves order...\n",
    "new_data = {\n",
    "    'mmdf_avg_fullness_pct': [],\n",
    "    'mmdf_max_fullness_pct': [],\n",
    "}\n",
    "\n",
    "### iterate over each row of the master Mira dataframe\n",
    "for row in df_mira.itertuples():\n",
    "    fs_key = row.darshan_file_system\n",
    "    mmdf_key = utc_timestamp_to_YYYYMMDD( row.darshan_start_time ).strftime(\"%Y-%m-%d\")\n",
    "    if mmdf_key in df_mmdf.loc[fs_key].index:\n",
    "        ### only look at today's data\n",
    "        df = df_mmdf.loc[fs_key].loc[mmdf_key]\n",
    "        \n",
    "        data_cols = [ True if x else False for x in df['data?'] ]\n",
    "\n",
    "        ### calculate a percent fullness - don't bother saving the id of this fullest server though\n",
    "        new_data['mmdf_max_fullness_pct'].append( 1.0 - df[ data_cols ]['free_pct'].min() )\n",
    "        new_data['mmdf_avg_fullness_pct'].append( 1.0 - df[ data_cols ]['free_pct'].mean() )\n",
    "    else:\n",
    "        new_data['mmdf_max_fullness_pct'].append( np.nan )\n",
    "        new_data['mmdf_avg_fullness_pct'].append( np.nan )\n",
    "\n",
    "for new_col_name, new_col_data in new_data.iteritems():\n",
    "    df_mira[new_col_name] = new_col_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pandas.concat( (df_mira, df_edison) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.drop(df.index[df['coverage_factor'] > 1.2], inplace=True)\n",
    "\n",
    "df.drop(df.index[(df['system'] == 'mira') & (df['darshan_jobid'] == 1039807)], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthesize new metrics and normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['darshan_agg_perf_by_slowest_gibs'] = df['darshan_agg_perf_by_slowest'] / 1024.0\n",
    "df['lmt_ops_opencloses'] = df['lmt_ops_opens'] + df['lmt_ops_closes']\n",
    "df['ggio_ops_opencloses'] = df['ggio_opens'] + df['ggio_closes']\n",
    "df['ggio_ops_rw'] = df['ggio_read_reqs'] + df['ggio_write_reqs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Can also just add up all the MDS ops (of interest).\n",
    "### Even though they are not weighted evenly in terms\n",
    "### of cost on MDS, this is a rough approximation of\n",
    "### activity since metadata rates are also bursty and\n",
    "### probably don't overlap often.\n",
    "for i in df.keys():\n",
    "    if \"_ops_\" in i:\n",
    "        if \"lmt_ops_total\" in df:\n",
    "            df['lmt_ops_total'] += df[i]\n",
    "        else:\n",
    "            df['lmt_ops_total'] = df[i]\n",
    "_COUNTER_LABELS['lmt_ops_total'] = \"Server Metadata Ops\"\n",
    "_COUNTER_LABELS['lmt_ops_opencloses'] = \"Server Open/Close Ops\"\n",
    "_COUNTER_LABELS['ggio_ops_opencloses'] = \"Server Open/Close/Creat Ops\"\n",
    "_COUNTER_LABELS['ggio_ops_rw'] = \"Server Read-Write Ops\"\n",
    "_COUNTER_LABELS['nodehr_coverage_factor'] = \"Coverage Factor (Node Hours)\"\n",
    "_COUNTER_LABELS['coverage_factor'] = \"Coverage Factor (Bandwidth)\"\n",
    "\n",
    "### Scale op counts to make them plottable:\n",
    "for i in df.keys():\n",
    "    if \"_ops_\" in i or i == \"ggio_read_dirs\":\n",
    "        max_val = df[i].max()\n",
    "        if max_val > 2e9:\n",
    "            df[i] = df[i] / 1e9\n",
    "            _COUNTER_LABELS[i] += \" (GOps)\"\n",
    "            print \"Scaling %s to GOps\" % i\n",
    "        elif max_val > 2e6:\n",
    "            df[i] = df[i] / 1e6\n",
    "            _COUNTER_LABELS[i] += \" (MOps)\"\n",
    "            print \"Scaling %s to MOps\" % i\n",
    "        elif max_val > 2e3:\n",
    "            df[i] = df[i] / 1e3\n",
    "            _COUNTER_LABELS[i] += \" (KOps)\"\n",
    "            print \"Scaling %s to KOps\" % i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define what we want UMAMI to show us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### This list will contain the input parameters for each UMAMI we want to generate\n",
    "umami_list = [\n",
    "    ### For generating the \"I/O contention\" case study figure (v2)\n",
    "    {\n",
    "        'fs': 'scratch2',\n",
    "        'app': 'HACC-IO',\n",
    "        'rw': 'write',\n",
    "        'suffix': None,\n",
    "        'other_filters': [\n",
    "            (df['darshan_end_time'] <= time.mktime(datetime.datetime(2017,  3,  3, 12,  0,  0).timetuple())),\n",
    "        ],\n",
    "        'rows': [\n",
    "            ('darshan_agg_perf_by_slowest_gibs', True),\n",
    "            ('coverage_factor',                  True),\n",
    "            ('nodehr_coverage_factor',           True),\n",
    "            ('lmt_mds_ave',                      False),\n",
    "            ('lmt_ops_opencloses',               False),\n",
    "            ('job_max_radius',                   False),\n",
    "            ('job_concurrent_jobs',              False),\n",
    "        ],\n",
    "\n",
    "    },\n",
    "    ### For generating the \"namespace contention\" case study figure\n",
    "    {\n",
    "        'fs': 'mira-fs1',\n",
    "        'app': 'VPIC-IO',\n",
    "        'rw': 'write',\n",
    "        'suffix': None,\n",
    "        'other_filters': [\n",
    "             (df['darshan_end_time'] >= time.mktime(datetime.datetime(2017,  3,  1,  0,  0,  0).timetuple())),\n",
    "             (df['darshan_end_time'] <= time.mktime(datetime.datetime(2017,  3, 12,  0,  0,  0).timetuple())),\n",
    "        ],\n",
    "        'rows': _DEFAULT_ROW_PLOTS['mira'],\n",
    "    },\n",
    "    ### For generating the \"storage capacity\" case study figure\n",
    "    {\n",
    "        'fs': 'scratch3',\n",
    "        'app': 'HACC-IO',\n",
    "        'rw': 'write',\n",
    "        'suffix': '-long-term',\n",
    "        'other_filters': [\n",
    "            (df['darshan_end_time'] >= time.mktime(datetime.datetime(2017,  2, 21,  0,  0,  0).timetuple())),\n",
    "            (df['darshan_end_time'] <= time.mktime(datetime.datetime(2017,  3, 15,  0,  0,  0).timetuple())),\n",
    "        ],\n",
    "        'rows': [\n",
    "            ('darshan_agg_perf_by_slowest_gibs', True),\n",
    "            ('coverage_factor',                  True),\n",
    "#           ('nodehr_coverage_factor',           True),\n",
    "            ('lmt_oss_max',                      False),\n",
    "            ('ost_max_pct',                      False),\n",
    "        ],\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions required to generate UMAMIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def umami_filter(df, file_system, app, rw, other_filters=None):\n",
    "    \"\"\"\n",
    "    Translates a few basic logical input parameters into a filtered dataframe\n",
    "    \"\"\"\n",
    "    filter_list = []\n",
    "    if file_system is not None:\n",
    "        filter_list.append((df['darshan_file_system'] == file_system))\n",
    "    if app is not None:\n",
    "        filter_list.append((df['darshan_app'] == app))\n",
    "    if rw is not None:\n",
    "        filter_list.append((df[\"darshan_rw\"] == rw))\n",
    "\n",
    "    if other_filters is not None:\n",
    "        filter_list = filter_list + other_filters\n",
    "    ### Apply filters to cut down on the data we're going to present\n",
    "    num_rows = len(df)\n",
    "    print \"Start with %d rows before filtering\" % num_rows\n",
    "    net_filter = [ True for i in range(len(df.index))]\n",
    "    for idx, condition in enumerate(filter_list):\n",
    "        ct = len( [ x for x in net_filter if x ] )\n",
    "        net_filter &= condition\n",
    "        num_drops = (ct - len( [ x for x in net_filter if x ] ))\n",
    "        print \"Dropped %d rows after filter #%d (%d left)\" % (num_drops, idx, ct-num_drops)\n",
    "\n",
    "    print \"%d rows will be included in UMAMI\" % len(df[net_filter].index)\n",
    "    assert len(df[net_filter].index) > 0\n",
    "    \n",
    "    return df[net_filter].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_umami( df_plot, rows_to_plot, output_file=None ):\n",
    "    fig = plt.figure()\n",
    "    fig.set_size_inches(6, len(rows_to_plot) * 12 / 9)\n",
    "\n",
    "    ### Required to adjust the column widths of our figure (width_ratios)\n",
    "    gridspec = matplotlib.gridspec.GridSpec(len(rows_to_plot), 2, width_ratios=[4,1])\n",
    "\n",
    "    last_ax_ts = None\n",
    "    for idx, (plot_variable, big_is_good) in enumerate(rows_to_plot):\n",
    "        def dt64todatetime(dt64):\n",
    "            \"\"\"\n",
    "            the dataframe stores datetimes as np.datetime64,\n",
    "            which is expressed in nanoseconds (1e9 seconds).  To\n",
    "            convert this to a datetime.datetime object which we\n",
    "            can transform in matplotlib, some amount of\n",
    "            gymnastics is required.\n",
    "            \"\"\"\n",
    "            return datetime.datetime.fromtimestamp(dt64.astype(int) * 1e-9)\n",
    "\n",
    "        ### Cast all pandas times (numpy.datetime64) into Python datetimes\n",
    "        x = [ datetime.datetime.fromtimestamp(x) for x in df_plot['darshan_end_time'].values ]\n",
    "        y = df_plot[plot_variable].values\n",
    "\n",
    "        ### first plot the timeseries of the given variable\n",
    "        ax_ts = fig.add_subplot(gridspec[2*idx])\n",
    "        ax_ts.plot(x, y,\n",
    "                   linestyle='-',\n",
    "                   marker='x',\n",
    "                   linewidth=_DASHBOARD_LINE_WIDTH * 1.0,\n",
    "                   color=_DASHBOARD_LINE_COLOR)\n",
    "\n",
    "        # textwrap.wrap inserts line breaks into each label\n",
    "        ax_ts.set_ylabel('\\n'.join(textwrap.wrap(\n",
    "                            text=_COUNTER_LABELS.get(plot_variable, plot_variable).replace('/', '-'),\n",
    "                                width=15, break_on_hyphens=True)).replace('-', '/'),\n",
    "                            fontsize=_DASHBOARD_FONT_SIZE,\n",
    "                            rotation=0,\n",
    "                            horizontalalignment='right',\n",
    "                            verticalalignment='center'\n",
    "                        )\n",
    "        ax_ts.grid()\n",
    "\n",
    "        # blank out the labels for all subplots except the bottom-most one\n",
    "        if idx != len(rows_to_plot) - 1:\n",
    "            ax_ts.set_xticklabels([])\n",
    "        else:\n",
    "            last_ax_ts = ax_ts\n",
    "            # resize and rotate the labels for the timeseries plot\n",
    "            for tick in ax_ts.xaxis.get_major_ticks():\n",
    "                tick.label.set_fontsize(_DASHBOARD_FONT_SIZE) \n",
    "                tick.label.set_rotation(45)\n",
    "\n",
    "        # also adjust the font size for the y labels\n",
    "        for tick in ax_ts.yaxis.get_major_ticks():\n",
    "            tick.label.set_fontsize(_DASHBOARD_FONT_SIZE)\n",
    "\n",
    "        ### then plot the boxplot summary of the given variable\n",
    "        ax_box = fig.add_subplot(gridspec[2*idx+1])\n",
    "        boxp = ax_box.boxplot(y[0:-1], ### note: do not include last measurement in boxplot\n",
    "                       widths=0.70,\n",
    "                       boxprops={'linewidth':_DASHBOARD_LINE_WIDTH},\n",
    "                       medianprops={'linewidth':_DASHBOARD_LINE_WIDTH},\n",
    "                       whiskerprops={'linewidth':_DASHBOARD_LINE_WIDTH},\n",
    "                       capprops={'linewidth':_DASHBOARD_LINE_WIDTH},\n",
    "                       flierprops={'linewidth':_DASHBOARD_LINE_WIDTH},\n",
    "                       whis=[5,95])\n",
    "\n",
    "        # scale the extents of the y ranges a little for clarity\n",
    "        orig_ylim = ax_ts.get_ylim()\n",
    "        new_ylim = map(lambda a, b: a*(1 + b), orig_ylim, (-0.1, 0.1))\n",
    "        ax_ts.set_ylim(new_ylim)\n",
    "        \n",
    "        yticks = ax_ts.get_yticks().tolist()\n",
    "        \n",
    "        # the following is a heuristic to determine how close the topmost\n",
    "        # tick label is to the edge of the plot.  if it's too close, blank\n",
    "        # it out so it doesn't overlap with the bottom-most tick label\n",
    "        # of the row above it\n",
    "        critical_fraction = abs(1.0 - (yticks[-1] - new_ylim[0]) / (new_ylim[-1] - new_ylim[0]))\n",
    "        if idx > 0 and critical_fraction < 0.01:\n",
    "            ### note that setting one of the yticks to a string resets the\n",
    "            ### formatting so that the tick labels appear as floats.  since\n",
    "            ### we (hopefully) would get integral ticks otherwise, force\n",
    "            ### them to ints.  This will mess things up if the yrange is\n",
    "            ### very narrow and must be expressed as floats.\n",
    "            yticks = map(int, yticks)\n",
    "            yticks[-1] = \" \"\n",
    "            ax_ts.set_yticklabels(yticks)\n",
    "                        \n",
    "        # lock in the y range to match the timeseries plot, just in case\n",
    "        ax_box.set_ylim(ax_ts.get_ylim())\n",
    "\n",
    "        # determine the color of our highlights based on quartile\n",
    "        percentiles = [ np.percentile(y[0:-1], percentile) for percentile in 25, 50, 75, 100 ]\n",
    "        for color_index, percentile in enumerate(percentiles):\n",
    "            if y[-1] <= percentile:\n",
    "                break\n",
    "        if big_is_good:\n",
    "            highlight_color = _DASHBOARD_HIGHLIGHT_COLORS[color_index]\n",
    "        else:\n",
    "            highlight_color = _DASHBOARD_HIGHLIGHT_COLORS[(1+color_index)*-1]\n",
    "\n",
    "        # highlight the latest measurement on the timeseries plot\n",
    "        x_last = matplotlib.dates.date2num(x[-1])\n",
    "        x_2nd_last = matplotlib.dates.date2num(x[-2])\n",
    "        ax_ts.plot([x_2nd_last, x_last],\n",
    "                   [y[-2], y[-1]],\n",
    "                   linestyle='-',\n",
    "                   color=highlight_color,\n",
    "                   linewidth=_DASHBOARD_LINE_WIDTH * 2.0)\n",
    "        ax_ts.plot([x_last], [y[-1]],\n",
    "                   marker='*',\n",
    "                   color=highlight_color,\n",
    "                   markersize=15)\n",
    "\n",
    "        # where does this last data point lie on the distribution?\n",
    "        ax_box.plot([0,2], [y[-1],y[-1]], linestyle='--', color=highlight_color, linewidth=2.0, zorder=10)\n",
    "\n",
    "        # blank out all labels\n",
    "        ax_box.set_yticklabels([\"\"])\n",
    "        ax_box.set_xticklabels([\"\"])\n",
    "        ax_box.yaxis.grid()\n",
    "\n",
    "    fig.subplots_adjust(hspace=0.0, wspace=0.0)\n",
    "    fig.autofmt_xdate()\n",
    "    last_ax_ts.xaxis.set_major_formatter(matplotlib.dates.DateFormatter('%b %d'))\n",
    "\n",
    "    if output_file is not None:\n",
    "        fig.savefig(output_file, bbox_inches=\"tight\")\n",
    "        print \"Saved %s\" % output_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINALLY generate the actual UMAMIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for umami_config in umami_list:\n",
    "\n",
    "    output_file = \"umami-%s-%s-%s%s.pdf\" % (\n",
    "        umami_config['fs'],\n",
    "        umami_config['app'].replace('-IO', '').lower(),\n",
    "        umami_config['rw'], \n",
    "        umami_config['suffix'] if umami_config['suffix'] is not None else \"\"\n",
    "    )\n",
    "    print \"Generating %s\" % output_file\n",
    "    ### Apply the filters we defined above to get a dataframe suitable for UMAMI\n",
    "    df_plot = umami_filter(df,\n",
    "                           umami_config['fs'],\n",
    "                           umami_config['app'],\n",
    "                           umami_config['rw'],\n",
    "                           umami_config['other_filters'])\n",
    "\n",
    "    generate_umami( df_plot, umami_config['rows'], output_file=output_file )\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
