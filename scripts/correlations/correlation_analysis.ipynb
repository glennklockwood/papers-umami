{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Statistical Analysis of TOKIO-ABC Results\n",
      "\n",
      "This notebook performs various statistical analyses of the summary data generated by each TOKIO-ABC job.  These data are loaded from __a summary csv file__ where each row represents a single job and contains the relevant data extracted from\n",
      "\n",
      "1. the darshan log\n",
      "2. the server-side I/O monitoring (Lustre LMT or GPFS GGIOSTAT)\n",
      "3. optional system-specific monitoring including\n",
      "    - OST health info (Lustre)\n",
      "    - Concurrent job count (Slurm)\n",
      "    - Job radius (Cray XC)\n",
      "    \n",
      "This input CSV is generated at NERSC by\n",
      "\n",
      "1. running `utils/nersc_generate_job_summary.sh` to generate json summary records for each darshan log\n",
      "2. running `utils/json2csv.py` to convert all jsons into a single csv file\n",
      "\n",
      "This script is used to identify interesting patterns and correlations across jobs.  For intra-job inspection, use other analysis notebooks such as `analysis/per_ost_deep_dive.ipynb`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "%matplotlib inline\n",
      "import matplotlib\n",
      "import matplotlib.pyplot as plt\n",
      "matplotlib.rcParams.update({'font.size': 18})\n",
      "plt.rcParams['image.cmap'] = 'gray'\n",
      "import matplotlib.gridspec\n",
      "import os"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "import pandas\n",
      "import numpy as np\n",
      "import scipy\n",
      "import scipy.stats as stats\n",
      "import scipy.interpolate\n",
      "import json\n",
      "import datetime\n",
      "import bisect\n",
      "import warnings\n",
      "import textwrap\n",
      "\n",
      "def wrap(text, width=15):\n",
      "    \"\"\"wrapper for the wrapper\"\"\"\n",
      "    return '\\n'.join(textwrap.wrap(text=text,width=width))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Relative path to the repository's root directory\n",
      "_REPO_BASE_DIR = os.path.join('..', '..')\n",
      "\n",
      "### Translates cryptic counter names into something suitable for labeling plots\n",
      "counter_labels = json.load(open(os.path.join(_REPO_BASE_DIR, 'scripts', 'counter_labels.json'), 'r'))\n",
      "\n",
      "### For consistency, always plot file systems in the same order\n",
      "_FILE_SYSTEM_ORDER = [ 'scratch1', 'scratch2', 'scratch3', 'mira-fs1' ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def plot_corr(df,size=20):\n",
      "    \"\"\"\n",
      "    Function plots a graphical correlation matrix for each pair\n",
      "    of columns in the dataframe.  From\n",
      "    \n",
      "    http://stackoverflow.com/questions/29432629/correlation-matrix-using-pandas\n",
      "\n",
      "    Input:\n",
      "        df: pandas DataFrame\n",
      "        size: vertical and horizontal size of the plot\n",
      "    \"\"\"\n",
      "    matplotlib.rc('xtick', labelsize=20)\n",
      "    matplotlib.rc('ytick', labelsize=20)\n",
      "    corr = df.corr()\n",
      "    fig, ax = plt.subplots(figsize=(size, size))\n",
      "    ax.matshow(corr, cmap=plt.get_cmap('seismic'),\n",
      "                    norm=matplotlib.colors.Normalize(vmin=-1.,vmax=1.) )\n",
      "    plt.xticks(range(len(corr.columns)), corr.columns, rotation='vertical')\n",
      "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
      "    return corr"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Load data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Edison\n",
      "df_edison = pandas.DataFrame.from_csv(os.path.join(_REPO_BASE_DIR,\n",
      "                                                   'data',\n",
      "                                                   'dat',\n",
      "                                                   'tokio-lustre',\n",
      "                                                   'edison-abc-stats_2-14_3-23.csv')).dropna()\n",
      "df_edison['darshan_rw'] = [ 'write' if x == 1 else 'read' for x in df_edison['darshan_write_mode?'] ]\n",
      "df_edison['darshan_file_mode'] = [ 'shared' if x in ['H5Part','MPIIO'] else 'fpp' for x in df_edison['darshan_api'] ]\n",
      "df_edison.rename(columns={'lmt_bytes_covered': 'coverage_factor'}, inplace=True)\n",
      "df_edison['system'] = \"edison\"\n",
      "df_edison['iops_coverage_factor'] = -1.0\n",
      "\n",
      "### Mira\n",
      "df_mira = pandas.DataFrame.from_csv(os.path.join(_REPO_BASE_DIR,\n",
      "                                                'data',\n",
      "                                                'dat',\n",
      "                                                'tokio-gpfs',\n",
      "                                                'alcf-abc-stats_2-25_3-19.dat')).dropna()\n",
      "rename_dict = { '# platform': \"system\" }\n",
      "for key in df_mira.keys():\n",
      "    if key == 'file_sys':\n",
      "        rename_dict[key] = 'darshan_file_system'\n",
      "    elif key not in rename_dict and not key.startswith('ggio_'):\n",
      "        rename_dict[key] = 'darshan_' + key\n",
      "df_mira.rename(columns=rename_dict, inplace=True)\n",
      "df_mira['darshan_file_mode'] = [ 'shared' if x in ['H5Part','MPIIO'] else 'fpp' for x in df_mira['darshan_api'] ]\n",
      "df_mira['coverage_factor'] = df_mira['darshan_total_bytes'] / (df_mira['ggio_bytes_read'] + df_mira['ggio_bytes_written'])\n",
      "df_mira['iops_coverage_factor'] = (df_mira['darshan_total_rws'] / (df_mira['ggio_read_reqs'] + df_mira['ggio_write_reqs']))\n",
      "\n",
      "### Both\n",
      "df_concat = pandas.concat( (df_mira, df_edison) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Filter Data\n",
      "\n",
      "Two notable filters are applied:\n",
      "\n",
      "1. All jobs where the bytes coverage factor and ops coverage factor are greater than 1.2 are discarded because they reflect severely misaligned or gappy data.\n",
      "\n",
      "2. Mira job 1039807 is excluded because ggiostat returned highly abnormal results starting that day.  See e-mail from Shane and Phil on March 23 about this.\n",
      "\n",
      "3. All Edison jobs from March 12 were discarded because LMT broke as a result of daylight saving time rolling over.  This filter was applied _before_ the input CSV files loaded above were generated, so it does not need to be applied here."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for df in df_mira, df_edison, df_concat:\n",
      "    df.drop(df.index[df['coverage_factor'] > 1.2], inplace=True)\n",
      "    df.drop(df.index[df['iops_coverage_factor'] > 1.2], inplace=True)\n",
      "    \n",
      "    df.drop(df.index[(df['system'] == 'mira') & (df['darshan_jobid'] == 1039807)], inplace=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Normalize Performance\n",
      "Different file systems, benchmarks, and read/write modes are capable of different peak bandwidths.  As such, we want to normalize the absolute performance (`summarize_key`) by something.  For convenience we calculate the denominator for normalization a couple of different ways (e.g., the mean, median, and max measurement).  We also limit normalization to unique combinations of variables specified by `normalization_group` below."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Calculate the normalization factors (the denominators), then apply that factor to all of the data in the DataFrame.  These normalized data will be saved as new columns in the DataFrame."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### The actual variable we want to normalize\n",
      "summarize_key = 'darshan_agg_perf_by_slowest'\n",
      "\n",
      "for df in df_edison, df_mira, df_concat:\n",
      "    ### Specify which keys we want to group together before normalizing\n",
      "    normalization_group = df.groupby(['darshan_app', 'darshan_file_system', 'darshan_file_mode', 'darshan_rw'])\n",
      "\n",
      "    ### Dict to store the denominators for normalization\n",
      "    normalization_data = {\n",
      "        'mean':   normalization_group.darshan_agg_perf_by_slowest.mean(),\n",
      "        'median': normalization_group.darshan_agg_perf_by_slowest.median(),\n",
      "        'max':    normalization_group.darshan_agg_perf_by_slowest.max(),\n",
      "    }\n",
      "\n",
      "    ### Normalize every row in the DataFrame by all of our denominators\n",
      "    new_cols = {}\n",
      "    for func in normalization_data.keys():\n",
      "        new_col_key = 'darshan_normalized_perf_by_%s' % func\n",
      "        new_cols[new_col_key] = []\n",
      "        for index, row in df.iterrows():\n",
      "            new_cols[new_col_key].append(\n",
      "                row[summarize_key] / normalization_data[func]\n",
      "                                                       [row['darshan_app']]\n",
      "                                                       [row['darshan_file_system']]\n",
      "                                                       [row['darshan_file_mode']]\n",
      "                                                       [row['darshan_rw']]\n",
      "            )\n",
      "\n",
      "    ### Also just do per-file system\n",
      "    normalization_group = df.groupby('darshan_file_system')\n",
      "    normalization_data = {\n",
      "        'mean':   normalization_group.darshan_agg_perf_by_slowest.mean(),\n",
      "        'median': normalization_group.darshan_agg_perf_by_slowest.median(),\n",
      "        'max':    normalization_group.darshan_agg_perf_by_slowest.max(),\n",
      "    }\n",
      "    for func in normalization_data.keys():\n",
      "        new_col_key = 'darshan_normalized_perf_by_fs_%s' % func\n",
      "        new_cols[new_col_key] = []\n",
      "        for index, row in df.iterrows():\n",
      "            new_cols[new_col_key].append(\n",
      "                row[summarize_key] / normalization_data[func][row['darshan_file_system']])\n",
      "\n",
      "    ### Take our normalized data and add them as new columns\n",
      "    for new_col, new_col_data in new_cols.iteritems():\n",
      "        df[new_col] = new_col_data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Multivariate Correlation Analysis\n",
      "`performance_key` is the variable we wish to use to represent performance.  It is typically\n",
      "\n",
      "- `darshan_agg_perf_by_slowest`, which is the absolute performance measured by each benchmark run\n",
      "- `darshan_normalized_perf_by_max`, which is normalized by the maximum observed performance\n",
      "- `darshan_normalized_perf_by_mean`, which is normalized by the mean observed performance"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "performance_key = 'darshan_normalized_perf_by_max'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Pearson Correlation Analysis\n",
      "Pearson analysis assumes that each variable is normally distributed.  It is easier to understand, but it is not technically correct for variables that are _not_ normally distributed, which include performance.  The Spearman coefficient would be better.\n",
      "\n",
      "At any rate, this correlation matrix is not of interest to this paper so don't bother generating it here."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### make a pretty plot to flag highlights\n",
      "if False:\n",
      "    corr_edison = plot_corr(df_edison[\n",
      "        (df_edison['darshan_file_system'] == 'scratch1') \n",
      "        | (df_edison['darshan_file_system'] == 'scratch2') \n",
      "        | (df_edison['darshan_file_system'] == 'scratch3')\n",
      "    ], 10)\n",
      "    corr_mira = plot_corr(df[df['darshan_file_system'] == 'mira-fs1'], 10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Numerical Correlation Analysis\n",
      "Now we repeat this correlation analysis, but this time use `scipy.stats` instead of `pandas` so that we can calculate p-values associated with each correlation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def correlation_calculation(df, analysis_func=stats.pearsonr, only_print_key=performance_key, ignore_cols=[]):\n",
      "    \"\"\"\n",
      "    Calculate the Pearson correlation coefficient and the associated\n",
      "    p-value for every permutation of counter pairs\n",
      "    \"\"\"\n",
      "    num_cols = len(df.keys())\n",
      "    results = []\n",
      "    for i in range(num_cols - 1):\n",
      "        i_col = df.columns[i]\n",
      "        for j in range(i, num_cols):\n",
      "            j_col = df.columns[j]\n",
      "            try:\n",
      "                coeff, pval = analysis_func(df[i_col],\n",
      "                                            df[j_col])\n",
      "            except TypeError: # non-numeric column\n",
      "                continue\n",
      "            results.append((i_col,\n",
      "                            j_col,\n",
      "                            coeff,\n",
      "                            pval))\n",
      "\n",
      "    sorted_results = sorted(results, key=lambda x: x[3])\n",
      "    ret_results = []\n",
      "    for col_name1, col_name2, coeff, pval in sorted_results:\n",
      "        ### don't print trivial relationships\n",
      "        if pval == 0 or pval == 1:\n",
      "            continue\n",
      "        ### don't print relationships with very high p-values\n",
      "#       if pval > 0.05:\n",
      "#           continue\n",
      "        ### don't correlate data from the same source since much of it is degenerate\n",
      "        if col_name1.split('_',1)[0] == col_name2.split('_',1)[0]:\n",
      "            continue\n",
      "        ### don't print anything except for the key of interest (if provided)\n",
      "        if only_print_key is not None \\\n",
      "        and col_name1 != only_print_key \\\n",
      "        and col_name2 != only_print_key:\n",
      "            continue\n",
      "        print \"%10.4f %10.4g %30s : %-15s\" % (coeff, pval, col_name1, col_name2)\n",
      "        \n",
      "        ### sort the output key orders\n",
      "        if col_name1 == only_print_key:\n",
      "            ret_results.append((col_name1,col_name2))\n",
      "        elif col_name2 == only_print_key:\n",
      "            ret_results.append((col_name2,col_name1))\n",
      "        else:\n",
      "            ret_results.append((col_name1,col_name2) if col_name2 > col_name1 else (col_name2,col_name1))\n",
      "    return ret_results"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"===== Edison =====\"\n",
      "correlations_edison = correlation_calculation(df_edison)\n",
      "print \"====== Mira ======\"\n",
      "correlations_mira = correlation_calculation(df_mira)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Scatter Plots\n",
      "To visualize these correlations, we define pairs of counters to plot against each other:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scatterplots = [ \n",
      "    (performance_key, 'coverage_factor'),\n",
      "    (performance_key, 'lmt_oss_ave'),\n",
      "    (performance_key, 'job_concurrent_jobs'),\n",
      "    (performance_key, 'ost_avg_pct'),\n",
      "    (performance_key, 'ost_max_kib'),\n",
      "    (performance_key, 'coverage_factor'),\n",
      "    (performance_key, 'ggio_write_reqs'),\n",
      "    (performance_key, 'ggio_write_reqs'),\n",
      "]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "...and then plot them:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "blacklist = set([\n",
      "    'ost_max_id', 'ost_min_id', 'lmt_tot_zeros',\n",
      "    'ost_failures_lead_secs', 'ost_fullness_lead_secs',\n",
      "    'ost_max_kib', 'ost_avg_kib', 'ost_min_kib', \n",
      "    'lmt_tot_missing'\n",
      "])\n",
      "# for scatterplot in correlations_edison + correlations_mira:\n",
      "for scatterplot in scatterplots:\n",
      "    x_key = scatterplot[0]\n",
      "    y_key = scatterplot[1]\n",
      "    if x_key in blacklist or y_key in blacklist:\n",
      "        continue\n",
      "    if y_key in df_mira :\n",
      "        df_plot = df_mira\n",
      "        system = \"Mira\"\n",
      "    elif y_key in df_edison:\n",
      "        df_plot = df_edison\n",
      "        system = \"Edison\"\n",
      "    else:\n",
      "        warnings.warn(\"Cannot find key %s in any data frames\" % y_key)\n",
      "    fig = plt.figure(figsize=(6,4))\n",
      "    ax = fig.add_subplot(111)\n",
      "    \n",
      "    x = df_plot[x_key].values\n",
      "    x_label = counter_labels.get(x_key, x_key)\n",
      "    y = df_plot[y_key].values\n",
      "    y_label = counter_labels.get(y_key, y_key)\n",
      "#   ax.hexbin(x, y, gridsize=25, cmap='PuRd')\n",
      "    ax.plot(x, y, 'o', alpha=0.5)\n",
      "\n",
      "    ### attempt a linear fit to generate a visual aid\n",
      "    m, b = np.polyfit(x, y, 1)\n",
      "    ax.plot(x, m*x+b, \"-\")\n",
      "    \n",
      "    ### add window dressing to plots\n",
      "#   fig.suptitle('Correlation between %s and %s' \n",
      "#                 % (x_label.split('(',1)[0].strip(),\n",
      "#                    y_label.split('(',1)[0].strip()))\n",
      "    ax.set_title(\"Coefficient=%.4f, P-value=%.2g (%s)\" \n",
      "                    % sum((stats.pearsonr(x, y), (system,)), ()), fontsize=14 )\n",
      "    ax.set_xlabel(x_label)\n",
      "    ax.set_ylabel(y_label)\n",
      "    plt.grid(True)\n",
      "    output_file = \"scatter_%s_vs_%s.pdf\" % (x_key, y_key)\n",
      "    fig.savefig(output_file, bbox_inches=\"tight\")\n",
      "    print \"Saved %s\" % output_file"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Mira also has both server-side and client side IOPS.  Let's look at those specifically:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = plt.figure(figsize=(8,4))\n",
      "ax = fig.add_subplot(111)\n",
      "\n",
      "df = df_mira.sort_values(performance_key).copy()\n",
      "\n",
      "for x_key, y_key in [(performance_key, 'ggio_write_reqs'),\n",
      "                    (performance_key, 'ggio_read_reqs')]:\n",
      "    x = df[x_key].values\n",
      "    y = df[y_key].values / df[y_key].max()\n",
      "    corr = stats.pearsonr(x, y)\n",
      "    print counter_labels.get(y_key, y_key), corr[0], corr[1]\n",
      "    points = ax.plot(x, y,\n",
      "                     'o',\n",
      "                     alpha=0.4,\n",
      "                     markersize=6.0,\n",
      "                     label=\"%s\" % (counter_labels.get(y_key, y_key))\n",
      "                    )\n",
      "        \n",
      "    ### attempt a linear fit to generate a visual aid\n",
      "    m, b = np.polyfit(x, y, 1)\n",
      "    ax.plot(x, m*x+b,\n",
      "            \"-\",\n",
      "           color=points[0].get_color())\n",
      "    \n",
      "ax.set_xlabel(counter_labels.get(x_key, x_key))\n",
      "ax.set_ylabel(\"Fraction Peak Ops\")\n",
      "ax.legend()\n",
      "plt.grid(True)\n",
      "output_file = \"scatter_mira_ops.pdf\"\n",
      "fig.savefig(output_file, bbox_inches=\"tight\")\n",
      "print \"Saved %s\" % output_file"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = plt.figure(figsize=(8,4))\n",
      "ax = fig.add_subplot(111)\n",
      "\n",
      "df = df_mira.sort_values(performance_key).copy()\n",
      "\n",
      "for x_key, y_key in [('coverage_factor', 'iops_coverage_factor')]:\n",
      "    x = df[x_key].values\n",
      "    y = df[y_key].values / df[y_key].max()\n",
      "    corr = stats.pearsonr(x, y)\n",
      "    print counter_labels.get(y_key, y_key), corr[0], corr[1]\n",
      "    points = ax.plot(x, y,\n",
      "                     'o',\n",
      "                     alpha=0.4,\n",
      "                     markersize=6.0,\n",
      "                     label=\"%s\" % (counter_labels.get(y_key, y_key))\n",
      "                    )\n",
      "        \n",
      "    ### attempt a linear fit to generate a visual aid\n",
      "    m, b = np.polyfit(x, y, 1)\n",
      "    ax.plot(x, m*x+b,\n",
      "            \"-\",\n",
      "           color=points[0].get_color())\n",
      "    \n",
      "ax.set_xlabel(counter_labels.get(x_key, x_key))\n",
      "ax.set_ylabel(\"Fraction Peak Ops\")\n",
      "ax.legend()\n",
      "plt.grid(True)\n",
      "output_file = \"scatter_mira_ops.pdf\"\n",
      "fig.savefig(output_file, bbox_inches=\"tight\")\n",
      "print \"Saved %s\" % output_file"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Plot the correlation between coverage factor and performance for each file system separately"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# for scatterplot in correlations_mira:\n",
      "for idx, fs in enumerate(_FILE_SYSTEM_ORDER):\n",
      "    fig = plt.figure(figsize=(8,6))\n",
      "    ax = fig.add_subplot(111)\n",
      "    x_key = performance_key\n",
      "    y_key = 'coverage_factor'\n",
      "    if fs.startswith('mira') :\n",
      "        df_plot = df_mira.sort_values(performance_key)\n",
      "        system = \"Mira\"\n",
      "    elif fs.startswith('scratch'):\n",
      "        df_plot = df_edison.sort_values(performance_key)\n",
      "        system = \"Edison\"\n",
      "    else:\n",
      "        warnings.warn(\"Cannot find key %s in any data frames\" % y_key)\n",
      "    df_plot = df_plot[df_plot['darshan_file_system'] == fs]\n",
      "    \n",
      "    x = df_plot[x_key].values\n",
      "    y = df_plot[y_key].values\n",
      "    corr = stats.pearsonr(x, y)\n",
      "    label = \"%s (corr=%.2f, p-val=%4.2g, n=%d)\" % (fs, corr[0], corr[1], len(df_plot))\n",
      "    points = ax.plot(x, y,\n",
      "                     'o', \n",
      "                     alpha=0.75,\n",
      "                     markersize=4.0)\n",
      "\n",
      "    ### attempt a linear fit to generate a visual aid\n",
      "    m, b = np.polyfit(x, y, 1)\n",
      "    ax.plot(x, m*x+b,\n",
      "            \"-\",\n",
      "           color=points[0].get_color())\n",
      "    \n",
      "    ### make the plot pretty\n",
      "    ax.set_xlim([0.0,2.0 if 'mean' in x_key else 1.0])\n",
      "    ax.set_ylabel(\"Coverage Factor\")\n",
      "    ax.set_xlabel(counter_labels[performance_key])\n",
      "    ax.set_title(label, fontsize=14)\n",
      "    plt.grid(True)\n",
      "    output_file = \"scatter_perf-vs-cf_%s.pdf\" % fs\n",
      "    fig.savefig(output_file, bbox_inches=\"tight\")\n",
      "    print \"Saved %s\" % output_file\n",
      "    fig.savefig(output_file.replace('pdf', 'png'), bbox_inches=\"tight\")\n",
      "    print \"Saved %s\" % output_file\n",
      "    df_save = pandas.DataFrame({\"Performance Relative to Mean\": x,\n",
      "                                \"Coverage Factor\": y}).to_csv(output_file.replace('pdf', 'csv'))\n",
      "    print \"Saved\", output_file.replace('pdf', 'csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Strategic Scatter Plots\n",
      "\n",
      "Now instead of just generating a bunch of scatter plots that show interesting correlations, let's plot some to illustrate specific relationships.\n",
      "\n",
      "First build a dataframe with normalized data so that we can fairly compare Mira and Edison if desired:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "### Juggle these for the purposes of correlating performance\n",
      "old_performance_key = performance_key\n",
      "performance_key = 'darshan_normalized_perf_by_max'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_plot_e = df_edison[['darshan_file_system', 'darshan_app', 'darshan_rw', performance_key]].copy()\n",
      "df_plot_e['opens'] = df_edison['lmt_ops_opens']\n",
      "df_plot_e['closes'] = df_edison['lmt_ops_closes']\n",
      "df_plot_e['opens+closes'] = df_plot_e['opens'] + df_plot_e['closes']\n",
      "df_plot_e['opens+closes_normalized'] = df_plot_e['opens+closes'] / df_plot_e['opens+closes'].max()\n",
      "\n",
      "df_plot_m = df_mira[['darshan_file_system', 'darshan_app', 'darshan_rw', performance_key]].copy()\n",
      "df_plot_m['opens'] = df_mira['ggio_opens']\n",
      "df_plot_m['closes'] = df_mira['ggio_closes']\n",
      "df_plot_m['opens+closes'] = df_plot_m['opens'] + df_plot_m['closes']\n",
      "df_plot_m['opens+closes_normalized'] = df_plot_m['opens+closes'] / df_plot_m['opens+closes'].max()\n",
      "df_plot_m['readops'] = df_mira['ggio_read_reqs']\n",
      "df_plot_m['writeops'] = df_mira['ggio_write_reqs']\n",
      "df_plot_m['iops'] = df_plot_m['readops'] + df_plot_m['writeops']\n",
      "df_plot_m['iops_normalized'] = df_plot_m['iops'] / df_plot_m['iops'].max()\n",
      "\n",
      "df_plot = pandas.concat([df_plot_e, df_plot_m], axis=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Plot the relationship between performance and open+close rates.  Our expectation is that Mira is more sensitive to this since metadata and data are serviced by the same GPFS servers; by comparison, Edison's Lustre file systems have discrete MDSes which should decouple data performance from metadata performance.\n",
      "\n",
      "That said, there is a non-causational relationship between metadata rates and data rates on Edison by virtue of the fact that lots of metadata implies that a lot of I/O (perhaps small transactions) is also happening on those file systems.  Because we aren't capturing IOPS rates on Lustre, we have no way to tell what the combined bandwidth and IOPS loads on Lustre are."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "_USE_LOG = False\n",
      "fig = plt.figure(figsize=(8,6))\n",
      "ax = fig.add_subplot(111)\n",
      "\n",
      "criteria = {\n",
      "    'Edison': df_plot['darshan_file_system'] == 'scratch1', # != 'mira-fs1',\n",
      "    'Mira': df_plot['darshan_file_system'] == 'mira-fs1',\n",
      "}\n",
      "for label, criterion in criteria.iteritems():\n",
      "    x = df_plot[criterion] \\\n",
      "        .sort_values(performance_key)[performance_key].values\n",
      "\n",
      "    ### normalize to only the data we're looking at on Edison\n",
      "    y = df_plot[criterion] \\\n",
      "        .sort_values(performance_key)['opens+closes'] \\\n",
      "        / df_plot[criterion]['opens+closes'].max()\n",
      "\n",
      "    if label == 'Edison':\n",
      "        markersize=2.0\n",
      "        alpha=0.75\n",
      "    else:\n",
      "        markersize=8.0\n",
      "        alpha=0.50\n",
      "    ax.plot(x, y, ls=\"\", marker='o', alpha=alpha, label=label, markersize=markersize)\n",
      "    \n",
      "    if not _USE_LOG:\n",
      "        ### attempt a linear fit to generate a visual aid\n",
      "        m, b = np.polyfit(x, y, 1)\n",
      "        ax.plot(x, m*x+b, \"-\", label=label, linewidth=4.0)\n",
      "    print \"Coefficient=%.4f, P-value=%.2g (%s)\" % sum((stats.pearsonr(x, y), (label,)), ())\n",
      "\n",
      "ax.set_xlabel(wrap(\"Fraction Peak Performance (Application)\", 30))\n",
      "ax.set_ylabel(wrap(\"Fraction Peak Opens+Closes (Server)\", 30))\n",
      "if _USE_LOG:\n",
      "    ax.set_yscale(\"log\")\n",
      "ax.legend()\n",
      "ax.grid()\n",
      "del _USE_LOG"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "On Mira, is application performance related to IOPS or bandwidth load?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "_USE_LOG = False\n",
      "fig = plt.figure(figsize=(8,6))\n",
      "ax = fig.add_subplot(111)\n",
      "\n",
      "criteria = {\n",
      "#    'Edison': df_plot['darshan_file_system'] == 'scratch3', # != 'mira-fs1',\n",
      "    'Mira read': ((df_plot['darshan_file_system'] == 'mira-fs1') & (df_plot['darshan_rw'] == 'read')),\n",
      "    'Mira write': ((df_plot['darshan_file_system'] == 'mira-fs1') & (df_plot['darshan_rw'] == 'write')),\n",
      "}\n",
      "for label, criterion in criteria.iteritems():\n",
      "    x = df_plot[criterion].sort_values(performance_key)[performance_key].values\n",
      "    y = df_plot[criterion].sort_values(performance_key)['iops_normalized']\n",
      "    if label == 'Edison':\n",
      "        markersize=2.0\n",
      "        alpha=0.75\n",
      "    else:\n",
      "        markersize=8.0\n",
      "        alpha=0.50\n",
      "    ax.plot(x, y, ls=\"\", marker='o', alpha=alpha, label=label, markersize=markersize)\n",
      "    \n",
      "    if not _USE_LOG:\n",
      "        ### attempt a linear fit to generate a visual aid\n",
      "        m, b = np.polyfit(x, y, 1)\n",
      "        ax.plot(x, m*x+b, \"-\", label=label, linewidth=4.0)\n",
      "    print \"Coefficient=%.4f, P-value=%.2g (%s)\" % sum((stats.pearsonr(x, y), (label,)), ())\n",
      "\n",
      "ax.set_xlabel(wrap(\"Fraction Peak Performance (Application)\", 30))\n",
      "ax.set_ylabel(wrap(\"Fraction Peak IOPS (Server)\", 30))\n",
      "if _USE_LOG:\n",
      "    ax.set_yscale(\"log\")\n",
      "ax.legend()\n",
      "ax.grid()\n",
      "ax.set_title(wrap(\"No compelling relationship between IOPS and bandwidth on GPFS\", 30),\n",
      "             fontsize=14,\n",
      "             position=(0.05, 0.4),\n",
      "             horizontalalignment='left',\n",
      "             backgroundcolor='white',\n",
      "            )\n",
      "del _USE_LOG"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What about everything else on Mira?  What is affecting Mira's performance?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "_USE_LOG = False\n",
      "fig = plt.figure(figsize=(8,6))\n",
      "ax = fig.add_subplot(111)\n",
      "\n",
      "criteria = {\n",
      "#    'Edison': df_plot['darshan_file_system'] == 'scratch3', # != 'mira-fs1',\n",
      "    'Mira read': ((df_plot['darshan_file_system'] == 'mira-fs1') & (df_plot['darshan_rw'] == 'read')),\n",
      "    'Mira write': ((df_plot['darshan_file_system'] == 'mira-fs1') & (df_plot['darshan_rw'] == 'write')),\n",
      "}\n",
      "for label, criterion in criteria.iteritems():\n",
      "    x = df_plot[criterion].sort_values(performance_key)[performance_key].values\n",
      "    y = df_plot[criterion].sort_values(performance_key)['iops_normalized']\n",
      "    if label == 'Edison':\n",
      "        markersize=2.0\n",
      "        alpha=0.75\n",
      "    else:\n",
      "        markersize=8.0\n",
      "        alpha=0.50\n",
      "    ax.plot(x, y, ls=\"\", marker='o', alpha=alpha, label=label, markersize=markersize)\n",
      "    \n",
      "    if not _USE_LOG:\n",
      "        ### attempt a linear fit to generate a visual aid\n",
      "        m, b = np.polyfit(x, y, 1)\n",
      "        ax.plot(x, m*x+b, \"-\", label=label, linewidth=4.0)\n",
      "    print \"Coefficient=%.4f, P-value=%.2g (%s)\" % sum((stats.pearsonr(x, y), (label,)), ())\n",
      "\n",
      "ax.set_xlabel(wrap(\"Fraction Peak Performance (Application)\", 30))\n",
      "ax.set_ylabel(wrap(\"Fraction Peak IOPS (Server)\", 30))\n",
      "if _USE_LOG:\n",
      "    ax.set_yscale(\"log\")\n",
      "ax.legend()\n",
      "ax.grid()\n",
      "ax.set_title(wrap(\"No compelling relationship between IOPS and bandwidth on GPFS\", 30),\n",
      "             fontsize=14,\n",
      "             position=(0.05, 0.4),\n",
      "             horizontalalignment='left',\n",
      "             backgroundcolor='white',\n",
      "            )\n",
      "del _USE_LOG"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "performance_key = old_performance_key"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Distribution of each benchmark type"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Following cell defines which variable we wish to aggregate into boxplots and a few plotting parameters that depend on our choice of variable."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "boxplot_settings = {\n",
      "    'fontsize': 15,\n",
      "    'darshan_normalized_perf_by_fs_max': {\n",
      "        'output_file': \"perf-boxplots-per-fs.pdf\",\n",
      "        'ylabel': \"Fraction of Peak\\nFile System Performance\",\n",
      "        'title_pos': [ \n",
      "            {'x': 0.97, 'y': 0.80, 'horizontalalignment': 'right', 'fontsize': 14},\n",
      "            {'x': 0.04, 'y': 0.02, 'horizontalalignment': 'left', 'fontsize': 14}]\n",
      "    },\n",
      "    'darshan_normalized_perf_by_max': {\n",
      "        'output_file': \"perf-boxplots.pdf\",\n",
      "        'ylabel': \"Fraction of Peak\\nPer-Benchmark Performance\",\n",
      "        'title_pos': [ \n",
      "            {'x': 0.04, 'y': 0.02, 'horizontalalignment': 'left', 'fontsize': 14},\n",
      "            {'x': 0.04, 'y': 0.02, 'horizontalalignment': 'left', 'fontsize': 14}]\n",
      "    },\n",
      "}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We plot two sets of boxplots based on the normalization denominator:\n",
      "\n",
      "1. normalized by the maximum without any grouping (other than file system)\n",
      "2. normalized by the maximum of each unique combination of app-read/write-filemode\n",
      "\n",
      "The idea is to show that\n",
      "\n",
      "1. performance variation varies across different file systems and different applications\n",
      "2. even within an application, the magnitude of performance variation varies with file system"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for plot_variable in performance_key, performance_key.replace('_by_', '_by_fs_'):\n",
      "    fig, axes = plt.subplots(nrows=2, ncols=2, sharex=True, sharey=True)\n",
      "    fig.set_size_inches(8,6)\n",
      "    boxplot_group_by = [ 'darshan_file_mode', 'darshan_rw', 'darshan_app' ]\n",
      "    for idx, fs in enumerate(_FILE_SYSTEM_ORDER):\n",
      "        icol = idx / 2\n",
      "        irow = idx % 2\n",
      "        ax = axes[irow, icol]\n",
      "        df_concat.loc[df_concat[\"darshan_file_system\"] == fs]\\\n",
      "        .boxplot(\n",
      "            column=[plot_variable],\n",
      "            by=boxplot_group_by,\n",
      "            ax=axes[irow, icol],\n",
      "            medianprops={'linewidth':2 },\n",
      "            widths=0.75,\n",
      "            whis=[5,95])\n",
      "\n",
      "        settings = boxplot_settings[plot_variable]['title_pos'][irow]\n",
      "        title = ax.set_title(fs, **(settings))\n",
      "        title.set_bbox({'color': 'white', 'alpha': 0.5})\n",
      "        ax.set_xlabel(\"\")\n",
      "        ax.set_ylabel(\"\")\n",
      "        ax.xaxis.grid(True)\n",
      "\n",
      "        ### relabel the x axis labels\n",
      "        new_labels = []\n",
      "        for axis_label in ax.get_xticklabels():\n",
      "            current_label = axis_label.get_text()\n",
      "            axis_label.set_rotation(90)\n",
      "            if \"IOR\" in current_label:\n",
      "                if \"shared\" in current_label:\n",
      "                    new_label = \"IOR/shared\"\n",
      "                else:\n",
      "                    new_label = \"IOR/fpp\"\n",
      "            elif 'BD-CATS' in current_label:\n",
      "                new_label = \"BD-CATS\"\n",
      "            else:\n",
      "                new_label = current_label.split(',')[2].strip(')').strip().split('-')[0]\n",
      "            if 'write' in current_label:\n",
      "                new_label += \" Write\"\n",
      "            else:\n",
      "                new_label += \" Read\"\n",
      "            new_labels.append(new_label)\n",
      "\n",
      "        ### set x tick labels for only the bottom row\n",
      "        if irow == 0:\n",
      "            ax.set_xticklabels([])\n",
      "        else:\n",
      "            ax.set_xticklabels(new_labels,\n",
      "                               fontsize=boxplot_settings['fontsize'])\n",
      "\n",
      "        ax.yaxis.set_ticks([0.0, 1.0])\n",
      "        ax.set_ylim([-0.1, 1.1])\n",
      "        for ytick in ax.yaxis.get_major_ticks():\n",
      "            ytick.label.set_fontsize(boxplot_settings['fontsize'])\n",
      "\n",
      "        \n",
      "    fig.suptitle(\"\")\n",
      "    # fig.text(0.5, -0.4, 'common X', ha='center')\n",
      "    fig.text(0.0, 0.5,\n",
      "             boxplot_settings[plot_variable]['ylabel'],\n",
      "             verticalalignment='center',\n",
      "             horizontalalignment='center',\n",
      "             rotation='vertical',\n",
      "             fontsize=boxplot_settings['fontsize'])\n",
      "    fig.subplots_adjust(hspace=0.05, wspace=0.05)\n",
      "\n",
      "    output_file = boxplot_settings[plot_variable]['output_file']\n",
      "    fig.savefig(output_file, bbox_inches=\"tight\")\n",
      "    print \"Saved %s\" % output_file"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Also plot a more general overview of performance across each file system."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = plt.figure(figsize=(8,6))\n",
      "ax = fig.add_subplot(111)\n",
      "df_concat.boxplot(\n",
      "    column=[plot_variable],\n",
      "    by=[\"darshan_file_system\"],\n",
      "    ax=ax,\n",
      "    widths=0.75,\n",
      "    boxprops={'linewidth':2},\n",
      "    medianprops={'linewidth':2 },\n",
      "    whiskerprops={'linewidth':2},\n",
      "    capprops={'linewidth':2},\n",
      "    flierprops={'linewidth':2},\n",
      "    whis=[5,95])\n",
      "### add window dressing to plots\n",
      "# plt.xticks(rotation=45)\n",
      "fig.suptitle(\"\")\n",
      "ax.set_title(\"\", fontsize=14 )\n",
      "ax.set_xlabel(\"\")\n",
      "ax.set_ylabel(y_label)\n",
      "ax.xaxis.grid(False)\n",
      "\n",
      "output_file = \"perf-boxplots-fs.pdf\"\n",
      "fig.savefig(output_file, bbox_inches=\"tight\")\n",
      "print \"Saved %s\" % output_file"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Also create a boxplot of the coverage factor to demonstrate how often jobs were impacted by other jobs"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = plt.figure(figsize=(8,6))\n",
      "ax = fig.add_subplot(111)\n",
      "df_concat.boxplot(\n",
      "    column=['coverage_factor'],\n",
      "    by=[\"darshan_file_system\"],\n",
      "    ax=ax,\n",
      "    widths=0.75,\n",
      "    boxprops={'linewidth':2},\n",
      "    medianprops={'linewidth':2 },\n",
      "    whiskerprops={'linewidth':2},\n",
      "    capprops={'linewidth':2},\n",
      "    flierprops={'linewidth':2},\n",
      "    whis=[5,95])\n",
      "### add window dressing to plots\n",
      "# plt.xticks(rotation=45)\n",
      "fig.suptitle(\"\")\n",
      "ax.set_title(\"\", fontsize=14 )\n",
      "ax.set_xlabel(\"\")\n",
      "ax.set_ylabel(\"Coverage Factor\")\n",
      "ax.xaxis.grid(False)\n",
      "\n",
      "output_file = \"cf-boxplots-fs.pdf\"\n",
      "fig.savefig(output_file, bbox_inches=\"tight\")\n",
      "print \"Saved %s\" % output_file"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Also try a coverage factor histogram since boxplots don't represent the long tail very well."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = plt.figure(figsize=(8,6))\n",
      "ax = fig.add_subplot(111)\n",
      "\n",
      "x = []\n",
      "labels = []\n",
      "for fs in df_concat['darshan_file_system'].unique():\n",
      "    x.append(df_concat[df_concat[\"darshan_file_system\"]==fs][\"coverage_factor\"].reset_index(drop=True).dropna())\n",
      "    labels.append(fs) # in case the .unique() generator is nondeterministic\n",
      "    \n",
      "### retain the histogram results\n",
      "histogram = ax.hist(x,\n",
      "        bins=[0.101 * x for x in range(0,13)],\n",
      "        label=labels,\n",
      "        stacked=True\n",
      "    )\n",
      "# ax.yaxis.grid()\n",
      "ax.legend()\n",
      "ax.set_title(\"\", fontsize=14 )\n",
      "ax.set_xlabel(\"Coverage Factor\")\n",
      "ax.set_ylabel(\"Frequency\")\n",
      "fig.suptitle(\"\")\n",
      "\n",
      "output_file = \"cf-histogram-fs.pdf\"\n",
      "fig.savefig(output_file, bbox_inches=\"tight\")\n",
      "print \"Saved %s\" % output_file"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Calculate the cumulative distribution function (CDF) from the histogram data to get a probability distribution of observing a given coverage factor (CF)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in range(len(histogram[0])): # 4\n",
      "    sums = [0 for x in range(len(histogram[0][i]))]\n",
      "    for j in range(len(histogram[0][i])): # 13\n",
      "        sums[j] += histogram[0][i][j]\n",
      "probabilities = sums / sum(sums)\n",
      "cumul = 0.0\n",
      "print \"%5s %6s %10s %10s\" % (\"CF\", \"Value\", \"CDF\", \"1-CDF\")\n",
      "for idx, probability in enumerate(probabilities):\n",
      "    cumul += probability\n",
      "    print \"%5.3f %6.4f %10.4f %10.4f\" % ( histogram[1][idx], probability, cumul, 1.0 - cumul )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Cumulative Distribution Functions\n",
      "\n",
      "We now have a distribution for the coverage factor and performance.  What is the probability of getting anywhere near peak performance?  How does this vary with the coverage factor?\n",
      "\n",
      "First calculate the cumulative distribution function for keys of interest:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def calculate_cdf( values ):\n",
      "    \"\"\"Create a pandas.Series that is the CDF of a list-like object\"\"\"\n",
      "\n",
      "    denom = len(values) - 1\n",
      "    x = sorted(values / values.max())\n",
      "    y = [ i / denom for i in np.arange(len(x), dtype=np.float64) ]\n",
      "    return pandas.Series( y, index=x )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "### Calculate CDFs for (1) each file system and (2) performance and coverage factor\n",
      "cdfs = {}\n",
      "for fs in _FILE_SYSTEM_ORDER:\n",
      "    df_fs = df_concat[df_concat['darshan_file_system'] == fs]\n",
      "    for cdf_key in 'darshan_normalized_perf_by_max', 'coverage_factor':\n",
      "        if cdf_key not in cdfs:\n",
      "            cdfs[cdf_key] = {}\n",
      "        cdf = calculate_cdf(df_fs[cdf_key])\n",
      "        cdfs[cdf_key][fs] = {\n",
      "            'dependent_variable':  cdf.index,\n",
      "            'probability': cdf.values,\n",
      "        }"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### don't use counter_labels because the context is slightly different\n",
      "cdf_labels = {\n",
      "    'darshan_normalized_perf_by_max': \"Fraction Peak Performance\",\n",
      "    'coverage_factor': \"Coverage Factor\",\n",
      "}\n",
      "cdf_file_labels = {\n",
      "    'darshan_normalized_perf_by_max': \"perf\",\n",
      "    'coverage_factor': \"coverage-factor\",\n",
      "}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Plot CDF side-by-side\n",
      "\n",
      "fig, axes = plt.subplots(nrows=1, ncols=2, sharex=True, sharey=True)\n",
      "fig.set_size_inches(8,5)\n",
      "\n",
      "for fs in _FILE_SYSTEM_ORDER:\n",
      "    for idx, cdf_key in enumerate(sorted(cdfs.keys())):\n",
      "        axes[idx].plot(\n",
      "                       cdfs[cdf_key][fs]['dependent_variable'],\n",
      "                       cdfs[cdf_key][fs]['probability'],\n",
      "                       label=fs,\n",
      "                       linewidth=2)\n",
      "        axes[idx].set_xlabel(wrap(cdf_labels[cdf_key]), fontsize=14)\n",
      "        axes[idx].set_ylabel(\"\")\n",
      "        axes[idx].plot([0.0, 1.0],[0.5, 0.5], '--', linewidth=2.0, color='red')\n",
      "        axes[idx].legend().remove()\n",
      "        axes[idx].set_xticks([0.0, 0.25, 0.50, 0.75, 1.0])\n",
      "        for tick in axes[idx].xaxis.get_major_ticks():\n",
      "            tick.label.set_fontsize(16) \n",
      "        for tick in axes[idx].yaxis.get_major_ticks():\n",
      "            tick.label.set_fontsize(16)\n",
      "axes[1].xaxis.get_major_ticks()[0].set_visible(False)\n",
      "axes[0].set_title('(a)', position=(0.90,0.0), fontsize=24)\n",
      "axes[1].set_title('(b)', position=(0.90,0.0), fontsize=24)\n",
      "fig.legend( *(axes[0].get_legend_handles_labels()),\n",
      "    fontsize=14,\n",
      "               ncol=4,\n",
      "               mode=\"expand\",\n",
      "               bbox_to_anchor=(0.11, 0.0, 0.79, 1.0),\n",
      "               loc=\"upper left\",\n",
      "              borderaxespad=0.0)\n",
      "axes[0].set_ylabel(\"Cumulative Probability\", fontsize=14)\n",
      "for i in 0, 1:\n",
      "    axes[i].set_ylim([0.0, 1.0])\n",
      "    axes[i].set_xlim([0.0, 1.0])\n",
      "    axes[i].grid(True)\n",
      "    axes[i].yaxis.set_ticks([0.0,0.25,0.50,0.75,1.0])\n",
      "fig.suptitle(\"\")\n",
      "# fig.text(0.0, 0.5, \"Cumulative Probability\", va='center', rotation='vertical')\n",
      "fig.subplots_adjust(hspace=0.0, wspace=0.0)\n",
      "\n",
      "output_file = \"cdf-both.pdf\"\n",
      "fig.savefig(output_file, bbox_inches=\"tight\")\n",
      "print \"Saved %s\" % output_file"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "IOPS Coverage Factor"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = df_mira.copy()\n",
      "df['iops_coverage_factor'] = (df_mira['darshan_total_rws'] / (df_mira['ggio_read_reqs'] + df_mira['ggio_write_reqs']))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = plt.figure()\n",
      "ax = fig.add_subplot('111')\n",
      "x_counter = 'coverage_factor'\n",
      "x_counter = 'darshan_normalized_perf_by_max'\n",
      "y_counter = 'iops_coverage_factor'\n",
      "ax.plot(df[x_counter],\n",
      "        df[y_counter],\n",
      "        marker='o',\n",
      "        alpha=1.0,\n",
      "        linewidth=0.0,\n",
      "       )\n",
      "result = ax.hexbin( df[x_counter],\n",
      "            df[y_counter],\n",
      "            gridsize=15,\n",
      "            cmap='hot_r'\n",
      "       )\n",
      "\n",
      "ax.set_xlabel(x_counter)\n",
      "ax.set_ylabel(y_counter)\n",
      "ax.grid()\n",
      "print \"\"\"\n",
      "When performance is high, iops coverage factor is high (so application has exclusive access to iops)\n",
      "When performance is low, iops coverage factor is low\n",
      "\"\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### CDF\n",
      "fig = plt.figure()\n",
      "fig.set_size_inches(8,5)\n",
      "ax = fig.add_subplot(\"111\")\n",
      "ax.plot(calculate_cdf( df_mira['iops_coverage_factor'] ), label='IOPS', lw=2.0)\n",
      "ax.plot(calculate_cdf( df_mira['coverage_factor'] ), label='Bandwidth', lw=2.0)\n",
      "ax.set_xlabel(\"Coverage Factor\")\n",
      "ax.set_ylabel(\"Cumulative Probability\")\n",
      "ax.legend()\n",
      "ax.grid()\n",
      "output_file = \"cdf-cf-bw-and-ops.pdf\"\n",
      "fig.savefig(output_file, bbox_inches=\"tight\")\n",
      "print \"Saved %s\" % output_file\n",
      "\n",
      "\n",
      "### Also try bar plots\n",
      "fig = plt.figure()\n",
      "fig.set_size_inches(8,5)\n",
      "ax = fig.add_subplot(\"111\")\n",
      "y1 = df_mira['iops_coverage_factor']\n",
      "y2 = df_mira['coverage_factor']\n",
      "common_opts = {\n",
      "                \"width\": 1.0/15.0,\n",
      "                \"bins\": np.linspace(0.0, 1.0, 15),\n",
      "                'alpha': 0.75,\n",
      "                'lw': 3.0,\n",
      "#                 'zorder': 9,\n",
      "              }\n",
      "\n",
      "# ax.hist( [ y1, y2 ], label=[\"IOPS\", \"Bandwidth\"], **common_opts)\n",
      "for y, label in [ \n",
      "                 (y1, 'IOPS'),\n",
      "                 (y2, 'Bandwidth'),\n",
      "\n",
      "                ]:\n",
      "    ax.hist( y, label=label, **common_opts)\n",
      "\n",
      "ax.set_xlabel(\"Coverage Factor\")\n",
      "ax.set_ylabel(\"Frequency\")\n",
      "ax.legend()\n",
      "ax.yaxis.grid()\n",
      "output_file = \"hist-cf-bw-and-ops.pdf\"\n",
      "fig.savefig(output_file, bbox_inches=\"tight\")\n",
      "print \"Saved %s\" % output_file"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_sparse_points(x, y, hexbin)\n",
      "    \"\"\"\n",
      "    figure out points that correspond to sparse bins\n",
      "    \"\"\"\n",
      "    counters = hexbin.get_array()\n",
      "    verts = hexbin.get_offsets()\n",
      "    for offc in xrange(verts.shape[0]):\n",
      "        binx, biny = verts[offc][0],verts[offc][1]\n",
      "        if counts[offc] > 0:\n",
      "            for i x_val in x:\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}