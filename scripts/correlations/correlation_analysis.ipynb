{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Analysis of TOKIO-ABC Results\n",
    "\n",
    "This notebook performs various statistical analyses of the summary data generated by each TOKIO-ABC job.  These data are loaded from __a summary csv file__ where each row represents a single job and contains the relevant data extracted from\n",
    "\n",
    "1. the darshan log\n",
    "2. the server-side I/O monitoring (Lustre LMT or GPFS GGIOSTAT)\n",
    "3. optional system-specific monitoring including\n",
    "    - OST health info (Lustre)\n",
    "    - Concurrent job count (Slurm)\n",
    "    - Job radius (Cray XC)\n",
    "    \n",
    "This input CSV is generated at NERSC by\n",
    "\n",
    "1. running `utils/nersc_generate_job_summary.sh` to generate json summary records for each darshan log\n",
    "2. running `utils/json2csv.py` to convert all jsons into a single csv file\n",
    "\n",
    "This script is used to identify interesting patterns and correlations across jobs.  For intra-job inspection, use other analysis notebooks such as `analysis/per_ost_deep_dive.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams.update({'font.size': 18})\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "import matplotlib.gridspec\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats as stats\n",
    "import scipy.interpolate\n",
    "import json\n",
    "import datetime\n",
    "import bisect\n",
    "import warnings\n",
    "import textwrap\n",
    "\n",
    "### black magic necessary for processing Mira log files :(\n",
    "try:\n",
    "    import pytz\n",
    "    _USE_TZ = True\n",
    "except ImportError:\n",
    "    _USE_TZ = False\n",
    "\n",
    "def wrap(text, width=15):\n",
    "    \"\"\"wrapper for the wrapper\"\"\"\n",
    "    return '\\n'.join(textwrap.wrap(text=text,width=width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def utc_timestamp_to_argonne( timestamp ):\n",
    "    \"\"\"\n",
    "    This is a batty function that allows us to compare the UTC-based\n",
    "    timestamps from Darshan logs (start_time and end_time) to the\n",
    "    Chicago-based YYYY-MM-DD dates used to index the mmdf data.\n",
    "    \"\"\"\n",
    "    if _USE_TZ:\n",
    "        ### we know that these logs are from Chicago\n",
    "        tz = pytz.timezone(\"America/Chicago\")\n",
    "        \n",
    "        ### Darshan log's start time in UTC, so turn it into a datetime with UTC on it\n",
    "        darshan_time = pytz.utc.localize(datetime.datetime.utcfromtimestamp(timestamp))\n",
    "        \n",
    "        ### Then convert this UTC start time into a local start time so\n",
    "        ### we can compare it to the local mmdf timestamp\n",
    "        darshan_time_at_argonne = darshan_time.astimezone(tz)\n",
    "        return darshan_time_at_argonne\n",
    "    else:\n",
    "        ### we assume that this script is running on Argonne time; it's the best we can do\n",
    "        warnings.warn(\"pytz is not available so mmdf data might be misaligned by a day!\")\n",
    "        return datetime.datetime.fromtimestamp(timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Relative path to the repository's root directory\n",
    "_REPO_BASE_DIR = os.path.join('..', '..')\n",
    "\n",
    "### Translates cryptic counter names into something suitable for labeling plots\n",
    "counter_labels = json.load(open(os.path.join(_REPO_BASE_DIR, 'scripts', 'counter_labels.json'), 'r'))\n",
    "\n",
    "### For consistency, always plot file systems in the same order\n",
    "_FILE_SYSTEM_ORDER = [ 'scratch1', 'scratch2', 'scratch3', 'mira-fs1' ]\n",
    "\n",
    "_INPUT_EDISON_DATA_CSV = os.path.join(_REPO_BASE_DIR,\n",
    "                                      'data',\n",
    "                                      'dat',\n",
    "                                      'tokio-lustre',\n",
    "                                      'edison-abc-stats_2-14_3-28_v2.csv')\n",
    "_INPUT_MIRA_DATA_CSV = os.path.join(_REPO_BASE_DIR,\n",
    "                                    'data',\n",
    "                                    'dat',\n",
    "                                    'tokio-gpfs',\n",
    "                                    'alcf-abc-stats_2-25_3-27.dat')\n",
    "_INPUT_MIRA_MMDF_CSV = os.path.join(_REPO_BASE_DIR,\n",
    "                                    'data',\n",
    "                                    'dat',\n",
    "                                    'tokio-gpfs',\n",
    "                                    'mira_mmdf_1-25_3-27.csv')\n",
    "\n",
    "_COVERAGE_FACTOR_CUTOFF = 1.2\n",
    "\n",
    "_MIRA_JOBS_BLACKLIST = [ 1039807 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### This is a time range that encompasses data collected on both Edison and Mira\n",
    "\n",
    "# start time is inclusive\n",
    "_START_TIME = datetime.datetime(2017, 2, 24, 0, 0, 0)\n",
    "# end time is exclusive\n",
    "_END_TIME   = datetime.datetime(2017, 3, 26, 0, 0, 0)\n",
    "# _START_TIME = _END_TIME = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### All time\n",
    "\n",
    "# start time is inclusive\n",
    "_START_TIME = datetime.datetime(2016, 2, 24, 0, 0, 0)\n",
    "# end time is exclusive\n",
    "_END_TIME   = datetime.datetime(2017, 3, 26, 0, 0, 0)\n",
    "# _START_TIME = _END_TIME = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "We've been storing most of the per-job summary data in a single CSV per system.  We\n",
    "\n",
    "1. Load the CSV directly into a dataframe\n",
    "2. Drop any rows containing NANs, because if any of the core data is missing (e.g., application name), the whole record is useless.  Hopefully I haven't overlooked anything important in this assumption.\n",
    "3. Synthesize a few new columns (we call these \"metrics\" in the paper) to facilitate downstream analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Edison\n",
    "df_edison = pandas.DataFrame.from_csv(_INPUT_EDISON_DATA_CSV).dropna()\n",
    "df_edison['darshan_rw'] = [ 'write' if x == 1 else 'read' for x in df_edison['darshan_write_mode?'] ]\n",
    "df_edison['darshan_file_mode'] = [ 'shared' if x in ['H5Part','MPIIO'] else 'fpp' for x in df_edison['darshan_api'] ]\n",
    "df_edison.rename(columns={'lmt_bytes_covered': 'coverage_factor'}, inplace=True)\n",
    "df_edison['system'] = \"edison\"\n",
    "df_edison['iops_coverage_factor'] = -1.0\n",
    "df_edison['nodehr_coverage_factor'] = df_edison['job_num_nodes'] * \\\n",
    "                                      (df_edison['darshan_end_time'] - df_edison['darshan_start_time']) / 3600.0 / \\\n",
    "                                      (df_edison['job_concurrent_nodehrs'])\n",
    "\n",
    "\n",
    "### Mira\n",
    "df_mira = pandas.DataFrame.from_csv(_INPUT_MIRA_DATA_CSV).dropna()\n",
    "rename_dict = { '# platform': \"system\" }\n",
    "for key in df_mira.keys():\n",
    "    if key == 'file_sys':\n",
    "        rename_dict[key] = 'darshan_file_system'\n",
    "    elif key not in rename_dict and not key.startswith('ggio_'):\n",
    "        rename_dict[key] = 'darshan_' + key\n",
    "df_mira.rename(columns=rename_dict, inplace=True)\n",
    "df_mira['darshan_file_mode'] = [ 'shared' if x in ['H5Part','MPIIO'] else 'fpp' for x in df_mira['darshan_api'] ]\n",
    "df_mira['coverage_factor'] = df_mira['darshan_total_bytes'] / (df_mira['ggio_bytes_read'] + df_mira['ggio_bytes_written'])\n",
    "df_mira['iops_coverage_factor'] = (df_mira['darshan_total_rws'] / (df_mira['ggio_read_reqs'] + df_mira['ggio_write_reqs']))\n",
    "df_mira['nodehr_coverage_factor'] = -1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because I'm lazy, load the `mmdf` data separately and attach it to `df_mira`.  The mmdf CSV is generated by\n",
    "\n",
    "1. retrieving all of the `df_fs1_*.txt` files from `mira:/projects/radix-io/automated/runs/gpfs-logs/`\n",
    "2. running `tokio-cron-benchmarks:utils/parse_mmdf.py` script against `df_fs1_*.txt` (note that although `parse_mmdf.py` can distinguish between different file systems, this script currently doesn't filter for the correct `file_system` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_mmdf = pandas.DataFrame.from_csv(_INPUT_MIRA_MMDF_CSV, index_col=['file_system', 'date'])\n",
    "df_mmdf['free_kib'] = df_mmdf['free_kib_blocks'] + df_mmdf['free_kib_frags']\n",
    "df_mmdf['free_pct'] = df_mmdf['free_kib'] / df_mmdf['disk_size']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Walk the master dataframe and attach mmdf data.  Note that we're injecting NAs for missing mmdf data because missing mmdf data should not exclude the entire day from our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### I really hope iterrows behaves deterministically and preserves order...\n",
    "new_data = {\n",
    "    'mmdf_avg_fullness_pct': [],\n",
    "    'mmdf_max_fullness_pct': [],\n",
    "}\n",
    "\n",
    "### iterate over each row of the master Mira dataframe\n",
    "no_data = set([])\n",
    "for row in df_mira.itertuples():\n",
    "    fs_key = row.darshan_file_system\n",
    "    mmdf_key = utc_timestamp_to_argonne( row.darshan_start_time ).strftime(\"%Y-%m-%d\")\n",
    "    if mmdf_key in df_mmdf.loc[fs_key].index:\n",
    "        ### only look at today's data\n",
    "        df = df_mmdf.loc[fs_key].loc[mmdf_key]\n",
    "        \n",
    "        data_cols = [ True if x else False for x in df['data?'] ]\n",
    "\n",
    "        ### calculate a percent fullness - don't bother saving the id of this fullest server though\n",
    "        new_data['mmdf_max_fullness_pct'].append( 1.0 - df[ data_cols ]['free_pct'].min() )\n",
    "        new_data['mmdf_avg_fullness_pct'].append( 1.0 - df[ data_cols ]['free_pct'].mean() )\n",
    "    else:\n",
    "        no_data.add( datetime.datetime.fromtimestamp(row.darshan_start_time).strftime(\"%Y-%m-%d\") )\n",
    "        new_data['mmdf_max_fullness_pct'].append( np.nan )\n",
    "        new_data['mmdf_avg_fullness_pct'].append( np.nan )\n",
    "\n",
    "warnings.warn(\"No MMDF data found for the following dates:\\n\" + '\\n'.join(no_data))\n",
    "        \n",
    "for new_col_name, new_col_data in new_data.iteritems():\n",
    "    df_mira[new_col_name] = new_col_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now merge both DataFrames so we can look at all the data if we really want to.  This DataFrame will have a bunch of NANs for data that is only applicable to Mira or Edison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_concat = pandas.concat( (df_mira, df_edison) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Data\n",
    "\n",
    "Two notable filters are applied:\n",
    "\n",
    "1. All jobs where the bytes coverage factor and ops coverage factor are greater than 1.2 are discarded because they reflect severely misaligned or gappy data.\n",
    "\n",
    "2. Mira job 1039807 is excluded because ggiostat returned highly abnormal results starting that day.  See e-mail from Shane and Phil on March 23 about this.\n",
    "\n",
    "3. All Edison jobs from March 12 were discarded because LMT broke as a result of daylight saving time rolling over.  This filter was applied _before_ the input CSV files loaded above were generated, so it does not need to be applied here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for df in df_mira, df_edison, df_concat:\n",
    "    print '+'.join( df['system'].unique() )\n",
    "    print ''.join([ '=' * 50])\n",
    "    \n",
    "    count = len(df)\n",
    "    df.drop(df.index[df['coverage_factor'] > _COVERAGE_FACTOR_CUTOFF], inplace=True)\n",
    "    print \"Dropped %d records due to coverage factor > %.1f\" % (count - len(df), _COVERAGE_FACTOR_CUTOFF)\n",
    "    \n",
    "    count = len(df)\n",
    "    for blacklisted_job in _MIRA_JOBS_BLACKLIST:\n",
    "        df.drop(df.index[(df['system'] == 'mira') & (df['darshan_jobid'] == blacklisted_job)], inplace=True)\n",
    "    print \"Dropped %d records due to Mira job #1039807\" % (count - len(df))\n",
    "\n",
    "    count = len(df)\n",
    "    if _END_TIME is not None and _START_TIME is not None:\n",
    "        filter_time = [ datetime.datetime.fromtimestamp(df['darshan_start_time'][x]) < _START_TIME                    \n",
    "                        or\n",
    "                        datetime.datetime.fromtimestamp(df['darshan_start_time'][x]) > _END_TIME\n",
    "                        for x in df.index ]\n",
    "        df.drop(df.index[filter_time], inplace=True)\n",
    "    print \"Dropped %d records outside of time range %s - %s\" % (count - len(df), _START_TIME, _END_TIME)\n",
    "    print \"Total measurements to be analyzed:\", len(df)\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for df in df_mira, df_edison, df_concat:\n",
    "    print '+'.join( df['system'].unique() )\n",
    "    print ''.join([ '=' * 50])\n",
    "    print \"Earliest timestamp now\", datetime.datetime.fromtimestamp(df['darshan_start_time'].min())\n",
    "    print \"Latest timestamp now\", datetime.datetime.fromtimestamp(df['darshan_start_time'].max())\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize Performance\n",
    "Different file systems, benchmarks, and read/write modes are capable of different peak bandwidths.  As such, we want to normalize the absolute performance (`summarize_key`) by something.  For convenience we calculate the denominator for normalization a couple of different ways (e.g., the mean, median, and max measurement).  We also limit normalization to unique combinations of variables specified by `normalization_group` below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the normalization factors (the denominators), then apply that factor to all of the data in the DataFrame.  These normalized data will be saved as new columns in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### The actual variable we want to normalize\n",
    "summarize_key = 'darshan_agg_perf_by_slowest'\n",
    "\n",
    "for df in df_edison, df_mira, df_concat:\n",
    "    ### Specify which keys we want to group together before normalizing\n",
    "    normalization_group = df.groupby(['darshan_app', 'darshan_file_system', 'darshan_file_mode', 'darshan_rw'])\n",
    "\n",
    "    ### Dict to store the denominators for normalization\n",
    "    normalization_data = {\n",
    "        'mean':   normalization_group.darshan_agg_perf_by_slowest.mean(),\n",
    "        'median': normalization_group.darshan_agg_perf_by_slowest.median(),\n",
    "        'max':    normalization_group.darshan_agg_perf_by_slowest.max(),\n",
    "    }\n",
    "\n",
    "    ### Normalize every row in the DataFrame by all of our denominators\n",
    "    new_cols = {}\n",
    "    for func in normalization_data.keys():\n",
    "        new_col_key = 'darshan_normalized_perf_by_%s' % func\n",
    "        new_cols[new_col_key] = []\n",
    "        for index, row in df.iterrows():\n",
    "            new_cols[new_col_key].append(\n",
    "                row[summarize_key] / normalization_data[func]\n",
    "                                                       [row['darshan_app']]\n",
    "                                                       [row['darshan_file_system']]\n",
    "                                                       [row['darshan_file_mode']]\n",
    "                                                       [row['darshan_rw']]\n",
    "            )\n",
    "\n",
    "    ### Also just do per-file system\n",
    "    normalization_group = df.groupby('darshan_file_system')\n",
    "    normalization_data = {\n",
    "        'mean':   normalization_group.darshan_agg_perf_by_slowest.mean(),\n",
    "        'median': normalization_group.darshan_agg_perf_by_slowest.median(),\n",
    "        'max':    normalization_group.darshan_agg_perf_by_slowest.max(),\n",
    "    }\n",
    "    for func in normalization_data.keys():\n",
    "        new_col_key = 'darshan_normalized_perf_by_fs_%s' % func\n",
    "        new_cols[new_col_key] = []\n",
    "        for index, row in df.iterrows():\n",
    "            new_cols[new_col_key].append(\n",
    "                row[summarize_key] / normalization_data[func][row['darshan_file_system']])\n",
    "\n",
    "    ### Take our normalized data and add them as new columns\n",
    "    for new_col, new_col_data in new_cols.iteritems():\n",
    "        df[new_col] = new_col_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Correlation Analysis\n",
    "`performance_key` is the variable we wish to use to represent performance.  It is typically\n",
    "\n",
    "- `darshan_agg_perf_by_slowest`, which is the absolute performance measured by each benchmark run\n",
    "- `darshan_normalized_perf_by_max`, which is normalized by the maximum observed performance\n",
    "- `darshan_normalized_perf_by_mean`, which is normalized by the mean observed performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "performance_key = 'darshan_normalized_perf_by_max'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Correlation Analysis\n",
    "\n",
    "Pearson analysis assumes that each variable is normally distributed.  It is easier to understand, but it is not technically correct for variables that are _not_ normally distributed, which include performance.  The Spearman coefficient would be better and can be enabled below.\n",
    "\n",
    "We use `scipy.stats` to calculate p-values associated with each correlation.  The ultimate artifact of this process is a table of interesting correlations, their correlation coefficients, and color coding to indicate the confidence of those coefficients based on p-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ignore_cols = [\n",
    "    'lmt_tot_zeros',\n",
    "    'lmt_frac_zeros',\n",
    "    'lmt_frac_missing',\n",
    "    'ost_avg_kib',\n",
    "    'ost_min_pct',\n",
    "    'ost_min_kib',\n",
    "    'ost_max_kib',\n",
    "    'ost_count',\n",
    "#   'ost_bad_ost_count',\n",
    "    'ost_bad_ost_pct',\n",
    "    'ost_failures_lead_secs',\n",
    "    'ost_fullness_lead_secs',\n",
    "    'lmt_tot_missing',\n",
    "    'ost_avg_bad_ost_per_oss',\n",
    "    'ost_avg_bad_overload_factor',\n",
    "    'ost_bad_oss_count',\n",
    "    'ost_min_id',\n",
    "    'ost_max_id',\n",
    "    'job_min_radius',\n",
    "    'job_avg_radius',\n",
    "### second pass\n",
    "    'lmt_ops_getattrs',\n",
    "    'lmt_ops_getxattrs',\n",
    "    'lmt_ops_rmdirs',\n",
    "    'lmt_ops_unlinks',\n",
    "    'lmt_ops_renames',\n",
    "    'lmt_ops_setattrs',\n",
    "    'lmt_ops_mkdirs',\n",
    "    'ggio_inoded_updates',\n",
    "### third pass\n",
    "    \"lmt_mds_ave\",\n",
    "    \"lmt_oss_ave\",\n",
    "]\n",
    "\n",
    "### if one key has the same logical meaning as another, this will remap those\n",
    "### keys so they line up in the DataFrame\n",
    "equivalent_keys = {\n",
    "    'ggio_closes':     'lmt_ops_closes',\n",
    "    'ggio_opens': 'lmt_ops_opens',\n",
    "    'ggio_bytes_read': 'lmt_tot_bytes_read',\n",
    "    'ggio_bytes_written': 'lmt_tot_bytes_write',\n",
    "    'mmdf_max_fullness_pct': 'ost_max_pct',\n",
    "    'mmdf_avg_fullness_pct': 'ost_avg_pct',\n",
    "}\n",
    "\n",
    "### Specific names for the table\n",
    "counter_labels_table = {\n",
    "    'coverage_factor': \"Coverage Factor (Bandwidth)\",\n",
    "    'nodehr_coverage_factor': \"Coverage Factor (NodeHrs)\",\n",
    "    \"ost_avg_pct\": \"Avg LUN Fullness\",\n",
    "    \"ost_max_pct\": \"Fullness on Fullest LUN\",\n",
    "    \"lmt_oss_max\": \"Max CPU Load, Data Server\",\n",
    "    \"ost_bad_pct\": \"% Servers Failed Over\",\n",
    "    \"ost_bad_ost_count\": \"Failed-over Servers\",\n",
    "    \"lmt_ops_closes\": \"close(2) Calls\",\n",
    "    \"lmt_ops_opens\": \"open(2) Calls\",\n",
    "    \"lmt_tot_bytes_write\": \"Bytes Written\",\n",
    "    \"lmt_tot_bytes_read\": \"Bytes Read\",\n",
    "    \"lmt_mds_max\": \"Max CPU Load, Metadata Server\",\n",
    "    \"lmt_mds_ave\": \"Avg CPU Load, Metadata Server\",\n",
    "    \"job_concurrent_jobs\": \"# Concurrent Jobs\",\n",
    "    \"lmt_oss_ave\": \"Avg CPU Load, Data Server\",\n",
    "    \"job_max_radius\": \"Job Diameter\",\n",
    "    \"iops_coverage_factor\": \"Coverage Factor (IOPs)\",\n",
    "    \"ggio_write_reqs\": \"Write Ops\",\n",
    "    \"ggio_read_reqs\": \"Read Ops\",\n",
    "    \"ggio_read_dirs\": \"readdir(3) Calls\",\n",
    "}\n",
    "\n",
    "### Order in which table is to be printed\n",
    "print_order = [\n",
    "    'coverage_factor',\n",
    "    'nodehr_coverage_factor',\n",
    "    \"iops_coverage_factor\",\n",
    "    \"lmt_ops_closes\",\n",
    "    \"lmt_ops_opens\",\n",
    "#   \"ggio_read_dirs\",\n",
    "    \"lmt_tot_bytes_write\",\n",
    "    \"lmt_tot_bytes_read\",\n",
    "    \"ggio_write_reqs\",\n",
    "    \"ggio_read_reqs\",\n",
    "    \"ost_max_pct\",\n",
    "    \"ost_avg_pct\",\n",
    "    \"lmt_mds_max\",\n",
    "    \"lmt_oss_max\",\n",
    "    \"ost_bad_ost_count\",\n",
    "    \"job_concurrent_jobs\",\n",
    "#   \"job_max_radius\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def correlation_calculation(df,\n",
    "                            analysis_func=stats.pearsonr,\n",
    "                            only_print_key=performance_key,\n",
    "                            ignore_cols=[],\n",
    "                            max_pval=1.01):\n",
    "    \"\"\"\n",
    "    Calculate the Pearson correlation coefficient and the associated\n",
    "    p-value for various counter pairs.  If only_print_key is None,\n",
    "    every possible combination of columns in df is attempted; otherwise,\n",
    "    the column identified by only_print_key is compared against all other\n",
    "    columns.\n",
    "    \"\"\"\n",
    "    num_cols = len(df.keys())\n",
    "    results = []\n",
    "    \n",
    "    if only_print_key is None:\n",
    "        i_range = range(num_cols - 1)\n",
    "    else:\n",
    "        i_range = [ list(df.columns).index(only_print_key) ]\n",
    "\n",
    "    for i in i_range:\n",
    "        i_col = df.columns[i]\n",
    "        if only_print_key is None:\n",
    "            j_range = range(i, num_cols)\n",
    "        else:\n",
    "            j_range = range(len(df.columns))\n",
    "            j_range.remove(i_range[0]) # degenerate case\n",
    "        for j in j_range:\n",
    "            j_col = df.columns[j]\n",
    "            try:\n",
    "                ### The Scipy stats package barfs if x or y contain any\n",
    "                ### NANs, but we don't want to drop all records that\n",
    "                ### contain any nans.  So, we wait until the very last\n",
    "                ### minute to drop only those columns that contain nans\n",
    "                ### that we would otherwise try to correlate.\n",
    "                df_corr = df[[i_col, j_col]].dropna()\n",
    "                coeff, pval = analysis_func(df_corr[i_col],\n",
    "                                            df_corr[j_col])\n",
    "            except TypeError: # non-numeric column\n",
    "                continue\n",
    "            results.append((i_col,\n",
    "                            j_col,\n",
    "                            coeff,\n",
    "                            pval))\n",
    "\n",
    "    ### now start dropping correlations that we don't want/need\n",
    "    sorted_results = sorted(results, key=lambda x: x[3])\n",
    "    ret_results = []\n",
    "    for col_name1, col_name2, coeff, pval in sorted_results:\n",
    "        ### don't print trivial relationships\n",
    "        if pval == 0 or pval == 1:\n",
    "            continue\n",
    "        ### don't print relationships with very high p-values\n",
    "        if pval > max_pval:\n",
    "            continue\n",
    "        ### don't correlate data from the same source since much of it is degenerate\n",
    "        if col_name1.split('_',1)[0] == col_name2.split('_',1)[0]:\n",
    "            continue\n",
    "        ### don't print anything except for the key of interest (if provided)\n",
    "        if only_print_key is not None \\\n",
    "        and col_name1 != only_print_key \\\n",
    "        and col_name2 != only_print_key:\n",
    "            continue\n",
    "        if col_name1 in ignore_cols or col_name2 in ignore_cols:\n",
    "            continue\n",
    "#       print \"%10.4f %10.4g %30s : %-15s\" % (coeff, pval, col_name1, col_name2)\n",
    "        \n",
    "        ### sort the output key orders\n",
    "        if col_name1 == only_print_key:\n",
    "            ret_results.append([col_name1,col_name2,coeff,pval])\n",
    "        elif col_name2 == only_print_key:\n",
    "            ret_results.append([col_name2,col_name1,coeff,pval])\n",
    "        else:\n",
    "            if col_name2 > col_name1:\n",
    "                ret_results.append([col_name1,col_name2,coeff,pval])\n",
    "            else:\n",
    "                ret_results.append([col_name2,col_name1,coeff,pval])\n",
    "    return ret_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def correlation_dict_to_dataframe(correlations):\n",
    "    ### to_df: dict we will use to store arrays -> pd.Series\n",
    "    to_df = {}\n",
    "\n",
    "    ### common_key: needed to figure out which key is the independent variable\n",
    "    common_key = None\n",
    "\n",
    "    ### list that will become dataframe index\n",
    "    key_index = []\n",
    "\n",
    "    ### loop over all systems, all correlations, all pairs of variables\n",
    "    for system, records in correlations.iteritems():\n",
    "        for row in records:\n",
    "            ### try to figure out which of the two keys is not repeated\n",
    "            if common_key is None:\n",
    "                common_key = row[0]\n",
    "            if row[0] != common_key:\n",
    "                common_key = row[1]\n",
    "                unique_key = row[0]\n",
    "            else:\n",
    "                unique_key = row[1]\n",
    "\n",
    "            ### convert compatible key names from ggio_ to lmt_\n",
    "            if unique_key in equivalent_keys:\n",
    "                unique_key = equivalent_keys[unique_key]\n",
    "\n",
    "            ### build up a list of keys that will be our Index\n",
    "            if unique_key not in key_index:\n",
    "                key_index.append(unique_key)\n",
    "\n",
    "            ### fill out the dict \n",
    "            counterkey = \"counter_%s\" % system\n",
    "            corrkey = 'correlation_%s' % system\n",
    "            pvalkey = 'p-value_%s' % system\n",
    "            if counterkey not in to_df: to_df[counterkey] = []\n",
    "            if corrkey not in to_df: to_df[corrkey] = []\n",
    "            if pvalkey not in to_df: to_df[pvalkey] = []\n",
    "            to_df[counterkey].append(unique_key)\n",
    "            to_df[corrkey].append(row[2])\n",
    "            to_df[pvalkey].append(row[3])\n",
    "\n",
    "    ### Make an empty but indexed data frame\n",
    "    df = pandas.DataFrame(index=key_index)\n",
    "    for system in correlations.keys():\n",
    "        counterkey = \"counter_%s\" % system\n",
    "        corrkey = 'correlation_%s' % system\n",
    "        pvalkey = 'p-value_%s' % system\n",
    "\n",
    "        df[corrkey] = pandas.Series(to_df[corrkey], index=to_df[counterkey])\n",
    "        df[pvalkey] = pandas.Series(to_df[pvalkey], index=to_df[counterkey])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Calculate unconstrainted correlation table just for our edification\n",
    "### Calculate correlations between performance and everything else\n",
    "correlations = {}\n",
    "\n",
    "correlations['edison'] = correlation_calculation(df_edison)\n",
    "correlations['mira'] = correlation_calculation(df_mira)\n",
    "\n",
    "correlation_dict_to_dataframe( correlations )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Calculate correlations between a subset of interesting measurements\n",
    "### so we can generate the correlation table\n",
    "correlations = {}\n",
    "\n",
    "correlations['edison'] = correlation_calculation(df_edison, ignore_cols=ignore_cols)\n",
    "correlations['mira'] = correlation_calculation(df_mira, ignore_cols=ignore_cols)\n",
    "\n",
    "df = correlation_dict_to_dataframe( correlations )\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4,8))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "column_headers = {\n",
    "    \"correlation_edison\": \"Coefficient\\n(Edison)\",\n",
    "    \"correlation_mira\": \"Coefficient\\n(Mira)\",\n",
    "    \"p-value_edison\": \"P-value (Edison)\",\n",
    "    \"p-value_mira\": \"P-value (Mira)\",\n",
    "}\n",
    "\n",
    "coefficient_keys = [x for x in df.columns if x.startswith('correlation_')]\n",
    "\n",
    "### the index is column -1\n",
    "table = pandas.tools.plotting.table(ax,\n",
    "                            df[coefficient_keys].reindex(print_order),\n",
    "                            loc='upper right',\n",
    "                            colWidths=[0.8,0.8,3.8],\n",
    "                            bbox=[0, 0, 1, 1])\n",
    "table.set_fontsize(14)\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "### Rewrite the contents of the table that Pandas gave us\n",
    "cells_dict = table.get_celld()\n",
    "remap_values = {}\n",
    "for cell_pos, cell_obj in cells_dict.iteritems():\n",
    "    i, j = cell_pos\n",
    "    value = cell_obj.get_text().get_text()\n",
    "    height_scale = 1.0\n",
    "    if i == 0:    # column headers\n",
    "        remap_values[cell_pos] = column_headers.get(value, value)\n",
    "        height_scale = 2.0\n",
    "    elif j == -1: # index cell\n",
    "        remap_values[cell_pos] = counter_labels_table.get(value, value)\n",
    "        cell_obj._loc = 'right'\n",
    "    else:         # coefficient cell\n",
    "        index = cells_dict[(i,-1)].get_text().get_text()\n",
    "        column = cells_dict[(0,j)].get_text().get_text()\n",
    "        cell_obj._loc = 'center'\n",
    "        \n",
    "        ### need a special handler for ost_bad_ost_count because Pearson\n",
    "        ### will correlate against it even though it is invariant :(\n",
    "        ### TODO: fix it so that we don't try to correlate against constants\n",
    "        if index == 'ost_bad_ost_count':\n",
    "            remap_values[cell_pos] = \"N/A\"\n",
    "            cell_obj.set_color('white')\n",
    "#           cell_obj._loc = 'center'\n",
    "        elif value == \"nan\":\n",
    "            cell_obj.set_color('grey')\n",
    "            remap_values[cell_pos] = \"\"\n",
    "            cell_obj.set_alpha(0.25)\n",
    "#           cell_obj._loc = 'center'\n",
    "        else:\n",
    "            coeff = float(value)\n",
    "            pval = df.loc[index][column.replace('correlation','p-value')]\n",
    "\n",
    "            ### make moderate correlations **bold**\n",
    "            if abs(coeff) >= 0.30:\n",
    "                cell_obj.get_text().set_fontweight('bold')\n",
    "            elif abs(coeff) < 0.10:\n",
    "                cell_obj.get_text().set_fontstyle('italic')\n",
    "\n",
    "            ### color code cells based on p-value\n",
    "            if pval < 0.01:\n",
    "                set_color = 'blue'\n",
    "            elif pval < 0.05:\n",
    "                set_color = 'green'\n",
    "            else:\n",
    "                set_color = 'red'\n",
    "            \n",
    "            ### for debugging, since the resulting figure doesn't contain any p-values\n",
    "            print \"%30s pval=%10.4f; setting color to %s\" % (\n",
    "                index, pval, set_color\n",
    "            )\n",
    "            cell_obj.set_color(set_color)\n",
    "            cell_obj.set_alpha(0.25)\n",
    "#           cell_obj._loc = 'center'\n",
    "            remap_values[cell_pos] = \"%+.4f\" % coeff\n",
    "    cell_obj.set_height(height_scale * cell_obj.get_height())\n",
    "    cell_obj.set_edgecolor('black')\n",
    "\n",
    "### Actually rewrite the cells now\n",
    "for cell_pos, new_value in remap_values.iteritems():\n",
    "    cells_dict[cell_pos].get_text().set_text(new_value)\n",
    "    \n",
    "output_file = \"correlation_table.pdf\"\n",
    "fig.savefig(output_file, bbox_inches=\"tight\")\n",
    "print \"Saved %s\" % output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter Plots\n",
    "To visualize these correlations, we define pairs of counters to plot against each other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scatterplots = [ \n",
    "    ('edison', performance_key, 'coverage_factor'),\n",
    "    ('mira', performance_key, 'coverage_factor'),\n",
    "    ('mira', performance_key, 'iops_coverage_factor'),\n",
    "    ('edison', performance_key, 'lmt_oss_max'),\n",
    "    ('edison', performance_key, 'job_concurrent_jobs'),\n",
    "    ('edison', performance_key, 'ost_avg_pct'),\n",
    "    ('edison', performance_key, 'ost_max_kib'),\n",
    "    ('mira', performance_key, 'ggio_write_reqs'),\n",
    "    ('mira', performance_key, 'ggio_read_reqs'),\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and then plot them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "blacklist = set([\n",
    "    'ost_max_id', 'ost_min_id', 'lmt_tot_zeros',\n",
    "    'ost_failures_lead_secs', 'ost_fullness_lead_secs',\n",
    "    'ost_max_kib', 'ost_avg_kib', 'ost_min_kib', \n",
    "    'lmt_tot_missing'\n",
    "])\n",
    "# for scatterplot in correlations_edison + correlations_mira:\n",
    "for system, x_key, y_key in scatterplots:\n",
    "    if x_key in blacklist or y_key in blacklist:\n",
    "        continue\n",
    "    if system == \"edison\":\n",
    "        df_plot = df_edison\n",
    "        system = \"Edison\"\n",
    "    elif system == \"mira\":\n",
    "        df_plot = df_mira\n",
    "        system = \"Mira\"\n",
    "    else:\n",
    "        warnings.warn(\"Cannot do not understand system \" + system)\n",
    "        continue\n",
    "        \n",
    "    fig = plt.figure(figsize=(6,4))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    x = df_plot[x_key].values\n",
    "    x_label = counter_labels.get(x_key, x_key)\n",
    "    y = df_plot[y_key].values\n",
    "    y_label = counter_labels.get(y_key, y_key)\n",
    "### tmp\n",
    "    cutoff_time = datetime.datetime(2017, 3, 13, 0, 0, 0)\n",
    "    df_plot['tmp'] = [datetime.datetime.fromtimestamp(df_plot['darshan_start_time'][i]) for i in df_plot.index]\n",
    "    x1 = df_plot[df_plot['tmp'] < cutoff_time][x_key].values\n",
    "    y1 = df_plot[df_plot['tmp'] < cutoff_time][y_key].values\n",
    "    x2 = df_plot[df_plot['tmp'] >= cutoff_time][x_key].values\n",
    "    y2 = df_plot[df_plot['tmp'] >= cutoff_time][y_key].values\n",
    "### /tmp\n",
    "    \n",
    "    points_old = ax.plot(x1, y1, 'o', alpha=0.5)\n",
    "    points_new = ax.plot(x2, y2, 'o', alpha=0.5)\n",
    "\n",
    "    ### attempt a linear fit to generate a visual aid\n",
    "    m, b = np.polyfit(x1, y1, 1)\n",
    "    ax.plot(x, m*x+b, \"-\", color=points_old[0].get_color())\n",
    "    \n",
    "    m, b = np.polyfit(x, y, 1)\n",
    "    ax.plot(x, m*x+b, \"-\", color=points_new[0].get_color())\n",
    "    \n",
    "    ### add window dressing to plots\n",
    "#   fig.suptitle('Correlation between %s and %s' \n",
    "#                 % (x_label.split('(',1)[0].strip(),\n",
    "#                    y_label.split('(',1)[0].strip()))\n",
    "    ax.set_title(\"Coefficient=%.4f, P-value=%.2g (%s)\" \n",
    "                    % sum((stats.pearsonr(x1, y1), (system.title(),)), ()), fontsize=14 )\n",
    "    ax.set_xlabel(x_label)\n",
    "    ax.set_ylabel(y_label)\n",
    "    plt.grid(True)\n",
    "    output_file = \"scatter_%s_vs_%s.pdf\" % (x_key, y_key)\n",
    "    fig.savefig(output_file, bbox_inches=\"tight\")\n",
    "    print \"Saved %s\" % output_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mira also has both server-side and client side IOPs.  Let's look at those specifically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,4))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "df = df_mira.sort_values(performance_key).copy()\n",
    "\n",
    "for x_key, y_key in [(performance_key, 'ggio_write_reqs'),\n",
    "                    (performance_key, 'ggio_read_reqs')]:\n",
    "    x = df[x_key].values\n",
    "    y = df[y_key].values / df[y_key].max()\n",
    "    corr = stats.pearsonr(x, y)\n",
    "    print counter_labels.get(y_key, y_key), corr[0], corr[1]\n",
    "    points = ax.plot(x, y,\n",
    "                     'o',\n",
    "                     alpha=0.4,\n",
    "                     markersize=6.0,\n",
    "                     label=\"%s\" % (counter_labels.get(y_key, y_key))\n",
    "                    )\n",
    "        \n",
    "    ### attempt a linear fit to generate a visual aid\n",
    "    m, b = np.polyfit(x, y, 1)\n",
    "    ax.plot(x, m*x+b,\n",
    "            \"-\",\n",
    "           color=points[0].get_color())\n",
    "    \n",
    "ax.set_xlabel(counter_labels.get(x_key, x_key))\n",
    "ax.set_ylabel(\"Fraction Peak Ops\")\n",
    "ax.legend()\n",
    "plt.grid(True)\n",
    "output_file = \"scatter_mira_ops.pdf\"\n",
    "fig.savefig(output_file, bbox_inches=\"tight\")\n",
    "print \"Saved %s\" % output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,4))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "df = df_mira.sort_values(performance_key).copy()\n",
    "\n",
    "for x_key, y_key in [('coverage_factor', 'iops_coverage_factor')]:\n",
    "    x = df[x_key].values\n",
    "    y = df[y_key].values / df[y_key].max()\n",
    "    corr = stats.pearsonr(x, y)\n",
    "    print counter_labels.get(y_key, y_key), corr[0], corr[1]\n",
    "    points = ax.plot(x, y,\n",
    "                     'o',\n",
    "                     alpha=0.4,\n",
    "                     markersize=6.0,\n",
    "                     label=\"%s\" % (counter_labels.get(y_key, y_key))\n",
    "                    )\n",
    "        \n",
    "    ### attempt a linear fit to generate a visual aid\n",
    "    m, b = np.polyfit(x, y, 1)\n",
    "    ax.plot(x, m*x+b,\n",
    "            \"-\",\n",
    "           color=points[0].get_color())\n",
    "    \n",
    "ax.set_xlabel(counter_labels.get(x_key, x_key))\n",
    "ax.set_ylabel(\"Fraction Peak Ops\")\n",
    "ax.legend()\n",
    "plt.grid(True)\n",
    "output_file = \"scatter_mira_ops.pdf\"\n",
    "fig.savefig(output_file, bbox_inches=\"tight\")\n",
    "print \"Saved %s\" % output_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the correlation between coverage factor and performance for each file system separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for scatterplot in correlations_mira:\n",
    "for idx, fs in enumerate(_FILE_SYSTEM_ORDER):\n",
    "    fig = plt.figure(figsize=(8,6))\n",
    "    ax = fig.add_subplot(111)\n",
    "    x_key = performance_key\n",
    "    y_key = 'coverage_factor'\n",
    "    if fs.startswith('mira') :\n",
    "        df_plot = df_mira.sort_values(performance_key)\n",
    "        system = \"Mira\"\n",
    "    elif fs.startswith('scratch'):\n",
    "        df_plot = df_edison.sort_values(performance_key)\n",
    "        system = \"Edison\"\n",
    "    else:\n",
    "        warnings.warn(\"Cannot find key %s in any data frames\" % y_key)\n",
    "    df_plot = df_plot[df_plot['darshan_file_system'] == fs]\n",
    "    \n",
    "    x = df_plot[x_key].values\n",
    "    y = df_plot[y_key].values\n",
    "    corr = stats.pearsonr(x, y)\n",
    "    label = \"%s (corr=%.2f, p-val=%4.2g, n=%d)\" % (fs, corr[0], corr[1], len(df_plot))\n",
    "    points = ax.plot(x, y,\n",
    "                     'o', \n",
    "                     alpha=0.75,\n",
    "                     markersize=4.0)\n",
    "\n",
    "    ### attempt a linear fit to generate a visual aid\n",
    "    m, b = np.polyfit(x, y, 1)\n",
    "    ax.plot(x, m*x+b,\n",
    "            \"-\",\n",
    "           color=points[0].get_color())\n",
    "    \n",
    "    ### make the plot pretty\n",
    "    ax.set_xlim([0.0,2.0 if 'mean' in x_key else 1.0])\n",
    "    ax.set_ylabel(\"Coverage Factor\")\n",
    "    ax.set_xlabel(counter_labels[performance_key])\n",
    "    ax.set_title(label, fontsize=14)\n",
    "    plt.grid(True)\n",
    "    output_file = \"scatter_perf-vs-cf_%s.pdf\" % fs\n",
    "    fig.savefig(output_file, bbox_inches=\"tight\")\n",
    "    print \"Saved %s\" % output_file\n",
    "    fig.savefig(output_file.replace('pdf', 'png'), bbox_inches=\"tight\")\n",
    "    print \"Saved %s\" % output_file\n",
    "    df_save = pandas.DataFrame({\"Performance Relative to Mean\": x,\n",
    "                                \"Coverage Factor\": y}).to_csv(output_file.replace('pdf', 'csv'))\n",
    "    print \"Saved\", output_file.replace('pdf', 'csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of each benchmark type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following cell defines which variable we wish to aggregate into boxplots and a few plotting parameters that depend on our choice of variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boxplot_settings = {\n",
    "    'fontsize': 15,\n",
    "    'darshan_normalized_perf_by_fs_max': {\n",
    "        'output_file': \"perf-boxplots-per-fs.pdf\",\n",
    "        'ylabel': \"Fraction of Peak\\nFile System Performance\",\n",
    "        'title_pos': [ \n",
    "            {'x': 0.97, 'y': 0.80, 'horizontalalignment': 'right', 'fontsize': 14},\n",
    "            {'x': 0.04, 'y': 0.02, 'horizontalalignment': 'left', 'fontsize': 14}]\n",
    "    },\n",
    "    'darshan_normalized_perf_by_max': {\n",
    "        'output_file': \"perf-boxplots.pdf\",\n",
    "        'ylabel': \"Fraction of Peak\\nPer-Benchmark Performance\",\n",
    "        'title_pos': [ \n",
    "            {'x': 0.04, 'y': 0.02, 'horizontalalignment': 'left', 'fontsize': 14},\n",
    "            {'x': 0.04, 'y': 0.02, 'horizontalalignment': 'left', 'fontsize': 14}]\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot two sets of boxplots based on the normalization denominator:\n",
    "\n",
    "1. normalized by the maximum without any grouping (other than file system)\n",
    "2. normalized by the maximum of each unique combination of app-read/write-filemode\n",
    "\n",
    "The idea is to show that\n",
    "\n",
    "1. performance variation varies across different file systems and different applications\n",
    "2. even within an application, the magnitude of performance variation varies with file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for plot_variable in performance_key, performance_key.replace('_by_', '_by_fs_'):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=2, sharex=True, sharey=True)\n",
    "    fig.set_size_inches(8,6)\n",
    "    boxplot_group_by = [ 'darshan_file_mode', 'darshan_rw', 'darshan_app' ]\n",
    "    for idx, fs in enumerate(_FILE_SYSTEM_ORDER):\n",
    "        icol = idx / 2\n",
    "        irow = idx % 2\n",
    "        ax = axes[irow, icol]\n",
    "        df_concat.loc[df_concat[\"darshan_file_system\"] == fs]\\\n",
    "        .boxplot(\n",
    "            column=[plot_variable],\n",
    "            by=boxplot_group_by,\n",
    "            ax=axes[irow, icol],\n",
    "            medianprops={'linewidth':2 },\n",
    "            widths=0.75,\n",
    "            whis=[5,95])\n",
    "\n",
    "        settings = boxplot_settings[plot_variable]['title_pos'][irow]\n",
    "        title = ax.set_title(fs, **(settings))\n",
    "        title.set_bbox({'color': 'white', 'alpha': 0.5})\n",
    "        ax.set_xlabel(\"\")\n",
    "        ax.set_ylabel(\"\")\n",
    "        ax.xaxis.grid(True)\n",
    "\n",
    "        ### relabel the x axis labels\n",
    "        new_labels = []\n",
    "        for axis_label in ax.get_xticklabels():\n",
    "            current_label = axis_label.get_text()\n",
    "            axis_label.set_rotation(90)\n",
    "            if \"IOR\" in current_label:\n",
    "                if \"shared\" in current_label:\n",
    "                    new_label = \"IOR/shared\"\n",
    "                else:\n",
    "                    new_label = \"IOR/fpp\"\n",
    "            elif 'BD-CATS' in current_label:\n",
    "                new_label = \"BD-CATS\"\n",
    "            else:\n",
    "                new_label = current_label.split(',')[2].strip(')').strip().split('-')[0]\n",
    "            if 'write' in current_label:\n",
    "                new_label += \" Write\"\n",
    "            else:\n",
    "                new_label += \" Read\"\n",
    "            new_labels.append(new_label)\n",
    "\n",
    "        ### set x tick labels for only the bottom row\n",
    "        if irow == 0:\n",
    "            ax.set_xticklabels([])\n",
    "        else:\n",
    "            ax.set_xticklabels(new_labels,\n",
    "                               fontsize=boxplot_settings['fontsize'])\n",
    "\n",
    "        ax.yaxis.set_ticks( np.linspace(0.0, 1.0, 3) )\n",
    "        ax.set_ylim([-0.1, 1.1])\n",
    "        for ytick in ax.yaxis.get_major_ticks():\n",
    "            ytick.label.set_fontsize(boxplot_settings['fontsize'])\n",
    "\n",
    "        \n",
    "    fig.suptitle(\"\")\n",
    "    # fig.text(0.5, -0.4, 'common X', ha='center')\n",
    "    fig.text(0.0, 0.5,\n",
    "             boxplot_settings[plot_variable]['ylabel'],\n",
    "             verticalalignment='center',\n",
    "             horizontalalignment='center',\n",
    "             rotation='vertical',\n",
    "             fontsize=boxplot_settings['fontsize'])\n",
    "    fig.subplots_adjust(hspace=0.05, wspace=0.05)\n",
    "\n",
    "    output_file = boxplot_settings[plot_variable]['output_file']\n",
    "    fig.savefig(output_file, bbox_inches=\"tight\")\n",
    "    print \"Saved %s\" % output_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also plot a more general overview of performance across each file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = fig.add_subplot(111)\n",
    "df_concat.boxplot(\n",
    "    column=[plot_variable],\n",
    "    by=[\"darshan_file_system\"],\n",
    "    ax=ax,\n",
    "    widths=0.75,\n",
    "    boxprops={'linewidth':2},\n",
    "    medianprops={'linewidth':2 },\n",
    "    whiskerprops={'linewidth':2},\n",
    "    capprops={'linewidth':2},\n",
    "    flierprops={'linewidth':2},\n",
    "    whis=[5,95])\n",
    "### add window dressing to plots\n",
    "# plt.xticks(rotation=45)\n",
    "fig.suptitle(\"\")\n",
    "ax.set_title(\"\", fontsize=14 )\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_ylabel(y_label)\n",
    "ax.xaxis.grid(False)\n",
    "\n",
    "output_file = \"perf-boxplots-fs.pdf\"\n",
    "fig.savefig(output_file, bbox_inches=\"tight\")\n",
    "print \"Saved %s\" % output_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also create a boxplot of the coverage factor to demonstrate how often jobs were impacted by other jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = fig.add_subplot(111)\n",
    "df_concat.boxplot(\n",
    "    column=['coverage_factor'],\n",
    "    by=[\"darshan_file_system\"],\n",
    "    ax=ax,\n",
    "    widths=0.75,\n",
    "    boxprops={'linewidth':2},\n",
    "    medianprops={'linewidth':2 },\n",
    "    whiskerprops={'linewidth':2},\n",
    "    capprops={'linewidth':2},\n",
    "    flierprops={'linewidth':2},\n",
    "    whis=[5,95])\n",
    "### add window dressing to plots\n",
    "# plt.xticks(rotation=45)\n",
    "fig.suptitle(\"\")\n",
    "ax.set_title(\"\", fontsize=14 )\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_ylabel(\"Coverage Factor\")\n",
    "ax.xaxis.grid(False)\n",
    "\n",
    "output_file = \"cf-boxplots-fs.pdf\"\n",
    "fig.savefig(output_file, bbox_inches=\"tight\")\n",
    "print \"Saved %s\" % output_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also try a coverage factor histogram since boxplots don't represent the long tail very well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "x = []\n",
    "labels = []\n",
    "for fs in df_concat['darshan_file_system'].unique():\n",
    "    x.append(df_concat[df_concat[\"darshan_file_system\"]==fs][\"coverage_factor\"].reset_index(drop=True).dropna())\n",
    "    labels.append(fs) # in case the .unique() generator is nondeterministic\n",
    "    \n",
    "### retain the histogram results\n",
    "histogram = ax.hist(x,\n",
    "        bins=[0.101 * x for x in range(0,13)],\n",
    "        label=labels,\n",
    "        stacked=True\n",
    "    )\n",
    "# ax.yaxis.grid()\n",
    "ax.legend()\n",
    "ax.set_title(\"\", fontsize=14 )\n",
    "ax.set_xlabel(\"Coverage Factor\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "fig.suptitle(\"\")\n",
    "\n",
    "output_file = \"cf-histogram-fs.pdf\"\n",
    "fig.savefig(output_file, bbox_inches=\"tight\")\n",
    "print \"Saved %s\" % output_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the cumulative distribution function (CDF) from the histogram data to get a probability distribution of observing a given coverage factor (CF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(histogram[0])): # 4\n",
    "    sums = [0 for x in range(len(histogram[0][i]))]\n",
    "    for j in range(len(histogram[0][i])): # 13\n",
    "        sums[j] += histogram[0][i][j]\n",
    "probabilities = sums / sum(sums)\n",
    "cumul = 0.0\n",
    "print \"%5s %6s %10s %10s\" % (\"CF\", \"Value\", \"CDF\", \"1-CDF\")\n",
    "for idx, probability in enumerate(probabilities):\n",
    "    cumul += probability\n",
    "    print \"%5.3f %6.4f %10.4f %10.4f\" % ( histogram[1][idx], probability, cumul, 1.0 - cumul )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cumulative Distribution Functions\n",
    "\n",
    "We now have a distribution for the coverage factor and performance.  What is the probability of getting anywhere near peak performance?  How does this vary with the coverage factor?\n",
    "\n",
    "First calculate the cumulative distribution function for keys of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_cdf( values ):\n",
    "    \"\"\"Create a pandas.Series that is the CDF of a list-like object\"\"\"\n",
    "\n",
    "    denom = len(values) - 1\n",
    "    x = sorted(values / values.max())\n",
    "    y = [ i / denom for i in np.arange(len(x), dtype=np.float64) ]\n",
    "    return pandas.Series( y, index=x )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Calculate CDFs for (1) each file system and (2) performance and coverage factor\n",
    "cdfs = {}\n",
    "for fs in _FILE_SYSTEM_ORDER:\n",
    "    df_fs = df_concat[df_concat['darshan_file_system'] == fs]\n",
    "    for cdf_key in 'darshan_normalized_perf_by_max', 'coverage_factor':\n",
    "        if cdf_key not in cdfs:\n",
    "            cdfs[cdf_key] = {}\n",
    "        cdf = calculate_cdf(df_fs[cdf_key])\n",
    "        cdfs[cdf_key][fs] = {\n",
    "            'dependent_variable':  cdf.index,\n",
    "            'probability': cdf.values,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### don't use counter_labels because the context is slightly different\n",
    "cdf_labels = {\n",
    "    'darshan_normalized_perf_by_max': \"Fraction Peak Performance\",\n",
    "    'coverage_factor': \"Coverage Factor\",\n",
    "}\n",
    "cdf_file_labels = {\n",
    "    'darshan_normalized_perf_by_max': \"perf\",\n",
    "    'coverage_factor': \"coverage-factor\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Plot CDF side-by-side\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, sharex=True, sharey=True)\n",
    "fig.set_size_inches(8,4)\n",
    "\n",
    "for fs in _FILE_SYSTEM_ORDER:\n",
    "    for idx, cdf_key in enumerate(sorted(cdfs.keys())):\n",
    "        axes[idx].plot(\n",
    "                       cdfs[cdf_key][fs]['dependent_variable'],\n",
    "                       cdfs[cdf_key][fs]['probability'],\n",
    "                       label=fs,\n",
    "                       linewidth=2)\n",
    "        axes[idx].set_xlabel(wrap(cdf_labels[cdf_key]), fontsize=14)\n",
    "        axes[idx].set_ylabel(\"\")\n",
    "        axes[idx].plot([0.0, 1.0],[0.5, 0.5], '--', linewidth=2.0, color='red')\n",
    "        axes[idx].legend().remove()\n",
    "#       axes[idx].set_xticks([0.0, 0.25, 0.50, 0.75, 1.0])\n",
    "        axes[idx].set_xticks(np.linspace(0.0, 1.0, 6))\n",
    "        for tick in axes[idx].xaxis.get_major_ticks():\n",
    "            tick.label.set_fontsize(16) \n",
    "        for tick in axes[idx].yaxis.get_major_ticks():\n",
    "            tick.label.set_fontsize(16)\n",
    "axes[1].xaxis.get_major_ticks()[0].set_visible(False)\n",
    "axes[0].set_title('(a)', position=(0.90,0.0), fontsize=24)\n",
    "axes[1].set_title('(b)', position=(0.90,0.0), fontsize=24)\n",
    "#fig.legend(\n",
    "axes[0].legend(\n",
    "    *(axes[0].get_legend_handles_labels()),\n",
    "    fontsize=16\n",
    "#   ncol=4,\n",
    "#   mode=\"expand\",\n",
    "#   bbox_to_anchor=(0.11, 0.0, 0.79, 1.0),\n",
    "#   loc=\"upper left\",\n",
    "#   borderaxespad=0.0\n",
    "          )\n",
    "axes[0].set_ylabel(\"Cumulative Probability\", fontsize=14)\n",
    "for i in 0, 1:\n",
    "    axes[i].set_ylim([0.0, 1.0])\n",
    "    axes[i].set_xlim([0.0, 1.0])\n",
    "    axes[i].grid(True)\n",
    "    axes[i].yaxis.set_ticks([0.0,0.25,0.50,0.75,1.0])\n",
    "fig.suptitle(\"\")\n",
    "# fig.text(0.0, 0.5, \"Cumulative Probability\", va='center', rotation='vertical')\n",
    "fig.subplots_adjust(hspace=0.0, wspace=0.0)\n",
    "\n",
    "output_file = \"cdf-both.pdf\"\n",
    "fig.savefig(output_file, bbox_inches=\"tight\")\n",
    "print \"Saved %s\" % output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### The 99th percentile of Mira's coverage factors:\n",
    "# np.percentile( df_concat[df_concat['system'] == 'mira']['coverage_factor'], 99 )\n",
    "\n",
    "target_value = 0.99\n",
    "print \"Coverage factor of %.2f or higher corresponds to the %.2fth percentile\" % (\n",
    "    target_value,\n",
    "    stats.percentileofscore(df_concat[df_concat['system'] == 'mira']['coverage_factor'], target_value) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "descriptions = {\n",
    "    'darshan_normalized_perf_by_max': \"On %s, %4.1f%% of jobs got below %d%% peak performance\",\n",
    "    'coverage_factor': \"On %s, %4.1f%% of jobs got a coverage factor below %d%%\",\n",
    "}\n",
    "target_fraction = 0.99\n",
    "### Back up the caption used in the paper for the above CDF plot\n",
    "for cdf_key in 'darshan_normalized_perf_by_max', 'coverage_factor':\n",
    "    for fs in _FILE_SYSTEM_ORDER:\n",
    "        df_fs = df_concat[df_concat['darshan_file_system'] == fs]\n",
    "        value = scipy.interpolate.interp1d(\n",
    "            cdfs[cdf_key][fs]['dependent_variable'],\n",
    "            cdfs[cdf_key][fs]['probability'])(target_fraction)\n",
    "        print descriptions[cdf_key] % (fs, 100.0 * value, int(100.0 * target_fraction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"The line demarcing 50% probability corresponds to...\"\n",
    "\n",
    "target_fraction = 0.50\n",
    "### Back up the caption used in the paper for the above CDF plot\n",
    "for cdf_key in 'darshan_normalized_perf_by_max', 'coverage_factor':\n",
    "    for fs in _FILE_SYSTEM_ORDER:\n",
    "        df_fs = df_concat[df_concat['darshan_file_system'] == fs]\n",
    "        value = scipy.interpolate.interp1d(\n",
    "            cdfs[cdf_key][fs]['probability'],\n",
    "            cdfs[cdf_key][fs]['dependent_variable']\n",
    "        )(target_fraction)\n",
    "        print fs, cdf_key, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IOPS Coverage Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df_mira.copy()\n",
    "df['iops_coverage_factor'] = (df_mira['darshan_total_rws'] / (df_mira['ggio_read_reqs'] + df_mira['ggio_write_reqs']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot('111')\n",
    "x_counter = 'coverage_factor'\n",
    "x_counter = 'darshan_normalized_perf_by_max'\n",
    "y_counter = 'iops_coverage_factor'\n",
    "ax.plot(df[x_counter],\n",
    "        df[y_counter],\n",
    "        marker='o',\n",
    "        alpha=1.0,\n",
    "        linewidth=0.0,\n",
    "       )\n",
    "result = ax.hexbin( df[x_counter],\n",
    "            df[y_counter],\n",
    "            gridsize=15,\n",
    "            cmap='hot_r'\n",
    "       )\n",
    "\n",
    "ax.set_xlabel(x_counter)\n",
    "ax.set_ylabel(y_counter)\n",
    "ax.grid()\n",
    "print \"\"\"\n",
    "When performance is high, iops coverage factor is high (so application has exclusive access to iops)\n",
    "When performance is low, iops coverage factor is low\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### CDF\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(8,5)\n",
    "ax = fig.add_subplot(\"111\")\n",
    "ax.plot(calculate_cdf( df_mira['iops_coverage_factor'] ), label='IOPs', lw=2.0)\n",
    "ax.plot(calculate_cdf( df_mira['coverage_factor'] ), label='Bandwidth', lw=2.0)\n",
    "ax.set_xlabel(\"Coverage Factor\")\n",
    "ax.set_ylabel(\"Cumulative Probability\")\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "output_file = \"cdf-cf-bw-and-ops.pdf\"\n",
    "fig.savefig(output_file, bbox_inches=\"tight\")\n",
    "print \"Saved %s\" % output_file\n",
    "\n",
    "\n",
    "### Also try bar plots\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(8,5)\n",
    "ax = fig.add_subplot(\"111\")\n",
    "y1 = df_mira['iops_coverage_factor']\n",
    "y2 = df_mira['coverage_factor']\n",
    "common_opts = {\n",
    "                \"width\": 1.0/15.0,\n",
    "                \"bins\": np.linspace(0.0, 1.0, 15),\n",
    "                'alpha': 0.75,\n",
    "                'lw': 3.0,\n",
    "#                 'zorder': 9,\n",
    "              }\n",
    "\n",
    "# ax.hist( [ y1, y2 ], label=[\"IOPs\", \"Bandwidth\"], **common_opts)\n",
    "for y, label in [ \n",
    "                 (y1, 'IOPs'),\n",
    "                 (y2, 'Bandwidth'),\n",
    "\n",
    "                ]:\n",
    "    ax.hist( y, label=label, **common_opts)\n",
    "\n",
    "ax.set_xlabel(\"Coverage Factor\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.legend()\n",
    "ax.yaxis.grid()\n",
    "output_file = \"hist-cf-bw-and-ops.pdf\"\n",
    "fig.savefig(output_file, bbox_inches=\"tight\")\n",
    "print \"Saved %s\" % output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Also try bar plots for Edison\n",
    "\n",
    "for fs in df_concat[df_concat['system'] == 'edison']['darshan_file_system'].unique():\n",
    "    fig = plt.figure()\n",
    "    fig.set_size_inches(8,5)\n",
    "    ax = fig.add_subplot(\"111\")\n",
    "    y = df_concat[df_concat['darshan_file_system'] == fs]['coverage_factor']\n",
    "    common_opts = {\n",
    "                    \"width\": 1.0/15.0,\n",
    "                    \"bins\": np.linspace(0.0, 1.0, 11),\n",
    "                    'alpha': 0.75,\n",
    "                    'lw': 3.0,\n",
    "                  }\n",
    "    ax.hist( y, label='Bandwidth', **common_opts)\n",
    "\n",
    "    ax.set_xlabel(\"Coverage Factor\")\n",
    "    ax.set_ylabel(\"Frequency\")\n",
    "#   ax.legend()\n",
    "    ax.yaxis.grid()\n",
    "    ax.set_title(\"%s Bandwidth Coverage Factor\" % fs)\n",
    "    output_file = \"hist-cf-bw-%s.pdf\" % fs\n",
    "    fig.savefig(output_file, bbox_inches=\"tight\")\n",
    "    print \"Saved %s\" % output_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Statistical Distribution of Problem Sources\n",
    "\n",
    "The idea here is that we look at the bottom quartile of performance across all apps and see if there are any common threads of correlation.  For example, some by-hand poking at Mira data revealed that readdirs were usually high when performance was low.  PCA would be a better way to do this, but who has time to do that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "worst_runs_index = {}\n",
    "for system in 'edison', 'mira':\n",
    "    sys_index = df_concat[df_concat['system'] == system].index\n",
    "    perf_index = (df_concat.loc[sys_index] < np.percentile(df_concat.loc[sys_index], 25)).index\n",
    "    worst_runs_index[system] = perf_index"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
