{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Statistical Analysis of TOKIO-ABC Results\n",
      "\n",
      "This notebook performs various statistical analyses of the summary data generated by each TOKIO-ABC job.  These data are loaded from __a summary csv file__ where each row represents a single job and contains the relevant data extracted from\n",
      "\n",
      "1. the darshan log\n",
      "2. the server-side I/O monitoring (Lustre LMT or GPFS GGIOSTAT)\n",
      "3. optional system-specific monitoring including\n",
      "    - OST health info (Lustre)\n",
      "    - Concurrent job count (Slurm)\n",
      "    - Job radius (Cray XC)\n",
      "    \n",
      "This input CSV is generated at NERSC by\n",
      "\n",
      "1. running `utils/nersc_generate_job_summary.sh` to generate json summary records for each darshan log\n",
      "2. running `utils/json2csv.py` to convert all jsons into a single csv file\n",
      "\n",
      "This script is used to identify interesting patterns and correlations across jobs.  For intra-job inspection, use other analysis notebooks such as `analysis/per_ost_deep_dive.ipynb`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "%matplotlib inline\n",
      "import matplotlib\n",
      "import matplotlib.pyplot as plt\n",
      "matplotlib.rcParams.update({'font.size': 18})\n",
      "plt.rcParams['image.cmap'] = 'gray'\n",
      "import matplotlib.gridspec\n",
      "import os"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "import pandas\n",
      "import numpy as np\n",
      "import scipy\n",
      "import scipy.stats as stats\n",
      "import scipy.interpolate\n",
      "import json\n",
      "import datetime\n",
      "import bisect\n",
      "import warnings\n",
      "import textwrap\n",
      "\n",
      "### black magic necessary for processing Mira log files :(\n",
      "try:\n",
      "    import pytz\n",
      "    _USE_TZ = True\n",
      "except ImportError:\n",
      "    _USE_TZ = False\n",
      "\n",
      "def wrap(text, width=15):\n",
      "    \"\"\"wrapper for the wrapper\"\"\"\n",
      "    return '\\n'.join(textwrap.wrap(text=text,width=width))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def utc_timestamp_to_YYYYMMDD( timestamp ):\n",
      "    \"\"\"\n",
      "    This is a batty function that allows us to compare the UTC-based\n",
      "    timestamps from Darshan logs (start_time and end_time) to the\n",
      "    Chicago-based YYYY-MM-DD dates used to index the mmdf data.\n",
      "    \"\"\"\n",
      "    if _USE_TZ:\n",
      "        ### we know that these logs are from Chicago\n",
      "        tz = pytz.timezone(\"America/Chicago\")\n",
      "        \n",
      "        ### Darshan log's start time in UTC, so turn it into a datetime with UTC on it\n",
      "        darshan_time = pytz.utc.localize(datetime.datetime.utcfromtimestamp(timestamp))\n",
      "        \n",
      "        ### Then convert this UTC start time into a local start time so\n",
      "        ### we can compare it to the local mmdf timestamp\n",
      "        darshan_time_at_argonne = darshan_time.astimezone(tz)\n",
      "        return darshan_time_at_argonne\n",
      "    else:\n",
      "        ### we assume that this script is running on Argonne time; it's the best we can do\n",
      "        warnings.warn(\"pytz is not available so mmdf data might be misaligned by a day!\")\n",
      "        return datetime.datetime.fromtimestamp(timestamp)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Relative path to the repository's root directory\n",
      "_REPO_BASE_DIR = os.path.join('..', '..')\n",
      "\n",
      "### Translates cryptic counter names into something suitable for labeling plots\n",
      "counter_labels = json.load(open(os.path.join(_REPO_BASE_DIR, 'scripts', 'counter_labels.json'), 'r'))\n",
      "\n",
      "### For consistency, always plot file systems in the same order\n",
      "_FILE_SYSTEM_ORDER = [ 'scratch1', 'scratch2', 'scratch3', 'mira-fs1' ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def plot_corr(df,size=20):\n",
      "    \"\"\"\n",
      "    Function plots a graphical correlation matrix for each pair\n",
      "    of columns in the dataframe.  From\n",
      "    \n",
      "    http://stackoverflow.com/questions/29432629/correlation-matrix-using-pandas\n",
      "\n",
      "    Input:\n",
      "        df: pandas DataFrame\n",
      "        size: vertical and horizontal size of the plot\n",
      "    \"\"\"\n",
      "    matplotlib.rc('xtick', labelsize=20)\n",
      "    matplotlib.rc('ytick', labelsize=20)\n",
      "    corr = df.corr()\n",
      "    fig, ax = plt.subplots(figsize=(size, size))\n",
      "    ax.matshow(corr, cmap=plt.get_cmap('seismic'),\n",
      "                    norm=matplotlib.colors.Normalize(vmin=-1.,vmax=1.) )\n",
      "    plt.xticks(range(len(corr.columns)), corr.columns, rotation='vertical')\n",
      "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
      "    return corr"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Load data\n",
      "\n",
      "We've been storing most of the per-job summary data in a single CSV per system.  We\n",
      "\n",
      "1. Load the CSV directly into a dataframe\n",
      "2. Drop any rows containing NANs, because if any of the core data is missing (e.g., application name), the whole record is useless.  Hopefully I haven't overlooked anything important in this assumption.\n",
      "3. Synthesize a few new columns (we call these \"metrics\" in the paper) to facilitate downstream analysis"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Edison\n",
      "df_edison = pandas.DataFrame.from_csv(os.path.join(_REPO_BASE_DIR,\n",
      "                                                   'data',\n",
      "                                                   'dat',\n",
      "                                                   'tokio-lustre',\n",
      "                                                   'edison-abc-stats_2-14_3-23.csv')).dropna()\n",
      "df_edison['darshan_rw'] = [ 'write' if x == 1 else 'read' for x in df_edison['darshan_write_mode?'] ]\n",
      "df_edison['darshan_file_mode'] = [ 'shared' if x in ['H5Part','MPIIO'] else 'fpp' for x in df_edison['darshan_api'] ]\n",
      "df_edison.rename(columns={'lmt_bytes_covered': 'coverage_factor'}, inplace=True)\n",
      "df_edison['system'] = \"edison\"\n",
      "df_edison['iops_coverage_factor'] = -1.0\n",
      "\n",
      "### Mira\n",
      "df_mira = pandas.DataFrame.from_csv(os.path.join(_REPO_BASE_DIR,\n",
      "                                                'data',\n",
      "                                                'dat',\n",
      "                                                'tokio-gpfs',\n",
      "                                                'alcf-abc-stats_2-25_3-19.dat')).dropna()\n",
      "rename_dict = { '# platform': \"system\" }\n",
      "for key in df_mira.keys():\n",
      "    if key == 'file_sys':\n",
      "        rename_dict[key] = 'darshan_file_system'\n",
      "    elif key not in rename_dict and not key.startswith('ggio_'):\n",
      "        rename_dict[key] = 'darshan_' + key\n",
      "df_mira.rename(columns=rename_dict, inplace=True)\n",
      "df_mira['darshan_file_mode'] = [ 'shared' if x in ['H5Part','MPIIO'] else 'fpp' for x in df_mira['darshan_api'] ]\n",
      "df_mira['coverage_factor'] = df_mira['darshan_total_bytes'] / (df_mira['ggio_bytes_read'] + df_mira['ggio_bytes_written'])\n",
      "df_mira['iops_coverage_factor'] = (df_mira['darshan_total_rws'] / (df_mira['ggio_read_reqs'] + df_mira['ggio_write_reqs']))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Because I'm lazy, load the `mmdf` data separately and attach it to `df_mira`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#df_mmdf = pandas.DataFrame.from_csv(os.path.join(_REPO_BASE_DIR,\n",
      "df_mmdf = pandas.DataFrame.from_csv(os.path.join(_REPO_BASE_DIR,\n",
      "                                                'data',\n",
      "                                                'dat',\n",
      "                                                'tokio-gpfs',\n",
      "                                                'mira_mmdf_1-25_3-23.csv'),\n",
      "                                        index_col=['file_system', 'date'])\n",
      "df_mmdf['free_kib'] = df_mmdf['free_kib_blocks'] + df_mmdf['free_kib_frags']\n",
      "df_mmdf['free_pct'] = df_mmdf['free_kib'] / df_mmdf['disk_size']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Walk the master dataframe and attach mmdf data.  Note that we're injecting NAs for missing mmdf data because missing mmdf data should not exclude the entire day from our analysis."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### I really hope iterrows behaves deterministically and preserves order...\n",
      "new_data = {\n",
      "    'mmdf_avg_fullness_pct': [],\n",
      "    'mmdf_max_fullness_pct': [],\n",
      "}\n",
      "\n",
      "### iterate over each row of the master Mira dataframe\n",
      "for row in df_mira.itertuples():\n",
      "    fs_key = row.darshan_file_system\n",
      "    mmdf_key = utc_timestamp_to_YYYYMMDD( row.darshan_start_time ).strftime(\"%Y-%m-%d\")\n",
      "    if mmdf_key in df_mmdf.loc[fs_key].index:\n",
      "        ### only look at today's data\n",
      "        df = df_mmdf.loc[fs_key].loc[mmdf_key]\n",
      "        \n",
      "        data_cols = [ True if x else False for x in df['data?'] ]\n",
      "\n",
      "        ### calculate a percent fullness - don't bother saving the id of this fullest server though\n",
      "        new_data['mmdf_max_fullness_pct'].append( 1.0 - df[ data_cols ]['free_pct'].min() )\n",
      "        new_data['mmdf_avg_fullness_pct'].append( 1.0 - df[ data_cols ]['free_pct'].mean() )\n",
      "    else:\n",
      "        warnings.warn(\"no mmdf data for %s\" % datetime.datetime.fromtimestamp(row.darshan_start_time) )\n",
      "        new_data['mmdf_max_fullness_pct'].append( np.nan )\n",
      "        new_data['mmdf_avg_fullness_pct'].append( np.nan )\n",
      "\n",
      "for new_col_name, new_col_data in new_data.iteritems():\n",
      "    df_mira[new_col_name] = new_col_data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now merge both DataFrames so we can look at all the data if we really want to.  This DataFrame will have a bunch of NANs for data that is only applicable to Mira or Edison."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "df_concat = pandas.concat( (df_mira, df_edison) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Filter Data\n",
      "\n",
      "Two notable filters are applied:\n",
      "\n",
      "1. All jobs where the bytes coverage factor and ops coverage factor are greater than 1.2 are discarded because they reflect severely misaligned or gappy data.\n",
      "\n",
      "2. Mira job 1039807 is excluded because ggiostat returned highly abnormal results starting that day.  See e-mail from Shane and Phil on March 23 about this.\n",
      "\n",
      "3. All Edison jobs from March 12 were discarded because LMT broke as a result of daylight saving time rolling over.  This filter was applied _before_ the input CSV files loaded above were generated, so it does not need to be applied here."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for df in df_mira, df_edison, df_concat:\n",
      "    df.drop(df.index[df['coverage_factor'] > 1.2], inplace=True)\n",
      "    df.drop(df.index[df['iops_coverage_factor'] > 1.2], inplace=True)\n",
      "    \n",
      "    df.drop(df.index[(df['system'] == 'mira') & (df['darshan_jobid'] == 1039807)], inplace=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_mira"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Normalize Performance\n",
      "Different file systems, benchmarks, and read/write modes are capable of different peak bandwidths.  As such, we want to normalize the absolute performance (`summarize_key`) by something.  For convenience we calculate the denominator for normalization a couple of different ways (e.g., the mean, median, and max measurement).  We also limit normalization to unique combinations of variables specified by `normalization_group` below."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Calculate the normalization factors (the denominators), then apply that factor to all of the data in the DataFrame.  These normalized data will be saved as new columns in the DataFrame."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### The actual variable we want to normalize\n",
      "summarize_key = 'darshan_agg_perf_by_slowest'\n",
      "\n",
      "for df in df_edison, df_mira, df_concat:\n",
      "    ### Specify which keys we want to group together before normalizing\n",
      "    normalization_group = df.groupby(['darshan_app', 'darshan_file_system', 'darshan_file_mode', 'darshan_rw'])\n",
      "\n",
      "    ### Dict to store the denominators for normalization\n",
      "    normalization_data = {\n",
      "        'mean':   normalization_group.darshan_agg_perf_by_slowest.mean(),\n",
      "        'median': normalization_group.darshan_agg_perf_by_slowest.median(),\n",
      "        'max':    normalization_group.darshan_agg_perf_by_slowest.max(),\n",
      "    }\n",
      "\n",
      "    ### Normalize every row in the DataFrame by all of our denominators\n",
      "    new_cols = {}\n",
      "    for func in normalization_data.keys():\n",
      "        new_col_key = 'darshan_normalized_perf_by_%s' % func\n",
      "        new_cols[new_col_key] = []\n",
      "        for index, row in df.iterrows():\n",
      "            new_cols[new_col_key].append(\n",
      "                row[summarize_key] / normalization_data[func]\n",
      "                                                       [row['darshan_app']]\n",
      "                                                       [row['darshan_file_system']]\n",
      "                                                       [row['darshan_file_mode']]\n",
      "                                                       [row['darshan_rw']]\n",
      "            )\n",
      "\n",
      "    ### Also just do per-file system\n",
      "    normalization_group = df.groupby('darshan_file_system')\n",
      "    normalization_data = {\n",
      "        'mean':   normalization_group.darshan_agg_perf_by_slowest.mean(),\n",
      "        'median': normalization_group.darshan_agg_perf_by_slowest.median(),\n",
      "        'max':    normalization_group.darshan_agg_perf_by_slowest.max(),\n",
      "    }\n",
      "    for func in normalization_data.keys():\n",
      "        new_col_key = 'darshan_normalized_perf_by_fs_%s' % func\n",
      "        new_cols[new_col_key] = []\n",
      "        for index, row in df.iterrows():\n",
      "            new_cols[new_col_key].append(\n",
      "                row[summarize_key] / normalization_data[func][row['darshan_file_system']])\n",
      "\n",
      "    ### Take our normalized data and add them as new columns\n",
      "    for new_col, new_col_data in new_cols.iteritems():\n",
      "        df[new_col] = new_col_data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Multivariate Correlation Analysis\n",
      "`performance_key` is the variable we wish to use to represent performance.  It is typically\n",
      "\n",
      "- `darshan_agg_perf_by_slowest`, which is the absolute performance measured by each benchmark run\n",
      "- `darshan_normalized_perf_by_max`, which is normalized by the maximum observed performance\n",
      "- `darshan_normalized_perf_by_mean`, which is normalized by the mean observed performance"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "performance_key = 'darshan_normalized_perf_by_max'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Pearson Correlation Analysis\n",
      "Pearson analysis assumes that each variable is normally distributed.  It is easier to understand, but it is not technically correct for variables that are _not_ normally distributed, which include performance.  The Spearman coefficient would be better.\n",
      "\n",
      "At any rate, this correlation matrix is not of interest to this paper so don't bother generating it here."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### make a pretty plot to flag highlights\n",
      "if False:\n",
      "    corr_edison = plot_corr(df_edison[\n",
      "        (df_edison['darshan_file_system'] == 'scratch1') \n",
      "        | (df_edison['darshan_file_system'] == 'scratch2') \n",
      "        | (df_edison['darshan_file_system'] == 'scratch3')\n",
      "    ], 10)\n",
      "    corr_mira = plot_corr(df[df['darshan_file_system'] == 'mira-fs1'], 10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Numerical Correlation Analysis\n",
      "Now we repeat this correlation analysis, but this time use `scipy.stats` instead of `pandas` so that we can calculate p-values associated with each correlation.  The ultimate artifact of this process is a table of interesting correlations, their correlation coefficients, and color coding to indicate the confidence of those coefficients based on p-values."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def correlation_calculation(df,\n",
      "                            analysis_func=stats.pearsonr,\n",
      "                            only_print_key=performance_key,\n",
      "                            ignore_cols=[],\n",
      "                            max_pval=1.01):\n",
      "    \"\"\"\n",
      "    Calculate the Pearson correlation coefficient and the associated\n",
      "    p-value for every permutation of counter pairs\n",
      "    \"\"\"\n",
      "    num_cols = len(df.keys())\n",
      "    results = []\n",
      "    for i in range(num_cols - 1):\n",
      "        i_col = df.columns[i]\n",
      "        for j in range(i, num_cols):\n",
      "            j_col = df.columns[j]\n",
      "            try:\n",
      "                coeff, pval = analysis_func(df[i_col],\n",
      "                                            df[j_col])\n",
      "            except TypeError: # non-numeric column\n",
      "                warnings.warn(\"%s is a non-numeric column\" % i_col)\n",
      "                continue\n",
      "            results.append((i_col,\n",
      "                            j_col,\n",
      "                            coeff,\n",
      "                            pval))\n",
      "\n",
      "    sorted_results = sorted(results, key=lambda x: x[3])\n",
      "    ret_results = []\n",
      "    for col_name1, col_name2, coeff, pval in sorted_results:\n",
      "        ### don't print trivial relationships\n",
      "        if pval == 0 or pval == 1:\n",
      "            continue\n",
      "        ### don't print relationships with very high p-values\n",
      "        if pval > max_pval:\n",
      "            continue\n",
      "        ### don't correlate data from the same source since much of it is degenerate\n",
      "        if col_name1.split('_',1)[0] == col_name2.split('_',1)[0]:\n",
      "            continue\n",
      "        ### don't print anything except for the key of interest (if provided)\n",
      "        if only_print_key is not None \\\n",
      "        and col_name1 != only_print_key \\\n",
      "        and col_name2 != only_print_key:\n",
      "            continue\n",
      "        if col_name1 in ignore_cols or col_name2 in ignore_cols:\n",
      "            continue\n",
      "#       print \"%10.4f %10.4g %30s : %-15s\" % (coeff, pval, col_name1, col_name2)\n",
      "        \n",
      "        ### sort the output key orders\n",
      "        if col_name1 == only_print_key:\n",
      "            ret_results.append([col_name1,col_name2,coeff,pval])\n",
      "        elif col_name2 == only_print_key:\n",
      "            ret_results.append([col_name2,col_name1,coeff,pval])\n",
      "        else:\n",
      "            if col_name2 > col_name1:\n",
      "                ret_results.append([col_name1,col_name2,coeff,pval])\n",
      "            else:\n",
      "                ret_results.append([col_name2,col_name1,coeff,pval])\n",
      "    return ret_results"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ignore_cols = [\n",
      "    'lmt_tot_zeros',\n",
      "    'lmt_frac_zeros',\n",
      "    'lmt_frac_missing',\n",
      "    'ost_avg_kib',\n",
      "    'ost_min_pct',\n",
      "    'ost_min_kib',\n",
      "    'ost_max_kib',\n",
      "    'ost_count',\n",
      "    'ost_failures_lead_secs',\n",
      "    'ost_fullness_lead_secs',\n",
      "#   'lmt_oss_max',\n",
      "    'lmt_tot_missing',\n",
      "    'ost_avg_bad_ost_per_oss',\n",
      "    'ost_avg_bad_overload_factor',\n",
      "    'ost_bad_oss_count',\n",
      "    'ost_min_id',\n",
      "    'ost_max_id',\n",
      "    'job_min_radius',\n",
      "    'job_avg_radius',\n",
      "]\n",
      "\n",
      "### if one key has the same logical meaning as another, this will remap those\n",
      "### keys so they line up in the DataFrame\n",
      "equivalent_keys = {\n",
      "    'ggio_closes':     'lmt_ops_closes',\n",
      "    'ggio_opens': 'lmt_ops_opens',\n",
      "    'ggio_bytes_read': 'lmt_tot_bytes_read',\n",
      "    'ggio_bytes_written': 'lmt_tot_bytes_write',\n",
      "    'mmdf_max_fullness_pct': 'ost_max_pct',\n",
      "    'mmdf_avg_fullness_pct': 'ost_avg_pct',\n",
      "}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Calculate correlations between performance and everything else\n",
      "correlations = {}\n",
      "correlations['edison'] = correlation_calculation(df_edison, ignore_cols=ignore_cols)\n",
      "correlations['mira'] = correlation_calculation(df_mira, ignore_cols=ignore_cols)\n",
      "\n",
      "### to_df: dict we will use to store arrays -> pd.Series\n",
      "to_df = {}\n",
      "\n",
      "### common_key: needed to figure out which key is the independent variable\n",
      "common_key = None\n",
      "\n",
      "### list that will become dataframe index\n",
      "key_index = []\n",
      "\n",
      "### loop over all systems, all correlations, all pairs of variables\n",
      "for system, records in correlations.iteritems():\n",
      "    for row in records:\n",
      "        ### try to figure out which of the two keys is not repeated\n",
      "        if common_key is None:\n",
      "            common_key = row[0]\n",
      "        if row[0] != common_key:\n",
      "            common_key = row[1]\n",
      "            unique_key = row[0]\n",
      "        else:\n",
      "            unique_key = row[1]\n",
      "\n",
      "        ### convert compatible key names from ggio_ to lmt_\n",
      "        if unique_key in equivalent_keys:\n",
      "            unique_key = equivalent_keys[unique_key]\n",
      "\n",
      "        ### build up a list of keys that will be our Index\n",
      "        if unique_key not in key_index:\n",
      "            key_index.append(unique_key)\n",
      "            \n",
      "        ### fill out the dict \n",
      "        counterkey = \"counter_%s\" % system\n",
      "        corrkey = 'correlation_%s' % system\n",
      "        pvalkey = 'p-value_%s' % system\n",
      "        if counterkey not in to_df: to_df[counterkey] = []\n",
      "        if corrkey not in to_df: to_df[corrkey] = []\n",
      "        if pvalkey not in to_df: to_df[pvalkey] = []\n",
      "        to_df[counterkey].append(unique_key)\n",
      "        to_df[corrkey].append(row[2])\n",
      "        to_df[pvalkey].append(row[3])\n",
      "\n",
      "### Make an empty but indexed data frame\n",
      "df = pandas.DataFrame(index=key_index)\n",
      "for system in correlations.keys():\n",
      "    counterkey = \"counter_%s\" % system\n",
      "    corrkey = 'correlation_%s' % system\n",
      "    pvalkey = 'p-value_%s' % system\n",
      "\n",
      "    df[corrkey] = pandas.Series(to_df[corrkey], index=to_df[counterkey])\n",
      "    df[pvalkey] = pandas.Series(to_df[pvalkey], index=to_df[counterkey])\n",
      "\n",
      "df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = plt.figure(figsize=(4,12))\n",
      "ax = fig.add_subplot(111)\n",
      "\n",
      "column_headers = {\n",
      "    \"correlation_edison\": \"Coefficient\\n(Edison)\",\n",
      "    \"correlation_mira\": \"Coefficient\\n(Mira)\",\n",
      "    \"p-value_edison\": \"P-value (Edison)\",\n",
      "    \"p-value_mira\": \"P-value (Mira)\",\n",
      "}\n",
      "\n",
      "coefficient_keys = [x for x in df.columns if x.startswith('correlation_')]\n",
      "\n",
      "### the index is column -1\n",
      "table = pandas.tools.plotting.table(ax,\n",
      "                            df[coefficient_keys],\n",
      "                            loc='upper right',\n",
      "                            colWidths=[0.8,0.8,4],\n",
      "                            bbox=[0, 0, 1, 1])\n",
      "table.set_fontsize(14)\n",
      "ax.axis('tight')\n",
      "ax.axis('off')\n",
      "\n",
      "### Color table cells based on p-value\n",
      "cells_dict = table.get_celld()\n",
      "remap_values = {}\n",
      "for cell_pos, cell_obj in cells_dict.iteritems():\n",
      "    i, j = cell_pos\n",
      "    value = cell_obj.get_text().get_text()\n",
      "    height_scale = 1.0\n",
      "    if i == 0:    # column headers\n",
      "        remap_values[cell_pos] = column_headers.get(value, value)\n",
      "        height_scale = 2.0\n",
      "    elif j == -1: # index cell\n",
      "        remap_values[cell_pos] = counter_labels.get(value, value)\n",
      "        cell_obj._loc = 'right'\n",
      "    else:         # coefficient cell\n",
      "        index = cells_dict[(i,-1)].get_text().get_text()\n",
      "        column = cells_dict[(0,j)].get_text().get_text()\n",
      "        if value == \"nan\":\n",
      "            cell_obj.set_color('grey')\n",
      "            remap_values[cell_pos] = \"\"\n",
      "            cell_obj.set_alpha(0.25)\n",
      "            cell_obj._loc = 'center'\n",
      "        else:\n",
      "            coeff = float(value)\n",
      "            pval = df.loc[index][column.replace('correlation','p-value')]\n",
      "\n",
      "#           print index, column, \"coeff=\", coeff, \"pval=\", pval\n",
      "\n",
      "            if pval < 0.01:\n",
      "                set_color = 'blue'\n",
      "            elif pval < 0.05:\n",
      "                set_color = 'green'\n",
      "            else:\n",
      "                set_color = 'red'\n",
      "            cell_obj.set_color(set_color)\n",
      "            cell_obj.set_alpha(0.25)\n",
      "            cell_obj._loc = 'center'\n",
      "            remap_values[cell_pos] = \"%+.4f\" % coeff\n",
      "    cell_obj.set_height(height_scale * cell_obj.get_height())\n",
      "\n",
      "### Actually rewrite the cells now\n",
      "for cell_pos, new_value in remap_values.iteritems():\n",
      "    cells_dict[cell_pos].get_text().set_text(new_value)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Scatter Plots\n",
      "To visualize these correlations, we define pairs of counters to plot against each other:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scatterplots = [ \n",
      "    (performance_key, 'coverage_factor'),\n",
      "    (performance_key, 'lmt_oss_ave'),\n",
      "    (performance_key, 'job_concurrent_jobs'),\n",
      "    (performance_key, 'ost_avg_pct'),\n",
      "    (performance_key, 'ost_max_kib'),\n",
      "    (performance_key, 'coverage_factor'),\n",
      "    (performance_key, 'ggio_write_reqs'),\n",
      "    (performance_key, 'ggio_write_reqs'),\n",
      "]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "...and then plot them:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "blacklist = set([\n",
      "    'ost_max_id', 'ost_min_id', 'lmt_tot_zeros',\n",
      "    'ost_failures_lead_secs', 'ost_fullness_lead_secs',\n",
      "    'ost_max_kib', 'ost_avg_kib', 'ost_min_kib', \n",
      "    'lmt_tot_missing'\n",
      "])\n",
      "# for scatterplot in correlations_edison + correlations_mira:\n",
      "for scatterplot in scatterplots:\n",
      "    x_key = scatterplot[0]\n",
      "    y_key = scatterplot[1]\n",
      "    if x_key in blacklist or y_key in blacklist:\n",
      "        continue\n",
      "    if y_key in df_mira :\n",
      "        df_plot = df_mira\n",
      "        system = \"Mira\"\n",
      "    elif y_key in df_edison:\n",
      "        df_plot = df_edison\n",
      "        system = \"Edison\"\n",
      "    else:\n",
      "        warnings.warn(\"Cannot find key %s in any data frames\" % y_key)\n",
      "    fig = plt.figure(figsize=(6,4))\n",
      "    ax = fig.add_subplot(111)\n",
      "    \n",
      "    x = df_plot[x_key].values\n",
      "    x_label = counter_labels.get(x_key, x_key)\n",
      "    y = df_plot[y_key].values\n",
      "    y_label = counter_labels.get(y_key, y_key)\n",
      "#   ax.hexbin(x, y, gridsize=25, cmap='PuRd')\n",
      "    ax.plot(x, y, 'o', alpha=0.5)\n",
      "\n",
      "    ### attempt a linear fit to generate a visual aid\n",
      "    m, b = np.polyfit(x, y, 1)\n",
      "    ax.plot(x, m*x+b, \"-\")\n",
      "    \n",
      "    ### add window dressing to plots\n",
      "#   fig.suptitle('Correlation between %s and %s' \n",
      "#                 % (x_label.split('(',1)[0].strip(),\n",
      "#                    y_label.split('(',1)[0].strip()))\n",
      "    ax.set_title(\"Coefficient=%.4f, P-value=%.2g (%s)\" \n",
      "                    % sum((stats.pearsonr(x, y), (system,)), ()), fontsize=14 )\n",
      "    ax.set_xlabel(x_label)\n",
      "    ax.set_ylabel(y_label)\n",
      "    plt.grid(True)\n",
      "    output_file = \"scatter_%s_vs_%s.pdf\" % (x_key, y_key)\n",
      "    fig.savefig(output_file, bbox_inches=\"tight\")\n",
      "    print \"Saved %s\" % output_file"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Mira also has both server-side and client side IOPS.  Let's look at those specifically:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = plt.figure(figsize=(8,4))\n",
      "ax = fig.add_subplot(111)\n",
      "\n",
      "df = df_mira.sort_values(performance_key).copy()\n",
      "\n",
      "for x_key, y_key in [(performance_key, 'ggio_write_reqs'),\n",
      "                    (performance_key, 'ggio_read_reqs')]:\n",
      "    x = df[x_key].values\n",
      "    y = df[y_key].values / df[y_key].max()\n",
      "    corr = stats.pearsonr(x, y)\n",
      "    print counter_labels.get(y_key, y_key), corr[0], corr[1]\n",
      "    points = ax.plot(x, y,\n",
      "                     'o',\n",
      "                     alpha=0.4,\n",
      "                     markersize=6.0,\n",
      "                     label=\"%s\" % (counter_labels.get(y_key, y_key))\n",
      "                    )\n",
      "        \n",
      "    ### attempt a linear fit to generate a visual aid\n",
      "    m, b = np.polyfit(x, y, 1)\n",
      "    ax.plot(x, m*x+b,\n",
      "            \"-\",\n",
      "           color=points[0].get_color())\n",
      "    \n",
      "ax.set_xlabel(counter_labels.get(x_key, x_key))\n",
      "ax.set_ylabel(\"Fraction Peak Ops\")\n",
      "ax.legend()\n",
      "plt.grid(True)\n",
      "output_file = \"scatter_mira_ops.pdf\"\n",
      "fig.savefig(output_file, bbox_inches=\"tight\")\n",
      "print \"Saved %s\" % output_file"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = plt.figure(figsize=(8,4))\n",
      "ax = fig.add_subplot(111)\n",
      "\n",
      "df = df_mira.sort_values(performance_key).copy()\n",
      "\n",
      "for x_key, y_key in [('coverage_factor', 'iops_coverage_factor')]:\n",
      "    x = df[x_key].values\n",
      "    y = df[y_key].values / df[y_key].max()\n",
      "    corr = stats.pearsonr(x, y)\n",
      "    print counter_labels.get(y_key, y_key), corr[0], corr[1]\n",
      "    points = ax.plot(x, y,\n",
      "                     'o',\n",
      "                     alpha=0.4,\n",
      "                     markersize=6.0,\n",
      "                     label=\"%s\" % (counter_labels.get(y_key, y_key))\n",
      "                    )\n",
      "        \n",
      "    ### attempt a linear fit to generate a visual aid\n",
      "    m, b = np.polyfit(x, y, 1)\n",
      "    ax.plot(x, m*x+b,\n",
      "            \"-\",\n",
      "           color=points[0].get_color())\n",
      "    \n",
      "ax.set_xlabel(counter_labels.get(x_key, x_key))\n",
      "ax.set_ylabel(\"Fraction Peak Ops\")\n",
      "ax.legend()\n",
      "plt.grid(True)\n",
      "output_file = \"scatter_mira_ops.pdf\"\n",
      "fig.savefig(output_file, bbox_inches=\"tight\")\n",
      "print \"Saved %s\" % output_file"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Plot the correlation between coverage factor and performance for each file system separately"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# for scatterplot in correlations_mira:\n",
      "for idx, fs in enumerate(_FILE_SYSTEM_ORDER):\n",
      "    fig = plt.figure(figsize=(8,6))\n",
      "    ax = fig.add_subplot(111)\n",
      "    x_key = performance_key\n",
      "    y_key = 'coverage_factor'\n",
      "    if fs.startswith('mira') :\n",
      "        df_plot = df_mira.sort_values(performance_key)\n",
      "        system = \"Mira\"\n",
      "    elif fs.startswith('scratch'):\n",
      "        df_plot = df_edison.sort_values(performance_key)\n",
      "        system = \"Edison\"\n",
      "    else:\n",
      "        warnings.warn(\"Cannot find key %s in any data frames\" % y_key)\n",
      "    df_plot = df_plot[df_plot['darshan_file_system'] == fs]\n",
      "    \n",
      "    x = df_plot[x_key].values\n",
      "    y = df_plot[y_key].values\n",
      "    corr = stats.pearsonr(x, y)\n",
      "    label = \"%s (corr=%.2f, p-val=%4.2g, n=%d)\" % (fs, corr[0], corr[1], len(df_plot))\n",
      "    points = ax.plot(x, y,\n",
      "                     'o', \n",
      "                     alpha=0.75,\n",
      "                     markersize=4.0)\n",
      "\n",
      "    ### attempt a linear fit to generate a visual aid\n",
      "    m, b = np.polyfit(x, y, 1)\n",
      "    ax.plot(x, m*x+b,\n",
      "            \"-\",\n",
      "           color=points[0].get_color())\n",
      "    \n",
      "    ### make the plot pretty\n",
      "    ax.set_xlim([0.0,2.0 if 'mean' in x_key else 1.0])\n",
      "    ax.set_ylabel(\"Coverage Factor\")\n",
      "    ax.set_xlabel(counter_labels[performance_key])\n",
      "    ax.set_title(label, fontsize=14)\n",
      "    plt.grid(True)\n",
      "    output_file = \"scatter_perf-vs-cf_%s.pdf\" % fs\n",
      "    fig.savefig(output_file, bbox_inches=\"tight\")\n",
      "    print \"Saved %s\" % output_file\n",
      "    fig.savefig(output_file.replace('pdf', 'png'), bbox_inches=\"tight\")\n",
      "    print \"Saved %s\" % output_file\n",
      "    df_save = pandas.DataFrame({\"Performance Relative to Mean\": x,\n",
      "                                \"Coverage Factor\": y}).to_csv(output_file.replace('pdf', 'csv'))\n",
      "    print \"Saved\", output_file.replace('pdf', 'csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Distribution of each benchmark type"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Following cell defines which variable we wish to aggregate into boxplots and a few plotting parameters that depend on our choice of variable."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "boxplot_settings = {\n",
      "    'fontsize': 15,\n",
      "    'darshan_normalized_perf_by_fs_max': {\n",
      "        'output_file': \"perf-boxplots-per-fs.pdf\",\n",
      "        'ylabel': \"Fraction of Peak\\nFile System Performance\",\n",
      "        'title_pos': [ \n",
      "            {'x': 0.97, 'y': 0.80, 'horizontalalignment': 'right', 'fontsize': 14},\n",
      "            {'x': 0.04, 'y': 0.02, 'horizontalalignment': 'left', 'fontsize': 14}]\n",
      "    },\n",
      "    'darshan_normalized_perf_by_max': {\n",
      "        'output_file': \"perf-boxplots.pdf\",\n",
      "        'ylabel': \"Fraction of Peak\\nPer-Benchmark Performance\",\n",
      "        'title_pos': [ \n",
      "            {'x': 0.04, 'y': 0.02, 'horizontalalignment': 'left', 'fontsize': 14},\n",
      "            {'x': 0.04, 'y': 0.02, 'horizontalalignment': 'left', 'fontsize': 14}]\n",
      "    },\n",
      "}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We plot two sets of boxplots based on the normalization denominator:\n",
      "\n",
      "1. normalized by the maximum without any grouping (other than file system)\n",
      "2. normalized by the maximum of each unique combination of app-read/write-filemode\n",
      "\n",
      "The idea is to show that\n",
      "\n",
      "1. performance variation varies across different file systems and different applications\n",
      "2. even within an application, the magnitude of performance variation varies with file system"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for plot_variable in performance_key, performance_key.replace('_by_', '_by_fs_'):\n",
      "    fig, axes = plt.subplots(nrows=2, ncols=2, sharex=True, sharey=True)\n",
      "    fig.set_size_inches(8,6)\n",
      "    boxplot_group_by = [ 'darshan_file_mode', 'darshan_rw', 'darshan_app' ]\n",
      "    for idx, fs in enumerate(_FILE_SYSTEM_ORDER):\n",
      "        icol = idx / 2\n",
      "        irow = idx % 2\n",
      "        ax = axes[irow, icol]\n",
      "        df_concat.loc[df_concat[\"darshan_file_system\"] == fs]\\\n",
      "        .boxplot(\n",
      "            column=[plot_variable],\n",
      "            by=boxplot_group_by,\n",
      "            ax=axes[irow, icol],\n",
      "            medianprops={'linewidth':2 },\n",
      "            widths=0.75,\n",
      "            whis=[5,95])\n",
      "\n",
      "        settings = boxplot_settings[plot_variable]['title_pos'][irow]\n",
      "        title = ax.set_title(fs, **(settings))\n",
      "        title.set_bbox({'color': 'white', 'alpha': 0.5})\n",
      "        ax.set_xlabel(\"\")\n",
      "        ax.set_ylabel(\"\")\n",
      "        ax.xaxis.grid(True)\n",
      "\n",
      "        ### relabel the x axis labels\n",
      "        new_labels = []\n",
      "        for axis_label in ax.get_xticklabels():\n",
      "            current_label = axis_label.get_text()\n",
      "            axis_label.set_rotation(90)\n",
      "            if \"IOR\" in current_label:\n",
      "                if \"shared\" in current_label:\n",
      "                    new_label = \"IOR/shared\"\n",
      "                else:\n",
      "                    new_label = \"IOR/fpp\"\n",
      "            elif 'BD-CATS' in current_label:\n",
      "                new_label = \"BD-CATS\"\n",
      "            else:\n",
      "                new_label = current_label.split(',')[2].strip(')').strip().split('-')[0]\n",
      "            if 'write' in current_label:\n",
      "                new_label += \" Write\"\n",
      "            else:\n",
      "                new_label += \" Read\"\n",
      "            new_labels.append(new_label)\n",
      "\n",
      "        ### set x tick labels for only the bottom row\n",
      "        if irow == 0:\n",
      "            ax.set_xticklabels([])\n",
      "        else:\n",
      "            ax.set_xticklabels(new_labels,\n",
      "                               fontsize=boxplot_settings['fontsize'])\n",
      "\n",
      "        ax.yaxis.set_ticks([0.0, 1.0])\n",
      "        ax.set_ylim([-0.1, 1.1])\n",
      "        for ytick in ax.yaxis.get_major_ticks():\n",
      "            ytick.label.set_fontsize(boxplot_settings['fontsize'])\n",
      "\n",
      "        \n",
      "    fig.suptitle(\"\")\n",
      "    # fig.text(0.5, -0.4, 'common X', ha='center')\n",
      "    fig.text(0.0, 0.5,\n",
      "             boxplot_settings[plot_variable]['ylabel'],\n",
      "             verticalalignment='center',\n",
      "             horizontalalignment='center',\n",
      "             rotation='vertical',\n",
      "             fontsize=boxplot_settings['fontsize'])\n",
      "    fig.subplots_adjust(hspace=0.05, wspace=0.05)\n",
      "\n",
      "    output_file = boxplot_settings[plot_variable]['output_file']\n",
      "    fig.savefig(output_file, bbox_inches=\"tight\")\n",
      "    print \"Saved %s\" % output_file"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Also plot a more general overview of performance across each file system."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = plt.figure(figsize=(8,6))\n",
      "ax = fig.add_subplot(111)\n",
      "df_concat.boxplot(\n",
      "    column=[plot_variable],\n",
      "    by=[\"darshan_file_system\"],\n",
      "    ax=ax,\n",
      "    widths=0.75,\n",
      "    boxprops={'linewidth':2},\n",
      "    medianprops={'linewidth':2 },\n",
      "    whiskerprops={'linewidth':2},\n",
      "    capprops={'linewidth':2},\n",
      "    flierprops={'linewidth':2},\n",
      "    whis=[5,95])\n",
      "### add window dressing to plots\n",
      "# plt.xticks(rotation=45)\n",
      "fig.suptitle(\"\")\n",
      "ax.set_title(\"\", fontsize=14 )\n",
      "ax.set_xlabel(\"\")\n",
      "ax.set_ylabel(y_label)\n",
      "ax.xaxis.grid(False)\n",
      "\n",
      "output_file = \"perf-boxplots-fs.pdf\"\n",
      "fig.savefig(output_file, bbox_inches=\"tight\")\n",
      "print \"Saved %s\" % output_file"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Also create a boxplot of the coverage factor to demonstrate how often jobs were impacted by other jobs"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = plt.figure(figsize=(8,6))\n",
      "ax = fig.add_subplot(111)\n",
      "df_concat.boxplot(\n",
      "    column=['coverage_factor'],\n",
      "    by=[\"darshan_file_system\"],\n",
      "    ax=ax,\n",
      "    widths=0.75,\n",
      "    boxprops={'linewidth':2},\n",
      "    medianprops={'linewidth':2 },\n",
      "    whiskerprops={'linewidth':2},\n",
      "    capprops={'linewidth':2},\n",
      "    flierprops={'linewidth':2},\n",
      "    whis=[5,95])\n",
      "### add window dressing to plots\n",
      "# plt.xticks(rotation=45)\n",
      "fig.suptitle(\"\")\n",
      "ax.set_title(\"\", fontsize=14 )\n",
      "ax.set_xlabel(\"\")\n",
      "ax.set_ylabel(\"Coverage Factor\")\n",
      "ax.xaxis.grid(False)\n",
      "\n",
      "output_file = \"cf-boxplots-fs.pdf\"\n",
      "fig.savefig(output_file, bbox_inches=\"tight\")\n",
      "print \"Saved %s\" % output_file"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Also try a coverage factor histogram since boxplots don't represent the long tail very well."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = plt.figure(figsize=(8,6))\n",
      "ax = fig.add_subplot(111)\n",
      "\n",
      "x = []\n",
      "labels = []\n",
      "for fs in df_concat['darshan_file_system'].unique():\n",
      "    x.append(df_concat[df_concat[\"darshan_file_system\"]==fs][\"coverage_factor\"].reset_index(drop=True).dropna())\n",
      "    labels.append(fs) # in case the .unique() generator is nondeterministic\n",
      "    \n",
      "### retain the histogram results\n",
      "histogram = ax.hist(x,\n",
      "        bins=[0.101 * x for x in range(0,13)],\n",
      "        label=labels,\n",
      "        stacked=True\n",
      "    )\n",
      "# ax.yaxis.grid()\n",
      "ax.legend()\n",
      "ax.set_title(\"\", fontsize=14 )\n",
      "ax.set_xlabel(\"Coverage Factor\")\n",
      "ax.set_ylabel(\"Frequency\")\n",
      "fig.suptitle(\"\")\n",
      "\n",
      "output_file = \"cf-histogram-fs.pdf\"\n",
      "fig.savefig(output_file, bbox_inches=\"tight\")\n",
      "print \"Saved %s\" % output_file"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Calculate the cumulative distribution function (CDF) from the histogram data to get a probability distribution of observing a given coverage factor (CF)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in range(len(histogram[0])): # 4\n",
      "    sums = [0 for x in range(len(histogram[0][i]))]\n",
      "    for j in range(len(histogram[0][i])): # 13\n",
      "        sums[j] += histogram[0][i][j]\n",
      "probabilities = sums / sum(sums)\n",
      "cumul = 0.0\n",
      "print \"%5s %6s %10s %10s\" % (\"CF\", \"Value\", \"CDF\", \"1-CDF\")\n",
      "for idx, probability in enumerate(probabilities):\n",
      "    cumul += probability\n",
      "    print \"%5.3f %6.4f %10.4f %10.4f\" % ( histogram[1][idx], probability, cumul, 1.0 - cumul )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Cumulative Distribution Functions\n",
      "\n",
      "We now have a distribution for the coverage factor and performance.  What is the probability of getting anywhere near peak performance?  How does this vary with the coverage factor?\n",
      "\n",
      "First calculate the cumulative distribution function for keys of interest:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def calculate_cdf( values ):\n",
      "    \"\"\"Create a pandas.Series that is the CDF of a list-like object\"\"\"\n",
      "\n",
      "    denom = len(values) - 1\n",
      "    x = sorted(values / values.max())\n",
      "    y = [ i / denom for i in np.arange(len(x), dtype=np.float64) ]\n",
      "    return pandas.Series( y, index=x )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "### Calculate CDFs for (1) each file system and (2) performance and coverage factor\n",
      "cdfs = {}\n",
      "for fs in _FILE_SYSTEM_ORDER:\n",
      "    df_fs = df_concat[df_concat['darshan_file_system'] == fs]\n",
      "    for cdf_key in 'darshan_normalized_perf_by_max', 'coverage_factor':\n",
      "        if cdf_key not in cdfs:\n",
      "            cdfs[cdf_key] = {}\n",
      "        cdf = calculate_cdf(df_fs[cdf_key])\n",
      "        cdfs[cdf_key][fs] = {\n",
      "            'dependent_variable':  cdf.index,\n",
      "            'probability': cdf.values,\n",
      "        }"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### don't use counter_labels because the context is slightly different\n",
      "cdf_labels = {\n",
      "    'darshan_normalized_perf_by_max': \"Fraction Peak Performance\",\n",
      "    'coverage_factor': \"Coverage Factor\",\n",
      "}\n",
      "cdf_file_labels = {\n",
      "    'darshan_normalized_perf_by_max': \"perf\",\n",
      "    'coverage_factor': \"coverage-factor\",\n",
      "}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Plot CDF side-by-side\n",
      "\n",
      "fig, axes = plt.subplots(nrows=1, ncols=2, sharex=True, sharey=True)\n",
      "fig.set_size_inches(8,5)\n",
      "\n",
      "for fs in _FILE_SYSTEM_ORDER:\n",
      "    for idx, cdf_key in enumerate(sorted(cdfs.keys())):\n",
      "        axes[idx].plot(\n",
      "                       cdfs[cdf_key][fs]['dependent_variable'],\n",
      "                       cdfs[cdf_key][fs]['probability'],\n",
      "                       label=fs,\n",
      "                       linewidth=2)\n",
      "        axes[idx].set_xlabel(wrap(cdf_labels[cdf_key]), fontsize=14)\n",
      "        axes[idx].set_ylabel(\"\")\n",
      "        axes[idx].plot([0.0, 1.0],[0.5, 0.5], '--', linewidth=2.0, color='red')\n",
      "        axes[idx].legend().remove()\n",
      "        axes[idx].set_xticks([0.0, 0.25, 0.50, 0.75, 1.0])\n",
      "        for tick in axes[idx].xaxis.get_major_ticks():\n",
      "            tick.label.set_fontsize(16) \n",
      "        for tick in axes[idx].yaxis.get_major_ticks():\n",
      "            tick.label.set_fontsize(16)\n",
      "axes[1].xaxis.get_major_ticks()[0].set_visible(False)\n",
      "axes[0].set_title('(a)', position=(0.90,0.0), fontsize=24)\n",
      "axes[1].set_title('(b)', position=(0.90,0.0), fontsize=24)\n",
      "#fig.legend(\n",
      "axes[0].legend(\n",
      "    *(axes[0].get_legend_handles_labels()),\n",
      "    fontsize=16\n",
      "#   ncol=4,\n",
      "#   mode=\"expand\",\n",
      "#   bbox_to_anchor=(0.11, 0.0, 0.79, 1.0),\n",
      "#   loc=\"upper left\",\n",
      "#   borderaxespad=0.0\n",
      "          )\n",
      "axes[0].set_ylabel(\"Cumulative Probability\", fontsize=14)\n",
      "for i in 0, 1:\n",
      "    axes[i].set_ylim([0.0, 1.0])\n",
      "    axes[i].set_xlim([0.0, 1.0])\n",
      "    axes[i].grid(True)\n",
      "    axes[i].yaxis.set_ticks([0.0,0.25,0.50,0.75,1.0])\n",
      "fig.suptitle(\"\")\n",
      "# fig.text(0.0, 0.5, \"Cumulative Probability\", va='center', rotation='vertical')\n",
      "fig.subplots_adjust(hspace=0.0, wspace=0.0)\n",
      "\n",
      "output_file = \"cdf-both.pdf\"\n",
      "fig.savefig(output_file, bbox_inches=\"tight\")\n",
      "print \"Saved %s\" % output_file"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "IOPS Coverage Factor"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = df_mira.copy()\n",
      "df['iops_coverage_factor'] = (df_mira['darshan_total_rws'] / (df_mira['ggio_read_reqs'] + df_mira['ggio_write_reqs']))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = plt.figure()\n",
      "ax = fig.add_subplot('111')\n",
      "x_counter = 'coverage_factor'\n",
      "x_counter = 'darshan_normalized_perf_by_max'\n",
      "y_counter = 'iops_coverage_factor'\n",
      "ax.plot(df[x_counter],\n",
      "        df[y_counter],\n",
      "        marker='o',\n",
      "        alpha=1.0,\n",
      "        linewidth=0.0,\n",
      "       )\n",
      "result = ax.hexbin( df[x_counter],\n",
      "            df[y_counter],\n",
      "            gridsize=15,\n",
      "            cmap='hot_r'\n",
      "       )\n",
      "\n",
      "ax.set_xlabel(x_counter)\n",
      "ax.set_ylabel(y_counter)\n",
      "ax.grid()\n",
      "print \"\"\"\n",
      "When performance is high, iops coverage factor is high (so application has exclusive access to iops)\n",
      "When performance is low, iops coverage factor is low\n",
      "\"\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### CDF\n",
      "fig = plt.figure()\n",
      "fig.set_size_inches(8,5)\n",
      "ax = fig.add_subplot(\"111\")\n",
      "ax.plot(calculate_cdf( df_mira['iops_coverage_factor'] ), label='IOPS', lw=2.0)\n",
      "ax.plot(calculate_cdf( df_mira['coverage_factor'] ), label='Bandwidth', lw=2.0)\n",
      "ax.set_xlabel(\"Coverage Factor\")\n",
      "ax.set_ylabel(\"Cumulative Probability\")\n",
      "ax.legend()\n",
      "ax.grid()\n",
      "output_file = \"cdf-cf-bw-and-ops.pdf\"\n",
      "fig.savefig(output_file, bbox_inches=\"tight\")\n",
      "print \"Saved %s\" % output_file\n",
      "\n",
      "\n",
      "### Also try bar plots\n",
      "fig = plt.figure()\n",
      "fig.set_size_inches(8,5)\n",
      "ax = fig.add_subplot(\"111\")\n",
      "y1 = df_mira['iops_coverage_factor']\n",
      "y2 = df_mira['coverage_factor']\n",
      "common_opts = {\n",
      "                \"width\": 1.0/15.0,\n",
      "                \"bins\": np.linspace(0.0, 1.0, 15),\n",
      "                'alpha': 0.75,\n",
      "                'lw': 3.0,\n",
      "#                 'zorder': 9,\n",
      "              }\n",
      "\n",
      "# ax.hist( [ y1, y2 ], label=[\"IOPS\", \"Bandwidth\"], **common_opts)\n",
      "for y, label in [ \n",
      "                 (y1, 'IOPS'),\n",
      "                 (y2, 'Bandwidth'),\n",
      "\n",
      "                ]:\n",
      "    ax.hist( y, label=label, **common_opts)\n",
      "\n",
      "ax.set_xlabel(\"Coverage Factor\")\n",
      "ax.set_ylabel(\"Frequency\")\n",
      "ax.legend()\n",
      "ax.yaxis.grid()\n",
      "output_file = \"hist-cf-bw-and-ops.pdf\"\n",
      "fig.savefig(output_file, bbox_inches=\"tight\")\n",
      "print \"Saved %s\" % output_file"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}