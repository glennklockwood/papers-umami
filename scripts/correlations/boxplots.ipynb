{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boxplots for PDSW Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams.update({'font.size': 18})\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "import matplotlib.gridspec\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats as stats\n",
    "import scipy.interpolate\n",
    "import json\n",
    "import datetime\n",
    "import bisect\n",
    "import warnings\n",
    "import textwrap\n",
    "\n",
    "### black magic necessary for processing Mira log files :(\n",
    "try:\n",
    "    import pytz\n",
    "    _USE_TZ = True\n",
    "except ImportError:\n",
    "    _USE_TZ = False\n",
    "\n",
    "def wrap(text, width=15):\n",
    "    \"\"\"wrapper for the wrapper\"\"\"\n",
    "    return '\\n'.join(textwrap.wrap(text=text,width=width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def utc_timestamp_to_argonne( timestamp ):\n",
    "    \"\"\"\n",
    "    This is a batty function that allows us to compare the UTC-based\n",
    "    timestamps from Darshan logs (start_time and end_time) to the\n",
    "    Chicago-based YYYY-MM-DD dates used to index the mmdf data.\n",
    "    \"\"\"\n",
    "    if _USE_TZ:\n",
    "        ### we know that these logs are from Chicago\n",
    "        tz = pytz.timezone(\"America/Chicago\")\n",
    "        \n",
    "        ### Darshan log's start time in UTC, so turn it into a datetime with UTC on it\n",
    "        darshan_time = pytz.utc.localize(datetime.datetime.utcfromtimestamp(timestamp))\n",
    "        \n",
    "        ### Then convert this UTC start time into a local start time so\n",
    "        ### we can compare it to the local mmdf timestamp\n",
    "        darshan_time_at_argonne = darshan_time.astimezone(tz)\n",
    "        return darshan_time_at_argonne\n",
    "    else:\n",
    "        ### we assume that this script is running on Argonne time; it's the best we can do\n",
    "        warnings.warn(\"pytz is not available so mmdf data might be misaligned by a day!\")\n",
    "        return datetime.datetime.fromtimestamp(timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Relative path to the repository's root directory\n",
    "_REPO_BASE_DIR = os.path.join('..', '..')\n",
    "\n",
    "### Translates cryptic counter names into something suitable for labeling plots\n",
    "counter_labels = json.load(open(os.path.join(_REPO_BASE_DIR, 'scripts', 'counter_labels.json'), 'r'))\n",
    "\n",
    "### For consistency, always plot file systems in the same order\n",
    "_FILE_SYSTEM_ORDER = [ 'scratch1', 'scratch2', 'scratch3', 'mira-fs1' ]\n",
    "\n",
    "_INPUT_EDISON_DATA_CSV = os.path.join(_REPO_BASE_DIR,\n",
    "                                      'data',\n",
    "                                      'dat',\n",
    "                                      'tokio-lustre',\n",
    "                                      'edison-abc-stats_2-14_3-28_v2.csv')\n",
    "_INPUT_MIRA_DATA_CSV = os.path.join(_REPO_BASE_DIR,\n",
    "                                    'data',\n",
    "                                    'dat',\n",
    "                                    'tokio-gpfs',\n",
    "                                    'alcf-abc-stats_2-25_3-27.dat')\n",
    "_INPUT_MIRA_MMDF_CSV = os.path.join(_REPO_BASE_DIR,\n",
    "                                    'data',\n",
    "                                    'dat',\n",
    "                                    'tokio-gpfs',\n",
    "                                    'mira_mmdf_1-25_3-27.csv')\n",
    "\n",
    "_COVERAGE_FACTOR_CUTOFF = 1.2\n",
    "\n",
    "_MIRA_JOBS_BLACKLIST = [ 1039807 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### This is a time range that encompasses data collected on both Edison and Mira\n",
    "\n",
    "# start time is inclusive\n",
    "_START_TIME = datetime.datetime(2017, 2, 24, 0, 0, 0)\n",
    "# end time is exclusive\n",
    "_END_TIME   = datetime.datetime(2017, 3, 26, 0, 0, 0)\n",
    "# _START_TIME = _END_TIME = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### All time\n",
    "\n",
    "# start time is inclusive\n",
    "_START_TIME = datetime.datetime(2016, 2, 24, 0, 0, 0)\n",
    "# end time is exclusive\n",
    "_END_TIME   = datetime.datetime(2017, 3, 26, 0, 0, 0)\n",
    "# _START_TIME = _END_TIME = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "We've been storing most of the per-job summary data in a single CSV per system.  We\n",
    "\n",
    "1. Load the CSV directly into a dataframe\n",
    "2. Drop any rows containing NANs, because if any of the core data is missing (e.g., application name), the whole record is useless.  Hopefully I haven't overlooked anything important in this assumption.\n",
    "3. Synthesize a few new columns (we call these \"metrics\" in the paper) to facilitate downstream analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Edison\n",
    "df_edison = pandas.DataFrame.from_csv(_INPUT_EDISON_DATA_CSV).dropna()\n",
    "df_edison['darshan_rw'] = [ 'write' if x == 1 else 'read' for x in df_edison['darshan_write_mode?'] ]\n",
    "df_edison['darshan_file_mode'] = [ 'shared' if x in ['H5Part','MPIIO'] else 'fpp' for x in df_edison['darshan_api'] ]\n",
    "df_edison.rename(columns={'lmt_bytes_covered': 'coverage_factor'}, inplace=True)\n",
    "df_edison['system'] = \"edison\"\n",
    "df_edison['iops_coverage_factor'] = -1.0\n",
    "df_edison['nodehr_coverage_factor'] = df_edison['job_num_nodes'] * \\\n",
    "                                      (df_edison['darshan_end_time'] - df_edison['darshan_start_time']) / 3600.0 / \\\n",
    "                                      (df_edison['job_concurrent_nodehrs'])\n",
    "\n",
    "\n",
    "### Mira\n",
    "df_mira = pandas.DataFrame.from_csv(_INPUT_MIRA_DATA_CSV).dropna()\n",
    "rename_dict = { '# platform': \"system\" }\n",
    "for key in df_mira.keys():\n",
    "    if key == 'file_sys':\n",
    "        rename_dict[key] = 'darshan_file_system'\n",
    "    elif key not in rename_dict and not key.startswith('ggio_'):\n",
    "        rename_dict[key] = 'darshan_' + key\n",
    "df_mira.rename(columns=rename_dict, inplace=True)\n",
    "df_mira['darshan_file_mode'] = [ 'shared' if x in ['H5Part','MPIIO'] else 'fpp' for x in df_mira['darshan_api'] ]\n",
    "df_mira['coverage_factor'] = df_mira['darshan_total_bytes'] / (df_mira['ggio_bytes_read'] + df_mira['ggio_bytes_written'])\n",
    "df_mira['iops_coverage_factor'] = (df_mira['darshan_total_rws'] / (df_mira['ggio_read_reqs'] + df_mira['ggio_write_reqs']))\n",
    "df_mira['nodehr_coverage_factor'] = -1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because I'm lazy, load the `mmdf` data separately and attach it to `df_mira`.  The mmdf CSV is generated by\n",
    "\n",
    "1. retrieving all of the `df_fs1_*.txt` files from `mira:/projects/radix-io/automated/runs/gpfs-logs/`\n",
    "2. running `tokio-cron-benchmarks:utils/parse_mmdf.py` script against `df_fs1_*.txt` (note that although `parse_mmdf.py` can distinguish between different file systems, this script currently doesn't filter for the correct `file_system` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_mmdf = pandas.DataFrame.from_csv(_INPUT_MIRA_MMDF_CSV, index_col=['file_system', 'date'])\n",
    "df_mmdf['free_kib'] = df_mmdf['free_kib_blocks'] + df_mmdf['free_kib_frags']\n",
    "df_mmdf['free_pct'] = df_mmdf['free_kib'] / df_mmdf['disk_size']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Walk the master dataframe and attach mmdf data.  Note that we're injecting NAs for missing mmdf data because missing mmdf data should not exclude the entire day from our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### I really hope iterrows behaves deterministically and preserves order...\n",
    "new_data = {\n",
    "    'mmdf_avg_fullness_pct': [],\n",
    "    'mmdf_max_fullness_pct': [],\n",
    "}\n",
    "\n",
    "### iterate over each row of the master Mira dataframe\n",
    "no_data = set([])\n",
    "for row in df_mira.itertuples():\n",
    "    fs_key = row.darshan_file_system\n",
    "    mmdf_key = utc_timestamp_to_argonne( row.darshan_start_time ).strftime(\"%Y-%m-%d\")\n",
    "    if mmdf_key in df_mmdf.loc[fs_key].index:\n",
    "        ### only look at today's data\n",
    "        df = df_mmdf.loc[fs_key].loc[mmdf_key]\n",
    "        \n",
    "        data_cols = [ True if x else False for x in df['data?'] ]\n",
    "\n",
    "        ### calculate a percent fullness - don't bother saving the id of this fullest server though\n",
    "        new_data['mmdf_max_fullness_pct'].append( 1.0 - df[ data_cols ]['free_pct'].min() )\n",
    "        new_data['mmdf_avg_fullness_pct'].append( 1.0 - df[ data_cols ]['free_pct'].mean() )\n",
    "    else:\n",
    "        no_data.add( datetime.datetime.fromtimestamp(row.darshan_start_time).strftime(\"%Y-%m-%d\") )\n",
    "        new_data['mmdf_max_fullness_pct'].append( np.nan )\n",
    "        new_data['mmdf_avg_fullness_pct'].append( np.nan )\n",
    "\n",
    "warnings.warn(\"No MMDF data found for the following dates:\\n\" + '\\n'.join(no_data))\n",
    "        \n",
    "for new_col_name, new_col_data in new_data.iteritems():\n",
    "    df_mira[new_col_name] = new_col_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now merge both DataFrames so we can look at all the data if we really want to.  This DataFrame will have a bunch of NANs for data that is only applicable to Mira or Edison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_concat = pandas.concat( (df_mira, df_edison) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Data\n",
    "\n",
    "Two notable filters are applied:\n",
    "\n",
    "1. All jobs where the bytes coverage factor and ops coverage factor are greater than 1.2 are discarded because they reflect severely misaligned or gappy data.\n",
    "\n",
    "2. Mira job 1039807 is excluded because ggiostat returned highly abnormal results starting that day.  See e-mail from Shane and Phil on March 23 about this.\n",
    "\n",
    "3. All Edison jobs from March 12 were discarded because LMT broke as a result of daylight saving time rolling over.  This filter was applied _before_ the input CSV files loaded above were generated, so it does not need to be applied here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for df in df_mira, df_edison, df_concat:\n",
    "    print '+'.join( df['system'].unique() )\n",
    "    print ''.join([ '=' * 50])\n",
    "    \n",
    "    count = len(df)\n",
    "    df.drop(df.index[df['coverage_factor'] > _COVERAGE_FACTOR_CUTOFF], inplace=True)\n",
    "    print \"Dropped %d records due to coverage factor > %.1f\" % (count - len(df), _COVERAGE_FACTOR_CUTOFF)\n",
    "    \n",
    "    count = len(df)\n",
    "    for blacklisted_job in _MIRA_JOBS_BLACKLIST:\n",
    "        df.drop(df.index[(df['system'] == 'mira') & (df['darshan_jobid'] == blacklisted_job)], inplace=True)\n",
    "    print \"Dropped %d records due to Mira job #1039807\" % (count - len(df))\n",
    "\n",
    "    count = len(df)\n",
    "    if _END_TIME is not None and _START_TIME is not None:\n",
    "        filter_time = [ datetime.datetime.fromtimestamp(df['darshan_start_time'][x]) < _START_TIME                    \n",
    "                        or\n",
    "                        datetime.datetime.fromtimestamp(df['darshan_start_time'][x]) > _END_TIME\n",
    "                        for x in df.index ]\n",
    "        df.drop(df.index[filter_time], inplace=True)\n",
    "    print \"Dropped %d records outside of time range %s - %s\" % (count - len(df), _START_TIME, _END_TIME)\n",
    "    print \"Total measurements to be analyzed:\", len(df)\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for df in df_mira, df_edison, df_concat:\n",
    "    print '+'.join( df['system'].unique() )\n",
    "    print ''.join([ '=' * 50])\n",
    "    print \"Earliest timestamp now\", datetime.datetime.fromtimestamp(df['darshan_start_time'].min())\n",
    "    print \"Latest timestamp now\", datetime.datetime.fromtimestamp(df['darshan_start_time'].max())\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize Performance\n",
    "Different file systems, benchmarks, and read/write modes are capable of different peak bandwidths.  As such, we want to normalize the absolute performance (`summarize_key`) by something.  For convenience we calculate the denominator for normalization a couple of different ways (e.g., the mean, median, and max measurement).  We also limit normalization to unique combinations of variables specified by `normalization_group` below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the normalization factors (the denominators), then apply that factor to all of the data in the DataFrame.  These normalized data will be saved as new columns in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### The actual variable we want to normalize\n",
    "summarize_key = 'darshan_agg_perf_by_slowest'\n",
    "\n",
    "for df in df_edison, df_mira, df_concat:\n",
    "    ### Specify which keys we want to group together before normalizing\n",
    "    normalization_group = df.groupby(['darshan_app', 'darshan_file_system', 'darshan_file_mode', 'darshan_rw'])\n",
    "\n",
    "    ### Dict to store the denominators for normalization\n",
    "    normalization_data = {\n",
    "        'mean':   normalization_group.darshan_agg_perf_by_slowest.mean(),\n",
    "        'median': normalization_group.darshan_agg_perf_by_slowest.median(),\n",
    "        'max':    normalization_group.darshan_agg_perf_by_slowest.max(),\n",
    "    }\n",
    "\n",
    "    ### Normalize every row in the DataFrame by all of our denominators\n",
    "    new_cols = {}\n",
    "    for func in normalization_data.keys():\n",
    "        new_col_key = 'darshan_normalized_perf_by_%s' % func\n",
    "        new_cols[new_col_key] = []\n",
    "        for index, row in df.iterrows():\n",
    "            new_cols[new_col_key].append(\n",
    "                row[summarize_key] / normalization_data[func]\n",
    "                                                       [row['darshan_app']]\n",
    "                                                       [row['darshan_file_system']]\n",
    "                                                       [row['darshan_file_mode']]\n",
    "                                                       [row['darshan_rw']]\n",
    "            )\n",
    "\n",
    "    ### Also just do per-file system\n",
    "    normalization_group = df.groupby('darshan_file_system')\n",
    "    normalization_data = {\n",
    "        'mean':   normalization_group.darshan_agg_perf_by_slowest.mean(),\n",
    "        'median': normalization_group.darshan_agg_perf_by_slowest.median(),\n",
    "        'max':    normalization_group.darshan_agg_perf_by_slowest.max(),\n",
    "    }\n",
    "    for func in normalization_data.keys():\n",
    "        new_col_key = 'darshan_normalized_perf_by_fs_%s' % func\n",
    "        new_cols[new_col_key] = []\n",
    "        for index, row in df.iterrows():\n",
    "            new_cols[new_col_key].append(\n",
    "                row[summarize_key] / normalization_data[func][row['darshan_file_system']])\n",
    "\n",
    "    ### Take our normalized data and add them as new columns\n",
    "    for new_col, new_col_data in new_cols.iteritems():\n",
    "        df[new_col] = new_col_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Correlation Analysis\n",
    "`performance_key` is the variable we wish to use to represent performance.  It is typically\n",
    "\n",
    "- `darshan_agg_perf_by_slowest`, which is the absolute performance measured by each benchmark run\n",
    "- `darshan_normalized_perf_by_max`, which is normalized by the maximum observed performance\n",
    "- `darshan_normalized_perf_by_mean`, which is normalized by the mean observed performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "performance_key = 'darshan_normalized_perf_by_max'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Correlation Analysis\n",
    "\n",
    "Pearson analysis assumes that each variable is normally distributed.  It is easier to understand, but it is not technically correct for variables that are _not_ normally distributed, which include performance.  The Spearman coefficient would be better and can be enabled below.\n",
    "\n",
    "We use `scipy.stats` to calculate p-values associated with each correlation.  The ultimate artifact of this process is a table of interesting correlations, their correlation coefficients, and color coding to indicate the confidence of those coefficients based on p-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ignore_cols = [\n",
    "    'lmt_tot_zeros',\n",
    "    'lmt_frac_zeros',\n",
    "    'lmt_frac_missing',\n",
    "    'ost_avg_kib',\n",
    "    'ost_min_pct',\n",
    "    'ost_min_kib',\n",
    "    'ost_max_kib',\n",
    "    'ost_count',\n",
    "#   'ost_bad_ost_count',\n",
    "    'ost_bad_ost_pct',\n",
    "    'ost_failures_lead_secs',\n",
    "    'ost_fullness_lead_secs',\n",
    "    'lmt_tot_missing',\n",
    "    'ost_avg_bad_ost_per_oss',\n",
    "    'ost_avg_bad_overload_factor',\n",
    "    'ost_bad_oss_count',\n",
    "    'ost_min_id',\n",
    "    'ost_max_id',\n",
    "    'job_min_radius',\n",
    "    'job_avg_radius',\n",
    "### second pass\n",
    "    'lmt_ops_getattrs',\n",
    "    'lmt_ops_getxattrs',\n",
    "    'lmt_ops_rmdirs',\n",
    "    'lmt_ops_unlinks',\n",
    "    'lmt_ops_renames',\n",
    "    'lmt_ops_setattrs',\n",
    "    'lmt_ops_mkdirs',\n",
    "    'ggio_inoded_updates',\n",
    "### third pass\n",
    "    \"lmt_mds_ave\",\n",
    "    \"lmt_oss_ave\",\n",
    "]\n",
    "\n",
    "### if one key has the same logical meaning as another, this will remap those\n",
    "### keys so they line up in the DataFrame\n",
    "equivalent_keys = {\n",
    "    'ggio_closes':     'lmt_ops_closes',\n",
    "    'ggio_opens': 'lmt_ops_opens',\n",
    "    'ggio_bytes_read': 'lmt_tot_bytes_read',\n",
    "    'ggio_bytes_written': 'lmt_tot_bytes_write',\n",
    "    'mmdf_max_fullness_pct': 'ost_max_pct',\n",
    "    'mmdf_avg_fullness_pct': 'ost_avg_pct',\n",
    "}\n",
    "\n",
    "### Specific names for the table\n",
    "counter_labels_table = {\n",
    "    'coverage_factor': \"Coverage Factor (Bandwidth)\",\n",
    "    'nodehr_coverage_factor': \"Coverage Factor (NodeHrs)\",\n",
    "    \"ost_avg_pct\": \"Avg LUN Fullness\",\n",
    "    \"ost_max_pct\": \"Fullness on Fullest LUN\",\n",
    "    \"lmt_oss_max\": \"Max CPU Load, Data Server\",\n",
    "    \"ost_bad_pct\": \"% Servers Failed Over\",\n",
    "    \"ost_bad_ost_count\": \"Failed-over Servers\",\n",
    "    \"lmt_ops_closes\": \"close(2) Calls\",\n",
    "    \"lmt_ops_opens\": \"open(2) Calls\",\n",
    "    \"lmt_tot_bytes_write\": \"Bytes Written\",\n",
    "    \"lmt_tot_bytes_read\": \"Bytes Read\",\n",
    "    \"lmt_mds_max\": \"Max CPU Load, Metadata Server\",\n",
    "    \"lmt_mds_ave\": \"Avg CPU Load, Metadata Server\",\n",
    "    \"job_concurrent_jobs\": \"# Concurrent Jobs\",\n",
    "    \"lmt_oss_ave\": \"Avg CPU Load, Data Server\",\n",
    "    \"job_max_radius\": \"Job Diameter\",\n",
    "    \"iops_coverage_factor\": \"Coverage Factor (IOPs)\",\n",
    "    \"ggio_write_reqs\": \"Write Ops\",\n",
    "    \"ggio_read_reqs\": \"Read Ops\",\n",
    "    \"ggio_read_dirs\": \"readdir(3) Calls\",\n",
    "}\n",
    "\n",
    "### Order in which table is to be printed\n",
    "print_order = [\n",
    "    'coverage_factor',\n",
    "    'nodehr_coverage_factor',\n",
    "    \"iops_coverage_factor\",\n",
    "    \"lmt_ops_closes\",\n",
    "    \"lmt_ops_opens\",\n",
    "#   \"ggio_read_dirs\",\n",
    "    \"lmt_tot_bytes_write\",\n",
    "    \"lmt_tot_bytes_read\",\n",
    "    \"ggio_write_reqs\",\n",
    "    \"ggio_read_reqs\",\n",
    "    \"ost_max_pct\",\n",
    "    \"ost_avg_pct\",\n",
    "    \"lmt_mds_max\",\n",
    "    \"lmt_oss_max\",\n",
    "    \"ost_bad_ost_count\",\n",
    "    \"job_concurrent_jobs\",\n",
    "#   \"job_max_radius\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of each benchmark type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following cell defines which variable we wish to aggregate into boxplots and a few plotting parameters that depend on our choice of variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boxplot_settings = {\n",
    "    'fontsize': 15,\n",
    "    'darshan_normalized_perf_by_fs_max': {\n",
    "        'output_file': \"perf-boxplots-per-fs.pdf\",\n",
    "        'ylabel': \"Fraction of Peak\\nFile System Performance\",\n",
    "        'title_pos': [ \n",
    "            {'x': 0.97, 'y': 0.80, 'horizontalalignment': 'right', 'fontsize': 14},\n",
    "            {'x': 0.04, 'y': 0.02, 'horizontalalignment': 'left', 'fontsize': 14}]\n",
    "    },\n",
    "    'darshan_normalized_perf_by_max': {\n",
    "        'output_file': \"perf-boxplots.pdf\",\n",
    "        'ylabel': \"Fraction of Peak Performance\",\n",
    "        'title_pos': [ \n",
    "            {'x': 0.04, 'y': 0.02, 'horizontalalignment': 'left', 'fontsize': 14},\n",
    "            {'x': 0.04, 'y': 0.02, 'horizontalalignment': 'left', 'fontsize': 14}]\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot two sets of boxplots based on the normalization denominator:\n",
    "\n",
    "1. normalized by the maximum without any grouping (other than file system)\n",
    "2. normalized by the maximum of each unique combination of app-read/write-filemode\n",
    "\n",
    "The idea is to show that\n",
    "\n",
    "1. performance variation varies across different file systems and different applications\n",
    "2. even within an application, the magnitude of performance variation varies with file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_boxplox(df_concat, fs, plot_variable, ax, irow, icol, other_settings={}):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "        separate = True\n",
    "        other_settings.update({\n",
    "            'boxprops': {'linewidth': 2},\n",
    "            'medianprops': {'linewidth': 4},\n",
    "            'whiskerprops': {'linewidth': 2},\n",
    "            'capprops': {'linewidth': 2},\n",
    "        })\n",
    "    else:\n",
    "        separate = False\n",
    "        other_settings.update({'medianprops': {'linewidth':2}})\n",
    "    boxplot_group_by = [ 'darshan_file_mode', 'darshan_rw', 'darshan_app' ]\n",
    "    df_concat.loc[df_concat[\"darshan_file_system\"] == fs]\\\n",
    "    .boxplot(\n",
    "        column=[plot_variable],\n",
    "        by=boxplot_group_by,\n",
    "        ax=ax,\n",
    "        widths=0.75,\n",
    "        whis=[5,95],\n",
    "        showfliers=False,\n",
    "        **(other_settings))\n",
    "\n",
    "    if separate:\n",
    "        settings = {'x': 0.04, 'y': 0.02, 'horizontalalignment': 'left', 'fontsize': 24}\n",
    "    else:\n",
    "        settings = boxplot_settings[plot_variable]['title_pos'][irow]\n",
    "    title = ax.set_title(fs, **(settings))\n",
    "    title.set_bbox({'color': 'white', 'alpha': 0.5})\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"\")\n",
    "    ax.xaxis.grid(True)\n",
    "\n",
    "    ### relabel the x axis labels\n",
    "    new_labels = []\n",
    "    for axis_label in ax.get_xticklabels():\n",
    "        current_label = axis_label.get_text()\n",
    "        axis_label.set_rotation(90)\n",
    "        if \"IOR\" in current_label:\n",
    "            if \"shared\" in current_label:\n",
    "                new_label = \"IOR/shared\"\n",
    "            else:\n",
    "                new_label = \"IOR/fpp\"\n",
    "        elif 'BD-CATS' in current_label:\n",
    "            new_label = \"BD-CATS\"\n",
    "        else:\n",
    "            new_label = current_label.split(',')[2].strip(')').strip().split('-')[0]\n",
    "        if 'write' in current_label:\n",
    "            new_label += \"(W)\"\n",
    "        else:\n",
    "            new_label += \"(R)\"\n",
    "        new_labels.append(new_label)\n",
    "\n",
    "    ### set x tick labels for only the bottom row\n",
    "    if irow == 0 and not separate:\n",
    "        ax.set_xticklabels([])\n",
    "    else:\n",
    "        ax.set_xticklabels(new_labels,\n",
    "                           fontsize=boxplot_settings['fontsize'])\n",
    "    if separate:\n",
    "        ax.yaxis.set_ticks( np.linspace(0.0, 1.0, 6) )\n",
    "    else:\n",
    "        ax.yaxis.set_ticks( np.linspace(0.0, 1.0, 3) )\n",
    "    ax.set_ylim([-0.1, 1.1])\n",
    "    for ytick in ax.yaxis.get_major_ticks():\n",
    "        ytick.label.set_fontsize(boxplot_settings['fontsize'])\n",
    "        \n",
    "    if separate:\n",
    "        return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for plot_variable in performance_key, performance_key.replace('_by_', '_by_fs_'):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=2, sharex=True, sharey=True)\n",
    "    fig.set_size_inches(8,6)\n",
    "    for idx, fs in enumerate(_FILE_SYSTEM_ORDER):\n",
    "        icol = idx / 2\n",
    "        irow = idx % 2\n",
    "        ax = axes[irow, icol]\n",
    "        create_boxplox(df_concat, fs, plot_variable, ax, irow, icol)\n",
    "        \n",
    "    fig.suptitle(\"\")\n",
    "    # fig.text(0.5, -0.4, 'common X', ha='center')\n",
    "    fig.text(0.0, 0.5,\n",
    "             boxplot_settings[plot_variable]['ylabel'],\n",
    "             verticalalignment='center',\n",
    "             horizontalalignment='center',\n",
    "             rotation='vertical',\n",
    "             fontsize=boxplot_settings['fontsize'])\n",
    "    fig.subplots_adjust(hspace=0.05, wspace=0.05)\n",
    "\n",
    "    output_file = boxplot_settings[plot_variable]['output_file']\n",
    "    fig.savefig(output_file, bbox_inches=\"tight\")\n",
    "    print \"Saved %s\" % output_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Break out each box plot as its own standalone plot for the purposes of the powerpoint presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boxplot_settings = {\n",
    "    'fontsize': 20,\n",
    "    'darshan_normalized_perf_by_max': {\n",
    "        'output_file': \"perf-boxplots.pdf\",\n",
    "        'ylabel': \"Fraction of\\nPeak Performance\",\n",
    "        'title_pos': [ \n",
    "            {'x': 0.97, 'y': 0.80, 'horizontalalignment': 'right', 'fontsize': 14},\n",
    "            {'x': 0.04, 'y': 0.02, 'horizontalalignment': 'left', 'fontsize': 14}]\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for plot_variable in [performance_key]:\n",
    "    for idx, fs in enumerate(_FILE_SYSTEM_ORDER):\n",
    "        fig, ax = create_boxplox(df_concat, fs, plot_variable, None, 0, 0)\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), fontsize=boxplot_settings['fontsize']*0.9, rotation=30, ha='right')\n",
    "        fig.suptitle(\"\")\n",
    "        fig.text(-0.05, 0.5,\n",
    "                 boxplot_settings[plot_variable]['ylabel'],\n",
    "                 verticalalignment='center',\n",
    "                 horizontalalignment='center',\n",
    "                 rotation='vertical',\n",
    "                 fontsize=boxplot_settings['fontsize']*0.9)\n",
    "        fig.subplots_adjust(hspace=0.05, wspace=0.05)\n",
    "\n",
    "        output_file = boxplot_settings[plot_variable]['output_file'].replace('.pdf', '_%s.pdf' % fs)\n",
    "        fig.savefig(output_file, bbox_inches=\"tight\")\n",
    "        print \"Saved %s\" % output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_variable = 'darshan_normalized_perf_by_max'\n",
    "\n",
    "boxplot_settings = {\n",
    "    'fontsize': 20,\n",
    "    'darshan_normalized_perf_by_max': {\n",
    "        'output_file': \"perf-boxplots-x3.pdf\",\n",
    "        'ylabel': \"Fraction of\\nPeak Performance\",\n",
    "        'title_pos': [ \n",
    "            {'x': 0.97, 'y': 0.80, 'horizontalalignment': 'right', 'fontsize': 14},\n",
    "            {'x': 0.04, 'y': 0.02, 'horizontalalignment': 'left', 'fontsize': 14}]\n",
    "    },\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, sharey=True)\n",
    "fig.set_size_inches(14,4)\n",
    "for icol, fs in enumerate(['mira-fs1', 'scratch3', 'scratch2']):\n",
    "    ax = axes[icol]\n",
    "    create_boxplox(df_concat, fs, plot_variable, ax, irow, icol)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), fontsize=boxplot_settings['fontsize']*0.8, rotation=30, ha='right')\n",
    "    fig.suptitle(\"\")\n",
    "#   fig.text(-0.05, 0.5,\n",
    "#            boxplot_settings[plot_variable]['ylabel'],\n",
    "#            verticalalignment='center',\n",
    "#            horizontalalignment='center',\n",
    "#            rotation='vertical',\n",
    "#            fontsize=boxplot_settings['fontsize']*0.9)\n",
    "    fig.subplots_adjust(hspace=0.05, wspace=0.05)\n",
    "\n",
    "fig.suptitle(\"\")\n",
    "# fig.text(0.5, -0.4, 'common X', ha='center')\n",
    "fig.text(0.03, 0.5,\n",
    "         boxplot_settings[plot_variable]['ylabel'],\n",
    "         verticalalignment='center',\n",
    "         horizontalalignment='center',\n",
    "         rotation='vertical',\n",
    "         fontsize=boxplot_settings['fontsize']*0.9)\n",
    "fig.subplots_adjust(hspace=0.05, wspace=0.05)\n",
    "\n",
    "output_file = boxplot_settings[plot_variable]['output_file']\n",
    "fig.savefig(output_file, bbox_inches=\"tight\", transparent=True)\n",
    "print \"Saved %s\" % output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
