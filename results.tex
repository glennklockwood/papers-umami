\section{System-Level Analysis} \label{sec:results}

We begin our analysis by using TOKIO and TOKIO-ABC to perform a broad study of the overall I/O climate of each system.
The goals of this analysis are to
establish baseline expectations for performance and variability and
formally quantify the relationships between I/O components.
%
%These two factors are critical to interpreting I/O performance, but they have been traditionally documented and communicated in informal anecdotes and institutional expert knowledge.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Statistical Overview} \label{sec:results/overview}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%To first establish a baseline of expected application performance from which we can gauge variation, we evaluate the overall behavior of the TOKIO-ABC workloads on all of the test systems.
We evaluate the overall behavior of the TOKIO-ABC workloads in order to
establish baseline performance.
Figure \ref{fig:perf-summary-boxplots-fs} shows the distribution of performance measured by all TOKIO-ABC workloads normalized to the highest performance observed on each file system.
Mira's distribution of variation is the most narrow, with all benchmarks' 25th percentiles well above 50\% of the peak performance.
By comparison, shared-file I/O performance (BD-CATS, IOR/shared, and VPIC) on Edison's file systems is appreciably lower than the maximum observed peak, which correspond to file-per-process read workloads (HACC and IOR/fpp).
Although the source of this disparity in variation between Mira and Edison is not clear from Figure \ref{fig:perf-summary-boxplots-fs} alone, we explore the underlying causes in subsequent sections.

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\columnwidth]{figs/perf-boxplots-per-fs.pdf}
    \caption{TOKIO-ABC I/O performance for Edison (scratch1, scratch2, scratch3) and Mira (mira-fs1), normalized to the maximum performance of all tests performed on each file system.
    Each box reflects the distribution of all four application workloads and both read and write performance.
    Whiskers extend to the 5th and 95th percentiles.}
    \label{fig:perf-summary-boxplots-fs}
\vspace{-.2in}
\end{figure}

Such variation in peak file system performance caused by different I/O access patterns is well documented\cite{Lofstead2010,Uselton2010,Xie2012}.
To focus solely on the variation caused by factors \emph{extrinsic} to each application, we then define the \emph{fraction of peak performance} as the performance of a job divided by the maximum performance observed for all jobs \emph{of the same I/O motif} as listed in Table \ref{tab:bench-config} and whether the job did reads or writes.
For example, the fraction peak performance for a HACC write test is only normalized to the maximum performance of all other HACC write tests on the same file system.
References to fraction peak performance are hereafter defined this way.

This fraction peak performance distribution, shown in Figure~\ref{fig:perf-summary-boxplots-motif}, reveals that the degree of performance variation \emph{within} each application also varies with each file system.
For example, the HACC write workload is susceptible to a long tail of performance degradation on mira-fs1 despite that file system's overall lower variation shown in Figure~\ref{fig:perf-summary-boxplots-fs}.
Similarly, all Edison file systems show a long tail of performance loss for the IOR/shared file read workload.
Edison's scratch3 also demonstrates very broad performance variation for the VPIC write workload, contrasting with the relatively narrow performance variation of this application on other systems.

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\columnwidth]{figs/perf-boxplots.pdf}
    \caption{TOKIO-ABC I/O performance for all file systems tested grouped by test
    applications and read/write mode.  Whiskers represent the 5th and 95th
    percentiles.}
    \label{fig:perf-summary-boxplots-motif}
\vspace{-.2in}
\end{figure}

Thus, Figure \ref{fig:perf-summary-boxplots-motif} demonstrates that 
performance variability is the result of factors intrinsic to the application
(e.g., I/O motif) \emph{and} factors intrinsic to the file system. 
Different I/O motifs result in different levels of performance \emph{and} variability.
Furthermore, these behaviors are not a function of the parallel file system software architecture either; all Edison file systems are Lustre-based, yet there is a marked difference in variability between scratch1/scratch and scratch3 shown in Figure~\ref{fig:perf-summary-boxplots-motif}.
Thus, these differences in performance variation must be a result of their different hardware configurations (discussed in Section \ref{sec:platforms}), their specific user workloads, or a combination of both.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Combined Application and Server Telemetry} \label{sec:results/combining}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

To obtain a better understanding of how performance variation is caused by factors extrinsic to the application, we then combine data from application-level analysis provided by Darshan with storage system traffic analysis provided by LMT (Edison) and ggiostat (Mira).
An intuitive source of performance loss would be from other jobs that are also consuming file system bandwidth, so to explore the effects of competing I/O traffic, we define the \emph{coverage factor} ($\mathit{CF}$) of a job $j$:

\begin{equation} \label{eq:cf}
    \mathit{CF}(j) = \frac{N_{\textup{bytes}}^{\textup{Darshan}}(j)}
    {\sum_{t,s}^{\textup{time,servers}}
    \left [ N_{\textup{bytes}}^{\textup{LMT,ggiostat}}(t,s) \right ] }
\end{equation}
%
where $N_{\textup{bytes}}^{\textup{Darshan}}$ is the number of bytes read and written by job $j$ according to its Darshan log, and $N_{\textup{bytes}}^{\textup{LMT,ggiostat}}$ is the number of bytes read and written to a parallel file system server $s$ during a 5-second time interval $t$.
The time interval over which the job ran ($\mathit{time}$) and the servers to which the job wrote ($\mathit{servers}$) are both taken from the job's Darshan log~\cite{snyder2016modular}.

The coverage factor is a direct reflection of how much I/O traffic a job competed against in the underlying file systems.
$CF = 1.0$ when all of the server-side I/O was caused by job $j$, while $CF = 0.5$ indicates that only half of the server-side I/O was attributable to job $j$ while the other half came from other sources.
In practice, it is possible to observe $CF > 1.0$ if
a) storage system traffic monitoring (e.g., LMT or ggiostat) did not capture data from all servers during a polling interval, or
b) clock skew existed between the compute nodes and the file system servers, causing the Darshan log and LMT/ggiostat to have an inconsistent understanding of when I/O happened.
To eliminate obviously erroneous data, we discard all test results where $CF > 1.2$ in the subsequent analysis.
% \todo{We should probably mention what \% of data this covers. It will be a likely question.}
%%% GKL: I don't want to admit to this unless a reviewer specifically asks.  The ratio is very high on Mira; 96 of the 214 measurements provided by Shane were dropped due to this  filter criterion

The distribution of coverage factors across all experiments run are shown in Figure~\ref{fig:cdfs}a which reveals that the majority of tests ($> 75\%$ on Edison and $> 80\%$ on Mira) have high coverage factors ($\mathit{CF} > 0.80$).
This is consistent with the observation that I/O occurs in bursts~\cite{Carns2011,Liu2016}, and the probability of two bursts coinciding and causing contention for bandwidth (thereby reducing $\mathit{CF}$) is relatively low.
In particular, Mira's CF distribution is so narrow that over 50\% of tests effectively ran completely uncontended for bandwidth; $\mathit{CF} >= 0.99$ corresponds to the 40th percentile on that system.

Despite this low incidence of overlapping bursts, though, the cumulative
distribution function of performance relative to the peak observed
bandwidth for each application (Figure~\ref{fig:cdfs}b) is more broadly distributed.
Edison's scratch3 exemplifies this; 26\% of jobs on that file system got less than half of the peak performance (fraction peak performance $< 0.50$) despite only 5\% of jobs showing $\mathit{CF} < 0.50$.  This indicates that the coverage factor (and therefore server-side I/O bandwidth) is not the only contributor to sub-optimal performance.  
This finding is consistent with the work of Uselton and Wright\cite{Uselton2013} who demonstrated that Lustre file system performance is constrained by the weighted sum of both bandwidth \emph{and} I/O operation (IOP) rate.  

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figs/cdf-both.pdf}
    \caption{Cumulative distribution function of the coverage factor (a) and the    performance relative to the maximum throughput observed across each file system (b).
    The line demarcating 50\% probability corresponds to coverage factors of 0.88, 0.87, 0.84, and 0.94 and peak performance fractions of 0.89, 0.86, 0.78, and 0.95 on Edison scratch1-scratch3 and Mira, respectively.}
    \label{fig:cdfs}
\vspace{-.2in}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figs/hist-cf-bw-and-ops.pdf}
    \caption{Distribution of the coverage factor for both bandwidth ($\textit{CF}_{\textup{bandwidth}}$) and read/write operations ($\textit{CF}_{\textup{iops}}$) for Mira.
    }
    \label{fig:hist-cf-mira}
\vspace{-.2in}
\end{figure}

We can generalize this notion of the coverage factor to any metric for which we can distinguish the contribution of an individual job from the global system-level measurement.
However, these alternative coverage factor metrics are not likely to correlate well with performance unless the job of interest is making a meaningful contribution to the system-wide load.
For example, the coverage factor for IOPs can be expressed as the fraction of read and write operations extracted from a job's Darshan log to the total read and write operations logged by ggiostat.  The distribution of this $\textit{CF}_\textit{IOPs}$ metric is shown alongside the bandwidth coverage factor, $\textit{CF}_\textit{BW}$, in  Figure \ref{fig:hist-cf-mira}.
This figure suggests that the TOKIO-ABC tests likely do not contribute a significant IOPs load Mira;
the relatively flat distribution of $\textit{CF}_\textit{IOPs}$ is likely a reflection of the background IOPs load which is not significantly perturbed when TOKIO-ABC jobs are running. \todo{is there is a clearer way to state the previous sentence?}

%%% following paragraph is weak; replaced with the one above
% To understand the relationship between IOP contention and performance degradation, we can also calculate the coverage factor of IOPS in addition to the coverage factor of bandwidth described in Equation \ref{eq:cf}.
% If the relationship between performance and bandwidth is proportionately affected by IOPS-based contention, we would expect the coverage factor of IOPS to follow a similar distribution. \todo{$\leftarrow$ weak sauce}
% However, as shown in Figure \ref{fig:hist-cf-mira}, $\textit{CF}_{\textup{iops}}$ for Mira jobs are broadly distributed, whereas $\textit{CF}_{\textup{bandwidth}}$ shows a preponderance of high values.
% From this, we can conclude that many tests on Mira achieved high performance \emph{despite} other I/O loads concurrently generating commensurate IOP loads.
% Thus, while high background IOPS loads can be a cause for performance loss, their presence does not necessarily preclude high performance.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Correlating Performance Metrics} \label{sec:results/correlating}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Our choice to define our correlation parameter according to application performance and server-side bandwidth and IOPs was motivated by a broad body of literature and an intuitive assumption that competition for bandwidth and IOPs affect performance the most dramatically.
However, the TOKIO framework is generalized to draw data from any resource that can be indexed on a per-job or time series basis.
As such, we can attempt to correlate a wide variety of measurements with job I/O performance to determine which measurements are likely culprits when looking for sources of poor performance.

To this end, we calculated the Pearson correlation coefficient between each job's fraction of peak performance (as defined in Section \ref{sec:results/overview}) with a wide range of metrics collected on Edison and Mira.
While many of these metrics are not expected to be normally distributed~\cite{Kim2010}, Pearson correlation coefficients are applied here to compare the directions and relative strengths of the relationships between each metric we analyzed.
In this sense, we find that this correlation coefficient is a suitable indicator of general trends and confidences in correlation.

An extensive amount of data was generated in this study, and a comprehensive statistical analysis will be presented in future work.
Nevertheless, we summarize some of the interesting correlations (and lack of correlations) in Figure \ref{fig:correlation-table} and highlight several interesting findings:

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figs/correlation_table.pdf}
    \caption{Correlation coefficients between fraction of peak performance measured for each I/O motif and a variety of server-side measurements and metrics.
    Box color indicates confidence; correlations with p-values $< 0.01$ are blue; p-values $< 0.05$ are green, and p-values $>= 0.05$ are red (and signify lack of correlation).
    Similarly, \textbf{bolded} values signify moderate correlation ($|r| > 0.30$), and \textit{italicized} values signify weak correlation ($|r| < 0.10$).
    Although the "\% Servers Failed Over" measurement were tracked on both Mira and Edison, no changes in the failover states of servers were ever observed over the course of this study.
    }
    \label{fig:correlation-table}
\vspace{-.2in}
\end{figure}

\begin{itemize}

\item As assumed in the previous sections, the coverage factors correlate with performance on all file systems, indicating that contending for file system I/O is a moderate contributor to performance loss.
The strength of this correlation on Mira was lower than Edison because the absolute performance of our tests on Mira were bound by the bandwidth of the exclusively allocated I/O nodes, leaving bandwidth headroom on the NSD servers for other jobs.
Mira's file system also demonstrated moderate sensitivity to contention for IOPs; unfortunately, Lustre IOPs were not collected for this study, so no comparison can be made to Edison.

\item Mira's performance correlates negatively with higher rates of \texttt{open(2)}/\texttt{close(2)} calls than Lustre.
Given that Mira's file system serves metadata from the same physical servers as data, this relationship is reasonable.
By comparison, Edison's file systems each have their own discrete metadata servers are specifically designed to decoupled bulk data transfer performance from metadata operations.

\item The fullness of each storage device (LUN on Mira and OST on Edison) has markedly different behavior between Edison and Mira.
While Mira's performance is uncorrelated with device capacity, Edison performance degrades as free space on OSTs is depleted.
This is a documented behavior of Lustre file systems.

\item Perhaps contrary to historic intuition, I/O performance shows minimal correlation with the number of other jobs running concurrently.
The lack of correlation with concurrent job count is consistent with our finding that I/O remains highly bursty; a large number of small jobs are highly unlikely to burst simultaneously, and each small job is not individually capable of significantly impacting our tests' coverage factors.

\item Similarly, the job diameter (a measurement of how spread out a job is across the compute fabric) has no discernible correlation with I/O performance on Edison.
Given the fact that the low-diameter dragonfly topology of Edison is designed to reduce the performance impact of job topology, this is consistent with expectation.

\end{itemize}

In addition to the measurements and metrics presented in Figure \ref{fig:correlation-table}, the TOKIO framework collected a larger number of system-specific measurements that did not correlate significantly to performance.
However it is important to underscore the fact that this demonstration was not intended to be exhaustive, and the correlations and p-values for the Edison system are likely diminished by the fact that all three Edison file systems, each with unique users, workloads, and governance policies, were combined for this analysis.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Job-Level Analysis} \label{sec:results/umami}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We envision TOKIO not only as a mechanism to understand broad system behavior as illustrated in Section~\ref{sec:results}, but also to understand the factors that contribute to the performance of individual application executions.
In this context, our goal is to produce rapid, actionable information that helps facilities support staff and power users make more effective use of their time.
The system-level analysis can be integrated with ongoing I/O instrumentation to describe:

\begin{enumerate}
\item where on the spectrum of normalcy a specific job's I/O behavior fell relative to other jobs with a similar I/O motif, and
\item which measurements and metrics are the best places to start investigating abnormal conditions based on how they have historically correlated with performance
\end{enumerate}

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\columnwidth]{figs/umami-scratch2-hacc-write.pdf}
    \caption{TOKIO-UMAMI demonstrating the \emph{file system climate} of HACC write workloads on the Edison scratch2 file system compared to a most recent run, which showed highly unusual \emph{file system weather}.
    The left panes show the measurements from previous runs of the same motif, and the box plots in the right panes summarize the distribution of these metrics.
    The star denotes the metrics for the job of interest and is colored according to the quartile in which it falls (red being the worst quartile and blue the best).
    Box plot whiskers extend to the 5th and 95th percentiles, with outliers being denoted as circles.}
    \label{fig:umami-scratch2-hacc-write}
\vspace{-.2in}
\end{figure}

To concisely visualize all of this information, we present the TOKIO Unified Measurements and Metrics Interface (UMAMI) with the goal of enabling users to quickly determine how a job of interest's performance compares to similar I/O workloads in the past.
TOKIO-UMAMI, demonstrated in Figure \ref{fig:umami-scratch2-hacc-write} and detailed in the following sections, presents historic measurements from the components monitored by TOKIO (the I/O climate) and summarizes each metric's distribution in an accompanying box plot.
These time series plots terminate with the measurements taken for the job of interest, thereby describing the I/O weather at the time that job ran.
By overlaying this weather on the climate as dashed lines in the box plots, TOKIO-UMAMI provides a quick visualization of how each metric's weather compared to the statistical distribution of past weather conditions.
With this conceptualization of a file system's weather and its overall climate, a user can differentiate between a long-term performance problem and a statistically rare event analogous to an extreme weather event.

In the following sections, we illustrate how TOKIO-UMAMI can be applied to the problem of diagnosing poor performance of individual jobs in three case studies.

\subsection{Case Study: I/O Contention}

The specific UMAMI example shown in Figure \ref{fig:umami-scratch2-hacc-write} represents a HACC write test which took place on March 3.
This particular job showed abnormally low performance as evidenced by the "Job performance (GiB/sec)" measurement and its value relative to previous instances of this type of job.
This abnormally poor job performance was accompanied by an unusually low coverage factor and high metadata load, and these unfavorable conditions are highlighted as red dashed lines in the box plots which denote their place in the least-favorable quartile of past measurements.
The metrics corresponding to blue dashed lines fell into the most favorable quartile for this problematic job, but as discussed in Section \ref{sec:results/correlating}, have no history of correlation with performance.
Thus, we can quickly attribute the poor performance of this HACC job to an I/O extrinsic to this HACC job which was competing for both bandwidth on the data servers and metadata resources on the metadata server.

\subsection{Case Study: Namespace contention}

Figure \ref{fig:umami-mira-fs1-vpic-write} represents a VPIC write workload
that showed poor performance on Mira.
However, its coverage factor is within normal parameters (the orange lines in each metric's box plot signifies a value in the second quartile) indicating a general lack of bandwidth contention.
Although the IOPS coverage factor is also abnormally low, previous conditions have been worse despite a lack of dramatic performance loss (e.g., on March 7).
The only metric that shows a unique, undesirable value is the number of \texttt{readdir(3)} operations handled by the file system.
This is indicative of an expansive file system traversal that was being performed at the same time as the job execution.
% The \emph{readdir(3)} metric was demonstrated to correlate moderately negatively with performance in Figure \ref{fig:correlation-table}.

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\columnwidth]{figs/umami-mira-fs1-vpic-write.pdf}
    \caption{TOKIO-UMAMI demonstrating the climate surrounding VPIC-IO write workloads on Mira compared to a most recent run, which showed highly unusual weather in the form of an excess of \texttt{readdir(3)} calls.
    Significance of each pane and its contents are the same as explained in Figure \ref{fig:umami-scratch2-hacc-write}.}
    \label{fig:umami-mira-fs1-vpic-write}
\vspace{-.2in}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Case Study: Storage Capacity}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The TOKIO framework was also able to identify longer-term performance issues which progressively degraded write performance.
Figure \ref{fig:umami-scratch3-hacc-write-long-term} shows the UMAMI view such an event on Edison's scratch3 file system where coverage factors were not highly abnormal despite an ongoing $2\times$ slowdown over the normal 50 GiB/sec.
The magnitude of performance loss followed the highest CPU load observed across all of the Lustre OSSes almost exactly, and this period coincided also with the scratch3 file system reaching critical levels of fullness.
Although such correlations cannot define causative relationships, these conditions indicated a relationship between critically full storage devices and CPU load (e.g., an increasing cost of scavenging empty blocks) that impacts application performance, consistent with our understanding of how Lustre performance degrades with increasing OST fullness.
 
\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\columnwidth]{figs/umami-scratch3-hacc-write-long-term.pdf}
    \caption{TOKIO-UMAMI of HACC write performance on Edison's scratch3 file system showing a longer-term period of performance degradation that was associated with unusually high OSS CPU load.
    Significance of each pane and its contents are the same as explained in Figure \ref{fig:umami-scratch2-hacc-write}.}
    \label{fig:umami-scratch3-hacc-write-long-term}
\vspace{-.2in}
\end{figure}
