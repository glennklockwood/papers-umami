\appendix

\section{Artifact Description} \label{sec:appendix/artifacts}

% Consider adding material here per guidance at
% http://sc17.supercomputing.org/2017/02/07/submitting-a-technical-paper-to-sc17-participate-in-the-sc17-reproducibility-initiative/
% see examples: http://sc17.supercomputing.org/submitters/technical-papers/reproducibility-initiatives-for-technical-papers/artifact-description-paper-title/

\todo{Here's the data we should ultimately provide, though probably not for initial submission}

\todo{Note that most of last year's submissions seemed to generate the appendix as a separate document whose pages were concatenated with the main manuscript. the way LaTeX handles appendices and subsections doesn't seem very nice.}

\todo{we need to agree on the scope of this.  do we just provide enough data to reproduce the results (figures) presented (e.g., the edison and mira CSVs, and the ggiostat/lmt dumps for the relevant time periods) or do we include the entire framework setup as well?  the latter is a much heavier lift than the former, and it's not clear to me (glenn) which is most in the spirit of this reproducibility initiative.}

\subsection{Abstract}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Here we describe the steps required to reproduce the analysis and results presented in Section \ref{sec:results} of this manuscript.

\subsection{Description}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Check-list (artifact meta information)}

{\small
\begin{itemize}
  \item {\bf Algorithm: }
  \item {\bf Program: }
  \item {\bf Compilation: }
  \item {\bf Transformations: }
  \item {\bf Binary: } 
  \item {\bf Data set: }
  \item {\bf Run-time environment: }
  \item {\bf Hardware: }
  \item {\bf Run-time state: }
  \item {\bf Execution: }
  \item {\bf Output: }
  \item {\bf Experiment workflow: }
  \item {\bf Experiment customization: }
  \item {\bf Publicly available?: }
\end{itemize}
}


\subsubsection{How software can be obtained (if available)}

\paragraph{TOKIO}
The implementation of TOKIO presented here, will be publicly released upon completion of the double-blind review process.  Its dependent component-level monitoring tools are considered software dependencies and are separately discussed below.

\paragraph{TOKIO-ABC}
This artifact is comprised of four open source applications with download, configuration, and build scripts, several required portability patches, and tools for automation and managing output data.  Upon completion of the double-blind review process, we will be releasing TOKIO-ABC as a publicly accessible git repository.

\subsubsection{Hardware dependencies}

There are no specific hardware dependencies for TOKIO or TOKIO-ABC.  There may be hardware dependencies of the component-level monitoring tools with which TOKIO integrates, but such tools are not considered artifacts of this work.

\subsubsection{Software dependencies}

\paragraph{TOKIO}

TOKIO specifically depends on Python 2.7 and a number of associated data analysis packages.  Specifically, the reference implementation presented here relied on pandas 0.18.1, matplotlib 2.0.0, numpy 1.11.1, and scipy 0.17.1.  For simplicity, we opted to use the software environment provided by Continuum IO's Anaconda version 4.3.14 with matplotlib explicitly upgraded to 2.0.0.

TOKIO also relies on a number of component-level monitoring tools.  As described in this paper, TOKIO can integrate with any tool that provides scalar or time-resolved data types, and for the purposes of this study, we used the following:

\begin{itemize}
\item Darshan 3.1.3 % \url{http://www.mcs.anl.gov/research/projects/darshan/}.
\item ggiostat
\item LMT % \url{https://github.com/LLNL/lmt}
\item Lustre 2.5.1 (health monitoring via lfs and lctl)
\item Slurm 17.02.1-2 and CLE 6.0UP03 (job topology on Edison)
\end{itemize}

\paragraph{TOKIO-ABC} The Automated Benchmark Collection is a meta-package that contains the specific versions of each benchmark used, specific patches applied to those upstream versions, and scripts that configure and build the collection.  As such, its external dependencies are those of the benchmark applications which include:

\begin{itemize}
\item autoconf 2.69 or later
\item automake 1.13 or newer
\item an MPI 3.0-compliant implementation of MPI
\item HDF5 1.8
\end{itemize}

Further details on known issues and specific version incompatibilities are documented in the TOKIO-ABC source distribution.

\subsubsection{Datasets}

For validation of this work as well as allowing the community to build upon our experimental data, we will release the entirety of the data presented in this paper upon completion of the double-blind review process.  This data set will include

\begin{itemize}
\item All scalar values recorded for every TOKIO-ABC job conducted over the experimental period in the form of CSV files
\item Unmodified Darshan logs for each ABC job
\item Time series data collected by LMT and ggiostat for each TOKIO-ABC job at 5-second resolution on a per-OST (LMT) or per-cluster (ggiostat) basis.  These data will be encoded in HDF5 format.
\item 
\end{itemize}

This dataset will be accompanied by documentation that describes the nomenclature and file formats.

\subsection{Installation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{TOKIO}

\paragraph{TOKIO-ABC}

\subsection{Experiment workflow}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Evaluation and expected result}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Experiment customization}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Notes}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
