\input{systemtable}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experimental Methods} \label{sec:platforms}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

To examine the utility and generality of integrating data from multiple
component-level monitoring tools into a single view (UMAMI), we conducted an
I/O benchmark study on two distinct HPC platforms described in Table \ref{tab:system-config}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{NERSC Edison} \label{sec:platforms/edison}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Edison is a Cray XC-30 system at the National Energy Research Scientific
Computing Center (NERSC).
Its scratch1 and scratch2 Lustre file systems are identically configured,
and users are evenly distributed across them.
However, access to Edison's scratch3 file system is granted only to users who require high parallel bandwidth, and therefore the scratch3 file system should reflect larger, more coherent I/O traffic.

Edison's architecture routes I/O traffic from its Aries high-speed network
to the InfiniBand SAN fabric via LNET I/O nodes.
Routing is configured such that each LNET I/O node handles traffic for only one of the three Edison file systems to ensure that each file system's traffic is isolated as it transits I/O nodes.
This also allows jobs of any size to use the maximum number of I/O nodes for each file system.
In this work, all output data was striped over all OSTs in each file system, and the input parameters listed in Table \ref{tab:bench-config} were chosen to saturate each file system's bandwidth.
Our IOR benchmarks demonstrated 90\% of the theoretical maximum performance.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{ALCF Mira} \label{sec:platforms/mira}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Mira is an IBM Blue Gene/Q system at the Argonne Leadership Computing Facility (ALCF).
In addition to the Spectrum Scale (GPFS) servers and LUNs listed, six of the network shared disk (NSD) servers also serve metadata from SSD-based LUNs.
Jobs on Mira are allocated I/O nodes and compute nodes in a fixed ratio (1 I/O node for every 128 compute node), causing storage bandwidth to scale linearly with the size of the job.
To keep compute resource consumption low, we opted to run every benchmark using 1,024 compute nodes, giving them eight I/O nodes and an aggregate peak bandwidth of $\sim$25 GB/sec.
Our IOR configuration achieved 80\% of peak performance for this job size.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{I/O performance regression tests} \label{sec:methods/tests}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

For this study, we ran the following benchmark applications
using three file systems across 39 days on Edison (1,014 benchmark runs) and on one file system across 29 days on Mira (118 benchmark runs).

\begin{itemize}[leftmargin=*]
\item 
\textbf{Hardware Accelerated Cosmology Code (HACC)}, a cosmology application~\cite{habib2012}, configured to generate 96 MiB per process using POSIX file-per-process checkpoint I/O.
 \item
\textbf{Vector Particle-In-Cell (VPIC)}, a plasma physics application~\cite{Bowers2008}, configured to write 1.0 GiB per process to a single HDF5 file using the H5Part API~\cite{H5Part}, and 
\textbf{BD-CATS}, a clustering analysis system used to analyze VPIC output data~\cite{Patwary2015}, configured to read 75\% of our VPIC output to emulate a 3D clustering analysis.
\item
\textbf{IOR}, a widely used tool to characterize parallel file system performance~\cite{Yildiz2016,Xie2012,Lofstead2010,Uselton2010}, used to determine each file system's performance under optimal I/O workloads.
 \end{itemize}

All benchmarks were run using 1,024 and 128 nodes (16 processes per node) on Mira and Edison, respectively.
These scales were chosen to sufficiently utilize the capability of the storage system while limiting the core-hour consumption,
and the resulting data volumes are summarized in Table~\ref{tab:bench-config}.

%On Edison we executed 1,014 benchmark runs using three file systems across 39 days, while on Mira we executed 118 benchmark runs using one file system across 29 days.

\input{benchmarktable}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Data Sources} \label{sec:methods}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We drew a total of 58 different metrics from a variety
of monitoring tools already in production use on Mira and Edison over the
course of our study.

\subsection{Application behavior} \label{sec:methods/darshan}

To capture the I/O patterns and user-observable application performance in this study, we used the Darshan I/O characterization tool~\cite{carns200924} which transparently records statistics about an application's I/O behavior at runtime.
It imposes minimal overhead because it defers the reduction of these statistics until the application exits,
allowing it to be deployed for all production applications on large-scale systems without perturbing performance.  Both Mira and Edison link Darshan into all compiled applications by default.

\subsection{Storage system traffic} \label{sec:methods/storagesystraffic}

Storage system traffic monitoring provides aggregate systemwide I/O workload metrics such as bytes read/written and operation counts for reads, writes, and metadata.
These time-series data are collected with minimal impact on application performance because they are gathered on the storage servers, not compute nodes.
On both Mira and Edison, these metrics were collected at five-second intervals using file-system-specific tools.  

\label{sec:methods/lmt}
\textbf{Lustre Monitoring Tools (LMT)} aggregates Lustre-specific counters on each object storage server (OSS) and metadata server (MDS) and presents them via a MySQL database.
LMT provides data including bytes read and written, CPU load averages, and metadata operation rates.

\label{sec:methods/ggiostat}
\textbf{ggiostat} is a tool developed at the ALCF to collect similar data from IBM Spectrum Scale file systems.
It retrieves and stores metrics from server and client clusters and provides bytes read/written and operation counts for reads, writes, and metadata operations.

\subsection{Health monitoring} \label{sec:methods/health}

Health-monitoring data describe what components are offline, failed-over, or in another degraded state and how much free capacity remains on the available devices.
On Edison, the fullness of each Lustre object storage target (OST) is recorded every fifteen minutes.  
On Mira, the fullness of each LUN and the failure status of each server is recorded upon job submission.
The mapping between OSTs/LUNs and OSS/NSD servers are also logged to identify devices that have failed over to a degraded state.

\subsection{Job scheduling} \label{sec:methods/scheduling}

Job-scheduling data provides details on the jobs that are concurrently running on a system and can help identify cases where I/O contention results from competing jobs.
In this study, we tracked the number of other jobs that were running and the number of core-hours consumed system-wide during the time our benchmark jobs ran.

% Retrieving these data on Edison is accomplished by querying the job accounting database for all jobs with a start time before the benchmark's end time and an end time after the benchmark's start time.
% Similarly, the job accounting data on Mira is stored in a database accessible via a Python API called Ni.
% The API allows pulling out a list of jobs for a given time range and running on a specific resource.

\subsection{Job topology} \label{sec:methods/other}

To identify any effects of job placement on Edison and Mira's high-speed networks, we calculate a job's maximum radius as an approximation of that job's degree of delocalization.
Using the topological coordinates of each job's compute node allocation to derive a center of mass of a job, we define this metric as the maximum distance between that center of mass and a compute node.

% Job layouts on Edison can be obtained by retrieving job node lists from the Slurm accounting database and joining these job nodes with dragonfly topology coordinates stored the Cray service database.
% On Mira, jobs are always close-packed on its torus which results in the maximum radius of each job being identical.
