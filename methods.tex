\section{Instrumentation methods} \label{sec:methods}

We identified the following categories of ongoing instrumentation as being
critical to understanding both the I/O climate and I/O weather of a
production system.  \todo{overview diagram here?}

\subsection{Application behavior}
\label{sec:methods/darshan}

Application behavior refers to the I/O pattern of a job as expressed from
the perspective of the application itself (i.e., the I/O pattern of the
application itself before any system-level optimizations are applied).
We rely on the Darshan I/O characterization tool~\cite{carns200924}
to capture this information.  Darshan transparently records concise,
bounded statistics about an application, such as the amount of time it
spent performing I/O, the distribution of access sizes, and what files
were accessed.  Reduction, compression, and storage of these statistics
is deferred until the application exits in order to minimize overhead.
Darshan has two notable properties that are desirable for use in TOKIO.
Because of its lightweight design, Darshan can be deployed for all
production applications on large-scale systems without perturbing
performance.  Because it operates at the application level, it is also highly
portable and can be deployed on nearly any major HPC platform.

% PHC: does the modularity thing have an impact on the rest of the paper?
%
% Recently, Darshan's architecture has been modularized to allow new sources
% of I/O data to be more easily integrated into its summaries \cite{snyder2016modular}.
% This modification allows for data from distinct components within the deep
% HPC I/O stack to be more easily correlated, providing a more comprehensive view
% of application I/O behavior. For instance, data captured from higher-level I/O
% libraries like HDF5 and MPI-IO can be correlated with data from the POSIX layer
% to determine potential inefficiencies in an I/O workload. Also, this modularized
% version of Darshan includes a Lustre module that can be correlated with data from
% other modules to gain insight into how an I/O workload interacts with the
% underlying file system.

\subsection{Storage system traffic}

Storage system traffic refers to the aggregate system-wide I/O workload
observed by the primary storage system.  On most modern-day HPC systems this
is reflected in the aggregate traffic that reaches the parallel file
system, and is most easily represented with ongoing time-series metrics.

\label{sec:methods/lmt}
The Lustre Monitoring Tool (LMT) is a widely used tool for Lustre file systems that collects Lustre-specific counters from \texttt{/proc/fs/lustre} on each Lustre OSS and MDS and presents them to external consumers via a MySQL database.
NERSC Edison implements LMT
as a part of the Cray Sonexion Lustre platform \cite{Keopp2014}, and we built
upon the pyLMT framework developed at NERSC \cite{Uselton2009} to preserve
server-side metrics during benchmark runs across all file systems evaluated.
These metrics include bytes read and written, CPU load averages, and metadata
operation rates on a per-server basis at five-second intervals.

\label{sec:methods/ggiostat}
We developed the \emph{ggiostat} tool in order to collect analogous data from
IBM Spectrum Scale (GPFS) file systems.  \assign{Zach}  It relies upon the
\texttt{mmpmon} monitoring system in GPFS to retrieve metrics from server and
client clusters.  These metrics are queried on five second intervals by a
persistent monitoring daemon and stored in an IBM DB2 database.
The metrics collected include bytes read and written,
read and write operations, and number of inode updates \todo{(is this
a proxy for metadata operation rates?)}.

\subsection{Health monitoring \assign{Glenn and Kevin}}
\label{sec:methods/health}

Health monitoring refers to the current fault status and capacity of the
storage system: what components are offline, and how much storage space
remains on the available components.

% lfs df
% lctl dl -t
% mmlsdisk
% mmdf
For Lustre file systems, we record the fullness of each Lustre object storage target (OST) every fifteen minutes and associate the last measurement to precede the start of a job with that job.
We also record the server to which each target is mapped at this time, allowing us to identify object storage servers that are the recipient of failed-over OSTs.
On GPFS, we record the fullness of each disk LUN that contains data and the failure status of each LUN at the start of each job.
These data are then associated with job in which the collection was triggered.

\subsection{Job scheduling \assign{Glenn and Kevin}}

Job scheduling refers to the mix of concurrent application jobs that are
running on the compute resources of a system at any given time.  Since I/O contention is often the result of other jobs running concurrently with a job of interest, we record the total number of jobs that were in flight at any point during the execution time of each TOKIO-ABC job.  On the Edison system, this is accomplished by querying the Slurm job accounting logs for all jobs with a start time before the job of interest's end time and an end time after the job of interest's start time.
\todo{describe how we pull concurrent job activity from Mira?}

\subsection{I/O performance regression tests} \label{sec:methods/tests}

The passive instrumentation methods described above are essential for
ongoing understanding production workloads, but they have an inherent
drawback in that there is no fixed reference point: the workload
and state of the system evolves over time.  We must therefore incorporate
routine, formalized I/O performance regression tests in order establish baseline behavior.

We developed the TOKIO Automated Benchmark Collection (TOKIO-ABC) to meet
this need.  TOKIO-ABC consists of a collection of benchmarks, including
both synthetic and application-derived workloads, that can be scaled in
both the number of processes and data volume according to the constraints
of the system on which it is deployed. Our two goals in choosing a scale
for TOKIO-ABC deployment are a) to saturate the storage system and b)
to limit core-hour consumption sufficiently for daily execution.  We use
a collection of benchmarks because performance may not well-represented
by a single benchmark result; each storage system has its own strengths
and weaknesses for different workloads.  The collection of benchmarks is
executed within a job script that can be scheduled nightly by a continuous
integration system or cron job.  The script insures that no more than
one TOKIO-ABC instance is active at a time, and all benchmark results
and Darshan logs are archived for analysis at the conclusion of the job.
The initial set of benchmarks included in TOKIO-ABC are as follows:

\begin{itemize}
\item \textbf{HACC} \assign{Shane}
\item \textbf{VPIC} \assign{Suren} 
Vector particle-in-cell (VPIC) is a highly scalable code developed to simulate
interactions among trillions of plasma physics particles  \cite{Bowers2008}.
VPIC-IO kernel extracts the I/O operations of a magnetic reconnection
simulation, where each MPI process writes 8M (i.e., $8 \times 1024 \times 1024$) particles. Each
particle has eight properties (six floating point and two integer) and each
property is a single dimension array. The total number of particles depends on
the number of MPI ranks used. The kernel uses the H5Part API \cite{H5Part} to write
the data to a single shared HDF5 file, and the execution of the I/O kernel includes
creating and opening a file, writing data to the file, and closing the file.

\item \textbf{BD-CATS} \assign{Suren} The BD-CATS clustering system~\cite{Patwary2015}  represents
one of the analyses that is commonly performed on the output of VPIC's particle data files.
For this study, we emulate the I/O workload of a clustering both particle
positions and momenta in three dimensions using the BD-CATS-IO I/O kernel benchmark.  This amounts to 75\% of the data
contained in the HDF5 file (generated by the VPIC-IO kernel) being read.

\item \textbf{IOR} \assign{Shane} The IOR benchmark has been used extensively
to characterize the performance characteristics of parallel file systems\cite{Yildiz2016,Xie2012,Lofstead2010,Uselton2010}
due to its extensive configurability.  For the purposes of this work, we applied
IOR to determine each file system's performance variability under conditions
where an application is performing I/O using the ideal parameters for each
file system.

\end{itemize}

\section{Data integration}

\todo{PHC: fill this in.  Maybe it comes after experimental platforms?  At
any rate, say something, even if brief, about how we bring together all of
the instrumentation sources we listed into a coherent data set.}

\section{Experimental platforms} \label{sec:platforms}

We have deployed the TOKIO framework on two distinct platforms in this study.

\begin{itemize}
\item \textbf{Cori} \assign{Glenn}
\item \textbf{Mira} \assign{Phil}
\end{itemize}

\todo{Here or in evaluation, describe how we sized the jobs for each
platform.  See mailing list discussion Jan27-Feb6.  Mira has fixed ratio of
ions to compute nodes, Cori does not, leads to different characteristics.  In
either case goal is to exercise file system as much as we can within job
sizes that will get through queue in daily cadence.}  The specific details are
described in Table \ref{tab:bench-config}.

% abandon all hope ye who try to edit this stupid table by hand.  I used
% http://www.tablesgenerator.com to make it.
\begin{table*}[h]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{\textbf{Benchmark}}                    & \multirow{2}{*}{\textbf{I/O Motif}}                           & \multicolumn{3}{c|}{\textbf{Mira}}                        & \multicolumn{3}{c|}{\textbf{Edison}}                      \\ \cline{3-8} 
                                                       &                                                               & \textbf{\# Nodes} & \textbf{\# Procs} & \textbf{\# Bytes} & \textbf{\# Nodes} & \textbf{\# Procs} & \textbf{\# Bytes} \\ \hline
IOR                                                    & \begin{tabular}[c]{@{}c@{}}MPI-IO\\ shared file\end{tabular}  & 1,024             & 16,384            & 1.0 TiB           & 128               & 2,048             & 0.5 TiB           \\ \hline
IOR                                                    & \begin{tabular}[c]{@{}c@{}}POSIX\\ file per proc\end{tabular} & 1,024             & 16,384            & 1.0 TiB           & 128               & 2,048             & 2.0 TiB           \\ \hline
HACC                                                   & \begin{tabular}[c]{@{}c@{}}GLEAN\\ file per proc\end{tabular} & 1,024             & 16,384            & 1.5 TiB           & 128               & 2,048             & 2.0 TiB           \\ \hline
\begin{tabular}[c]{@{}c@{}}VPIC\\ BD-CATS\end{tabular} & \begin{tabular}[c]{@{}c@{}}HDF5\\ shared file\end{tabular}    & 1,024             & 16,384            & 1.0 TiB           & 128               & 2,048             & 2.0 TiB           \\ \hline
\end{tabular}
\caption{Formalized regression testing parameters}
\label{tab:bench-config}
\end{table*}

\subsection{Mira benchmark configuration} \label{sec:platforms/mirabenchmarks}

Mira's I/O architecture is fundamentally different from that of the Cray
systems at NERSC, with fixed-size partitions of compute nodes connected to
a single I/O node that forwards application I/O requests to the SAN.
Saturating the I/O bandwidth of the underlying storage servers requires the
use of many I/O nodes, meaning that peak storage bandwidth can only be
attained using a large portion of the system's available compute nodes.
Running daily I/O benchmarks that span a large portion of Mira's compute
nodes is impractical due to the lengthy queue times of capability jobs and
the obvious objection system administrators would have to this practice.

For these reasons, we decided to use 1,024 Mira compute nodes (i.e., a single
rack), a partition size that should be small enough to move quickly through
scheduler queues, but large enough to exercise an adequate portion of the
storage system. We use 16 processes per node, resulting in a total of 16,384
processes executing each benchmark. The benchmark workloads were configured
to use enough I/O volume to drive the storage system for 1--2 minutes.
Full benchmark configurations for Mira are given below in
Table~\ref{tab:bench-config}.
