\section{Related work} \label{sec:related}

Several recent studies have explored how to combine and analyze multiple sources of I/O monitoring information.
Kunkel et al. developed SIOX~\cite{Kunkel:2014:SAC:2769884.2769901}, which aggregates information from multiple layers of the I/O stack,% into a global database
%with plugins for automatic optimization.
but it relies on instrumented versions of application libraries to collect metrics.
Agelastos et al. developed the Lightweight Distributed Metric Service (LDMS)~\cite{7013000} for scalable collection of compute node metrics which could be a data source for similar multicomponent analysie.
%.  The LDMS metrics include client-side I/O counters that could be integrated into TOKIO.
Liu et al. applied in-depth analysis to server-side I/O logs to deduce application-level I/O patterns and make scheduling recommendations~\cite{Liu2016}.

Other recent studies have explored how to quantify and combat various types of I/O performance variance.
Lofstead et al. similarly observed that variability can be caused by both external interference and internal interference within an application~\cite{Lofstead2010}.
They proposed an adaptive strategy that coordinates I/O activity within an application.
Dorier et al. proposed a middleware mechanism for coordinating I/O activity across applications to manage external interference~\cite{dorier2014calciom}.
Yildiz et al.'s systematic study of I/O interference in a controlled testbed environment found that poor flow control in the I/O path~\cite{Yildiz2016} also contributes to I/O variance.
Carns et al. reported I/O performance variability for seven frequently repeated production jobs during a two month study of the Intrepid Blue Gene/P system~\cite{carns2011understanding} and suggested that some access patterns are more susceptible to variability that others, but the root cause of that variability was not identified.

% Our work builds on these previous studies by introducing a framework that integrates multiple existing best-in-class characterization tools into a general analysis framework that can be adapted to any platform.

%for each
%component in a manner that can be rapidly adapted to different platforms.  We
%invision this as a way to rapidly provide feedback and contextual information to
%users as they execute their applications.

% Andrew Uselton's work on understanding I/O performance in terms of ensembles of
% bursts might be relevant\cite{Uselton2010}, but his method requires heavyweight
% I/O tracing to determine the statistical distribution of I/O bursts during an
% application execution and, as such, as better suited to characterizing the
% behavior of a specific file system as a one-time activity.

% A long time ago, David Skinner and Bill Kramer wrote a paper that characterized
% sources of performance variation~\cite{Skinner2005}, but it didn't really talk
% about I/O in a very meaningful way.

% More recently Xie et al also characterized I/O problems at Oak
% Ridge\cite{Xie2012} using IOR and quantified the effects of stripe counts,
% straggling writers, and shared-file I/O to target areas where middleware can
% optimize I/O for applications.

% Is any of the BeeGFS stuff (dynamically provisioning file systems) relevant to
% interference isolation?
