\section{Introduction} \label{sec:introduction}

% \emph{From Rob}: I think a component of the story is that when we approach
% these problems, there are a few challenges:
% \begin{enumerate}

% \item increasing number of interoperating components (in this case, additional
% BB and DVS and so forth)

The stratification performance and capacity in storage technology is motivating the design of increasingly complex parallel storage systems architectures.
For example, leadership-class computing systems are now being deployed with flash-based, on-fabric burst buffer tiers~\cite{Henseler2016} that provide even higher performance than traditional disk-based scratch file systems~\cite{Bhimji2016}.
Although designed to provide optimal performance and capacity on an economic basis, this increasing number of interoperating components also complicates the task of understanding I/O performance.

% \item different components have different "views" on I/O, different levels of
% monitoring, some of which aren't practical in production

The current state of practice is to monitor each component in the I/O stack separately.
However, different components approach I/O from different perspectives, often resulting in component-level monitoring data that are not obviously compatible.
For example, server-side monitoring tools such as LMT~\cite{lmt} measure a limited number of metrics as a high-frequency time series to achieve low overhead, while application-level profiling tools such as Darshan~\cite{carns200924} track metrics that are expressed as bounded
summaries of individual jobs.
Data types representing the same logical quantity, such as data written, may also be expressed in different units such as bytes, pages, and blocks, and these units of data may also be transformed by aggregation, coalescing, and caching as they traverse the I/O stack.

% \item no current framework for integration, lots of expert knowledge to
% construct the story of what happened and how to fix.

At present, the gaps of information resulting from these incompatibilities are filled using expert knowledge.
Absent this expert institutional knowledge though, one of the most fundamental challenges in understanding I/O performance is how to gauge the performance and behavior of application within a broader context and answer the question: 
does the I/O performance of a given job meet expectations given the capabilities of the system and the nature of the access pattern?
Relying on expert knowledge to answer this question is neither a scalable nor sustainable model for diagnosing performance variation as I/O subsystems of increasing size and complexity are deployed.
Thus, there is a growing need for a framework that integrates data from across all components and presents a coherent, holistic view of the inter-dependent behavior of these components to clarify the relationships that have traditionally fallen on I/O experts.

To address this need, we have developed a framework for the holistic instrumentation of I/O subsystems, called the Total Knowledge of I/O (TOKIO) framework.
TOKIO combines data from file system servers, application-level profiling, and other system-level sources into a uniform and normalized format in a way that is generalizable to different system architectures.
With this framework, we are then able to differentiate general performance expectations for different I/O motifs (analogous to the climate of the I/O system) from transient effects (analogous to the weather of the I/O system).
We use this notion of the \emph{I/O climate} to encompass the characteristics of storage components, their age and capacity, and the way they generally respond to a specific workload.
Complementary to the I/O climate, the \emph{I/O weather} is determined by momentary job scheduler load, contention, and short-term failure events.
By running a month-long experiment using formalized I/O regression benchmarking, we then show how the TOKIO framework can contextualize performance variation with these concepts of I/O climate and weather.
We demonstrate a universal metrics and measurements interface (TOKIO-UMAMI) that quickly classifies a job's I/O performance as being within the expected variation of the file system climate, or if it can be attributed to an extreme file system weather event. 
Finally, we show how the TOKIO framework can be applied to bridge the gap of expert knowledge by identifying common sources of I/O performance variation on two different file systems deployed at different HPC centers.

The primary contributions of this work are as follows:

\begin{itemize}
\item A framework for holistic instrumentation of I/O subsystems that is generalizable across different file systems and infrastructures
\item Demonstration of this framework on NERSC's Edison system and ALCF's Mira system, along with an automated benchmark collection to establish I/O climate records for several I/O motifs
\item A demonstration of the types of insights that can be gleaned from this approach based on case studies of four scientific applications and workloads drawn from the month-long benchmarking effort on Edison and Mira
\end{itemize}

\todo{Some text to more clearly define who we are helping with this.
An end-user can't use this without escalated privileges so it should target facilities.
Also only enabling post-mortem analysis.}
