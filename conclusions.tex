\section{Conclusions} \label{sec:conclusions}

By integrating data from existing tools to gather data from applications, storage systems, system health, and job scheduling into a single holistic analysis tool, we demonstrated that I/O performance variation can only be complete understood by examining all of the components within the I/O subsystem in a holistic manner.  We performed a month-long benchmarking study and presented several case studies to underscore these findings and characterize the I/O \emph{climate} on each system, then detect and investigate particular instances of abnormal I/O \emph{weather}.

By integrating a range of normalized monitoring metrics into the UMAMI diagram, we found that
contention with other I/O workloads for storage system bandwidth, metadata op rates, and storage capacity can, but do not always, have an impact on application I/O performance.
No single monitoring metric that predicts I/O performance universally across HPC platforms;
the most highly correlated metrics depend on system architecture, configuration parameters, workload characteristics, and system health.
Factors such as job radius and $\textit{CF}_{\textit{nodehrs}}$ do not capture sufficient detail to reliably describe performance loss.
From these findings, we have a basis for improving existing I/O monitoring tools to capture more detailed metrics that can strongly affect I/O performance.